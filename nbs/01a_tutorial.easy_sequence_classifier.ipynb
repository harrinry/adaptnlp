{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Sequence Classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence Classification (or Text Classification) is the NLP task of predicting a label for a sequence of words.\n",
    "\n",
    "For example, a string of `That movie was terrible because the acting was bad` could be tagged with a label of `negative`. A string of `That movie was great because the acting was good` could be tagged with a label of `positive`.\n",
    "\n",
    "A model that can predict sentiment from text is called a sentiment classifier, which is an example of a sequence classification model.\n",
    "\n",
    "##### Below, we'll walk through how we can use AdaptNLP's EasySequenceClassification module to easily do the following:\n",
    "1. Load pre-trained models and tag data using mini-batched inference\n",
    "2. Train and fine-tune a pre-trained model on your own dataset\n",
    "3. Evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load pre-trained models and tag data using mini-batched inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first get started by importing the EasySequenceClassifier class from AdaptNLP and instantiating the\n",
    "`EasySequenceClassifier` class object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp import EasySequenceClassifier\n",
    "from pprint import pprint\n",
    "\n",
    "classifier = EasySequenceClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can dynamically load models as you run inference.\n",
    "\n",
    "Let's check out the Hugging Face's model repository for some pre-trained sequence classification models that some wonderful people have uploaded. The repository can be found [here](https://huggingface.co/models?search=&sort=downloads&filter=text-classification)\n",
    "\n",
    "Let's tag some text with a model that [NLP Town](https://www.nlp.town/) has trained called `nlptown/bert-base-multilingual-uncased-sentiment`.\n",
    "\n",
    "This is a multi-lingual model that predicts how many stars (1-5) a text review has given a product. More information can be found via. the Transformers model card [here](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-31 02:21:25,011 loading file nlptown/bert-base-multilingual-uncased-sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting text: 100%|██████████| 1/1 [00:00<00:00, 98.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "{\"This didn't work at all\": [1 star (0.8421),\n",
      "                             2 stars (0.1379),\n",
      "                             3 stars (0.018),\n",
      "                             4 stars (0.0012),\n",
      "                             5 stars (0.0007)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "example_text = \"This didn't work at all\"\n",
    "\n",
    "sentences = classifier.tag_text(\n",
    "    text=example_text,\n",
    "    model_name_or_path=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    mini_batch_size=1,\n",
    ")\n",
    "\n",
    "print(\"Tag Score Outputs:\\n\")\n",
    "for sentence in sentences:\n",
    "    pprint({sentence.to_original_text(): sentence.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting text: 100%|██████████| 2/2 [00:00<00:00, 131.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "{\"This didn't work well at all.\": [1 star (0.622),\n",
      "                                   2 stars (0.3356),\n",
      "                                   3 stars (0.0403),\n",
      "                                   4 stars (0.0016),\n",
      "                                   5 stars (0.0005)]}\n",
      "{'I really liked it.': [1 star (0.0032),\n",
      "                        2 stars (0.0048),\n",
      "                        3 stars (0.054),\n",
      "                        4 stars (0.4813),\n",
      "                        5 stars (0.4567)]}\n",
      "{'It was really useful.': [1 star (0.006),\n",
      "                           2 stars (0.0093),\n",
      "                           3 stars (0.0701),\n",
      "                           4 stars (0.4136),\n",
      "                           5 stars (0.501)]}\n",
      "{'It broke after I bought it.': [1 star (0.4489),\n",
      "                                 2 stars (0.3935),\n",
      "                                 3 stars (0.1416),\n",
      "                                 4 stars (0.0121),\n",
      "                                 5 stars (0.0039)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "multiple_text = [\"This didn't work well at all.\",\n",
    "                 \"I really liked it.\",\n",
    "                 \"It was really useful.\",\n",
    "                 \"It broke after I bought it.\"]\n",
    "\n",
    "sentences = classifier.tag_text(\n",
    "    text=multiple_text,\n",
    "    model_name_or_path=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    mini_batch_size=2\n",
    ")\n",
    "\n",
    "print(\"Tag Score Outputs:\\n\")\n",
    "for sentence in sentences:\n",
    "    pprint({sentence.to_original_text(): sentence.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The output is going to be a probility distribution of what the text should be tagged. If you're running this on a GPU, you can specify the `mini_batch_size` parameter to run mini-batch inference against your data for faster run time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set `model_name_or_path` to any of Transformer's or Flair's pre-trained sequence classification models. Transformers models are again located [here](https://huggingface.co/models). You can also pass in the path of a custom trained Transformers model.\n",
    "\n",
    "Let's tag some text with another model, specifically Oliver Guhr's German sentiment model called `oliverguhr/german-sentiment-bert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-31 02:21:39,109 loading file oliverguhr/german-sentiment-bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting text: 100%|██████████| 4/4 [00:00<00:00, 132.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "{'Das hat überhaupt nicht gut funktioniert.': [positive (0.0008),\n",
      "                                               negative (0.9991),\n",
      "                                               neutral (0.0)]}\n",
      "{'Ich mochte es wirklich.': [positive (0.7023),\n",
      "                             negative (0.2029),\n",
      "                             neutral (0.0947)]}\n",
      "{'Es war wirklich nützlich.': [positive (0.9813),\n",
      "                               negative (0.0184),\n",
      "                               neutral (0.0002)]}\n",
      "{'Es ist kaputt gegangen, nachdem ich es gekauft habe.': [positive (0.0042),\n",
      "                                                          negative (0.9957),\n",
      "                                                          neutral (0.0001)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "german_text = [\"Das hat überhaupt nicht gut funktioniert.\",\n",
    "               \"Ich mochte es wirklich.\",\n",
    "               \"Es war wirklich nützlich.\",\n",
    "               \"Es ist kaputt gegangen, nachdem ich es gekauft habe.\"]\n",
    "sentences = classifier.tag_text(\n",
    "    german_text,\n",
    "    model_name_or_path=\"oliverguhr/german-sentiment-bert\",\n",
    "    mini_batch_size=1\n",
    ")\n",
    "\n",
    "print(\"Tag Score Outputs:\\n\")\n",
    "for sentence in sentences:\n",
    "    pprint({sentence.to_original_text(): sentence.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget you can still quickly run inference with the multi-lingual review sentiment model you loaded in earlier(memory permitting)! Just change the `model_name_or_path` param to the model you used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting text: 100%|██████████| 4/4 [00:00<00:00, 107.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "{'Das hat überhaupt nicht gut funktioniert.': [1 star (0.7224),\n",
      "                                               2 stars (0.2326),\n",
      "                                               3 stars (0.0418),\n",
      "                                               4 stars (0.0024),\n",
      "                                               5 stars (0.0008)]}\n",
      "{'Ich mochte es wirklich.': [1 star (0.0092),\n",
      "                             2 stars (0.0097),\n",
      "                             3 stars (0.0582),\n",
      "                             4 stars (0.3038),\n",
      "                             5 stars (0.6191)]}\n",
      "{'Es war wirklich nützlich.': [1 star (0.0124),\n",
      "                               2 stars (0.0158),\n",
      "                               3 stars (0.0853),\n",
      "                               4 stars (0.3754),\n",
      "                               5 stars (0.5111)]}\n",
      "{'Es ist kaputt gegangen, nachdem ich es gekauft habe.': [1 star (0.5459),\n",
      "                                                          2 stars (0.3205),\n",
      "                                                          3 stars (0.12),\n",
      "                                                          4 stars (0.0104),\n",
      "                                                          5 stars (0.0032)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "german_text = [\"Das hat überhaupt nicht gut funktioniert.\",\n",
    "               \"Ich mochte es wirklich.\",\n",
    "               \"Es war wirklich nützlich.\",\n",
    "               \"Es ist kaputt gegangen, nachdem ich es gekauft habe.\"]\n",
    "sentences = classifier.tag_text(\n",
    "    german_text,\n",
    "    model_name_or_path=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    mini_batch_size=1\n",
    ")\n",
    "\n",
    "print(\"Tag Score Outputs:\\n\")\n",
    "for sentence in sentences:\n",
    "    pprint({sentence.to_original_text(): sentence.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's release the german sentiment model to free up some memory for our next step...training! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.release_model(model_name_or_path=\"oliverguhr/german-sentiment-bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train and fine-tune a pre-trained model on your own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine you have your own dataset with text/label pairs you'd like to create a sequence classification model for.\n",
    "\n",
    "With the easy sequence classifier, you can take advantage of transfer learning by fine-tuning pre-trained models on your own custom datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The `EasySequenceClassifier` is integrated heavily with the `datasets.Dataset` and `transformers.Trainer` class objects, so please check out the [datasets](https://huggingface.co/datasets) and [transformers](https://huggingface.co/transformers) documentation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first need a \"custom\" dataset to start training our model. Our `EasySequenceClassifier.train()` method can run with either `datasets.Dataset` objects or CSV data file paths. Since the datasets library makes it so easy, we'll use the `datasets.load_dataset()` method to load in the IMDB Sentiment dataset. We'll show an example with a CSV later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'builder_name': 'imdb',\n",
      " 'citation': '@InProceedings{maas-EtAl:2011:ACL-HLT2011,\\n'\n",
      "             '  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  '\n",
      "             'Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, '\n",
      "             'Christopher},\\n'\n",
      "             '  title     = {Learning Word Vectors for Sentiment Analysis},\\n'\n",
      "             '  booktitle = {Proceedings of the 49th Annual Meeting of the '\n",
      "             'Association for Computational Linguistics: Human Language '\n",
      "             'Technologies},\\n'\n",
      "             '  month     = {June},\\n'\n",
      "             '  year      = {2011},\\n'\n",
      "             '  address   = {Portland, Oregon, USA},\\n'\n",
      "             '  publisher = {Association for Computational Linguistics},\\n'\n",
      "             '  pages     = {142--150},\\n'\n",
      "             '  url       = {http://www.aclweb.org/anthology/P11-1015}\\n'\n",
      "             '}\\n',\n",
      " 'config_name': 'plain_text',\n",
      " 'dataset_size': 133190346,\n",
      " 'description': 'Large Movie Review Dataset.\\n'\n",
      "                'This is a dataset for binary sentiment classification '\n",
      "                'containing substantially more data than previous benchmark '\n",
      "                'datasets. We provide a set of 25,000 highly polar movie '\n",
      "                'reviews for training, and 25,000 for testing. There is '\n",
      "                'additional unlabeled data for use as well.',\n",
      " 'download_checksums': {'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz': {'checksum': 'c40f74a18d3b61f90feba1e17730e0d38e8b97c05fde7008942e91923d1658fe',\n",
      "                                                                                           'num_bytes': 84125825}},\n",
      " 'download_size': 84125825,\n",
      " 'features': {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None),\n",
      "              'text': Value(dtype='string', id=None)},\n",
      " 'homepage': 'http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      " 'license': '',\n",
      " 'post_processed': PostProcessedInfo(features=None, resources_checksums={'train': {}, 'test': {}, 'unsupervised': {}, 'train[:10%]': {}, 'train[:1%]': {}, 'test[:1%]': {}}),\n",
      " 'post_processing_size': 0,\n",
      " 'size_in_bytes': 217316171,\n",
      " 'splits': {'test': SplitInfo(name='test', num_bytes=32650697, num_examples=25000, dataset_name='imdb'),\n",
      "            'train': SplitInfo(name='train', num_bytes=33432835, num_examples=25000, dataset_name='imdb'),\n",
      "            'unsupervised': SplitInfo(name='unsupervised', num_bytes=67106814, num_examples=50000, dataset_name='imdb')},\n",
      " 'supervised_keys': None,\n",
      " 'version': 1.0.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset, eval_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "# Uncomment below if you want to use less data so you don't spend an hour+ on training and evaluation\n",
    "#train_dataset, eval_dataset = load_dataset('imdb', split=['train[:1%]', 'test[:1%]'])\n",
    "\n",
    "pprint(vars(train_dataset.info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a brief look at what the IMDB Sentiment dataset looks like. We can see that the label column has two classes of 0 and 1. You can see the name of the classes mapped to the integers with `train_dataset.features[\"names\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1</td>\n",
       "      <td>That hilarious line is typical of what these n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1</td>\n",
       "      <td>Faith and Mortality... viewed through the lens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1</td>\n",
       "      <td>The unlikely duo of Zero Mostel and Harry Bela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1</td>\n",
       "      <td>*some spoilers*&lt;br /&gt;&lt;br /&gt;I was pleasantly su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1</td>\n",
       "      <td>... and I DO mean it. If not literally (after ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0        1  Bromwell High is a cartoon comedy. It ran at t...\n",
       "1        1  Homelessness (or Houselessness as George Carli...\n",
       "2        1  Brilliant over-acting by Lesley Ann Warren. Be...\n",
       "3        1  This is easily the most underrated film inn th...\n",
       "4        1  This is not the typical Mel Brooks film. It wa...\n",
       "..     ...                                                ...\n",
       "245      1  That hilarious line is typical of what these n...\n",
       "246      1  Faith and Mortality... viewed through the lens...\n",
       "247      1  The unlikely duo of Zero Mostel and Harry Bela...\n",
       "248      1  *some spoilers*<br /><br />I was pleasantly su...\n",
       "249      1  ... and I DO mean it. If not literally (after ...\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.set_format(type=\"pandas\", columns=[\"text\", \"label\"])\n",
    "train_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just run this to reformat back to a 'python' dataset\n",
    "train_dataset.set_format(columns=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment below to see training done with CSV files. The cell below will just save the `datasets.Dataset` objects you have in `train_dataset` and `eval_dataset` as CSVs and will train the model with the CSV file paths. Ignore to just continue to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.set_format(type=\"pandas\", columns=[\"text\", \"label\"])\n",
    "#eval_dataset.set_format(type=\"pandas\", columns=[\"text\", \"label\"])\n",
    "\n",
    "#train_dataset[:].to_csv(\"./IMDB train.csv\", index=False)\n",
    "#eval_dataset[:].to_csv(\"./IMDB eval.csv\", index=False)\n",
    "\n",
    "#train_dataset = \"./IMDB train.csv\"\n",
    "#eval_dataset = \"./IMDB eval.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things we'll need to specify before we start training are the training arguments. Training arguments consist mainly of the hyperparameters we want to provide the model. These may include batch size, initial learning rate, number of epochs, etc.\n",
    "\n",
    "We will be using the `transformers.TrainingArguments` data class to store our training args. These are compatible with the `transformers.Trainer` as well as AdaptNLP's train methods. For more documention on the `TrainingArguments` class, please look [here](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments). There are a lot of arguments available, but we will pass in the important args and use default values for the rest.\n",
    "\n",
    "The training arguments below specify the output directory for you model and checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_dir='./logs',\n",
    "    save_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the built-in `train()` method by passing in the training arguments. The training method will also be where you specify your data arguments which include the your train and eval datasets, the pre-trained model ID (this should have been loaded from your earlier cells, but can be loaded dynamically), text column name, label column name, and ordered label names (only required if loading in paths to CSV data file for dataset args).\n",
    "\n",
    "Please checkout AdaptNLP's package reference for more information [here](https://novetta.github.io/adaptnlp/class-api/sequence-classifier-module.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier.train(training_args=training_args,\n",
    "                 train_dataset=train_dataset,\n",
    "                 eval_dataset=eval_dataset,\n",
    "                 model_name_or_path=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "                 text_col_nm=\"text\",\n",
    "                 label_col_nm=\"label\",\n",
    "                 label_names=[\"positive\",\"negative\"]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you can evaluate the model with the eval dataset you passed in for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(model_name_or_path=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see it's a little weird that we're still using the `model_name_or_path` of the pre-trained model we fine-tuned and took advantage of via. transfer learning. We can release the model we've fine-tuned, and then load it back in using the directory that we've serialized the fine-tuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.release_model(model_name_or_path=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting text: 100%|██████████| 4/4 [00:00<00:00, 122.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "{\"This didn't work well at all.\": [neg (0.7344), pos (0.2656)]}\n",
      "{'I really liked it.': [neg (0.2935), pos (0.7065)]}\n",
      "{'It was really useful.': [neg (0.3237), pos (0.6763)]}\n",
      "{'It broke after I bought it.': [neg (0.6209), pos (0.3791)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = classifier.tag_text(\n",
    "    multiple_text,\n",
    "    model_name_or_path=\"./models\",\n",
    "    mini_batch_size=1\n",
    ")\n",
    "\n",
    "print(\"Tag Score Outputs:\\n\")\n",
    "for sentence in sentences:\n",
    "    pprint({sentence.to_original_text(): sentence.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
