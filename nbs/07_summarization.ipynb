{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Title (change me)\n",
    "> Default description (change me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    T5ForConditionalGeneration,\n",
    "    BartForConditionalGeneration,\n",
    ")\n",
    "\n",
    "from adaptnlp.callback import GeneratorCallback\n",
    "from adaptnlp.model import AdaptiveModel\n",
    "\n",
    "from fastcore.basics import store_attr\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "from fastai_minima.callback.core import Callback, CancelBatchException\n",
    "from fastai_minima.utils import apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersSummarizer(AdaptiveModel):\n",
    "    \"\"\"Adaptive model for Transformer's Conditional Generation or Language Models (Transformer's T5 and Bart\n",
    "    conditiional generation models have a language modeling head)\n",
    "\n",
    "    Usage:\n",
    "    ```python\n",
    "    >>> summarizer = TransformersSummarizer.load(\"transformers-summarizer-model\")\n",
    "    >>> summarizer.predict(text=\"Example text\", mini_batch_size=32)\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **tokenizer** - A tokenizer object from Huggingface's transformers (TODO)and tokenizers\n",
    "    * **model** - A transformers Conditional Generation (Bart or T5) or Language model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n",
    "        # Load up tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        super().__init__()\n",
    "        # Sets internal model\n",
    "        super().set_model(model)\n",
    "\n",
    "        # Set inputs to come in as `dict`\n",
    "        super().set_as_dict(True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading and constructing this classifier\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Transformer's pre-trained Summarizer Model\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "        summarizer = cls(tokenizer, model)\n",
    "        return summarizer\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        num_beams: int = 4,\n",
    "        min_length: int = 0,\n",
    "        max_length: int = 128,\n",
    "        early_stopping: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **num_beams** - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search.  Default to 4.\n",
    "        * **min_length** -  The min length of the sequence to be generated. Default to 0\n",
    "        * **max_length** - The max length of the sequence to be generated. Between min_length and infinity. Default to 128\n",
    "        * **early_stopping** - if set to True beam search is stopped when at least num_beams sentences finished per batch.\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n",
    "        \"\"\"\n",
    "\n",
    "        # Make all inputs list\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "\n",
    "        # T5 requires 'summarize: ' precursor text for pre-trained summarizer\n",
    "        if isinstance(self.model, T5ForConditionalGeneration):\n",
    "            text = [f'summarize: {t}' for t in text]\n",
    "\n",
    "        dataset = self._tokenize(text)\n",
    "        dl = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "\n",
    "        summaries = []\n",
    "\n",
    "        logger.info(f'Running summarizer on {len(dataset)} text sequences')\n",
    "        logger.info(f'Batch size = {mini_batch_size}')\n",
    "\n",
    "        cb = GeneratorCallback(num_beams, min_length, max_length, early_stopping, **kwargs)\n",
    "\n",
    "        preds,_ = super().get_preds(dl=dl, cbs=[cb])\n",
    "\n",
    "        preds = apply(lambda x: x.squeeze(0), preds)\n",
    "\n",
    "        for o in preds:\n",
    "            summaries.append(\n",
    "                [\n",
    "                    self.tokenizer.decode(\n",
    "                        o,\n",
    "                        skip_special_tokens=True,\n",
    "                        clean_up_tokenization_spaces=False,\n",
    "                    )\n",
    "                ].pop()\n",
    "            )\n",
    "\n",
    "        return summaries\n",
    "\n",
    "    def _tokenize(self, text: Union[List[str], str]) -> TensorDataset:\n",
    "        \"\"\" Batch tokenizes text and produces a `TensorDataset` with text \"\"\"\n",
    "\n",
    "        # Pre-trained Bart summarization model has a max length fo 1024 tokens for input\n",
    "        if isinstance(self.model, BartForConditionalGeneration):\n",
    "            tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=1024,\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "        else:\n",
    "            tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                add_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        # Bart doesn't use `token_type_ids`\n",
    "        dataset = TensorDataset(\n",
    "            tokenized_text[\"input_ids\"],\n",
    "            tokenized_text[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasySummarizer:\n",
    "    \"\"\"Summarization Module\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> summarizer = EasySummarizer()\n",
    "    >>> summarizer.summarize(text=\"Summarize this text\", model_name_or_path=\"t5-small\")\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.summarizers: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "\n",
    "    def summarize(\n",
    "        self,\n",
    "        text: Union[List[str], str],\n",
    "        model_name_or_path: str = \"t5-small\",\n",
    "        mini_batch_size: int = 32,\n",
    "        num_beams: int = 4,\n",
    "        min_length: int = 0,\n",
    "        max_length: int = 128,\n",
    "        early_stopping: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **model_name_or_path** - A String model id or path to a pre-trained model repository or custom trained model directory\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **num_beams** - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search.  Default to 4.\n",
    "        * **min_length** -  The min length of the sequence to be generated. Default to 0\n",
    "        * **max_length** - The max length of the sequence to be generated. Between min_length and infinity. Default to 128\n",
    "        * **early_stopping** - if set to True beam search is stopped when at least num_beams sentences finished per batch.\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n",
    "        \"\"\"\n",
    "        if not self.summarizers[model_name_or_path]:\n",
    "            self.summarizers[model_name_or_path] = TransformersSummarizer.load(\n",
    "                model_name_or_path\n",
    "            )\n",
    "\n",
    "        summarizer = self.summarizers[model_name_or_path]\n",
    "        return summarizer.predict(\n",
    "            text=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            num_beams=num_beams,\n",
    "            min_length=min_length,\n",
    "            max_length=max_length,\n",
    "            early_stopping=early_stopping,\n",
    "            **kwargs,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
