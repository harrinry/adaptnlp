{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "> Performing summarization within the AdaptNLP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    T5ForConditionalGeneration,\n",
    "    BartForConditionalGeneration,\n",
    ")\n",
    "\n",
    "from adaptnlp.callback import GeneratorCallback\n",
    "from adaptnlp.model import AdaptiveModel\n",
    "\n",
    "from fastcore.basics import store_attr\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "from fastai_minima.callback.core import Callback, CancelBatchException\n",
    "from fastai_minima.utils import apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersSummarizer(AdaptiveModel):\n",
    "    \"\"\"Adaptive model for Transformer's Conditional Generation or Language Models (Transformer's T5 and Bart\n",
    "    conditiional generation models have a language modeling head)\n",
    "\n",
    "    Usage:\n",
    "    ```python\n",
    "    >>> summarizer = TransformersSummarizer.load(\"transformers-summarizer-model\")\n",
    "    >>> summarizer.predict(text=\"Example text\", mini_batch_size=32)\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **tokenizer** - A tokenizer object from Huggingface's transformers (TODO)and tokenizers\n",
    "    * **model** - A transformers Conditional Generation (Bart or T5) or Language model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n",
    "        # Load up tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        super().__init__()\n",
    "        # Sets internal model\n",
    "        super().set_model(model)\n",
    "\n",
    "        # Set inputs to come in as `dict`\n",
    "        super().set_as_dict(True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading and constructing this classifier\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Transformer's pre-trained Summarizer Model\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "        summarizer = cls(tokenizer, model)\n",
    "        return summarizer\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        num_beams: int = 4,\n",
    "        min_length: int = 0,\n",
    "        max_length: int = 128,\n",
    "        early_stopping: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **num_beams** - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search.  Default to 4.\n",
    "        * **min_length** -  The min length of the sequence to be generated. Default to 0\n",
    "        * **max_length** - The max length of the sequence to be generated. Between min_length and infinity. Default to 128\n",
    "        * **early_stopping** - if set to True beam search is stopped when at least num_beams sentences finished per batch.\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n",
    "        \"\"\"\n",
    "\n",
    "        # Make all inputs list\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "\n",
    "        # T5 requires 'summarize: ' precursor text for pre-trained summarizer\n",
    "        if isinstance(self.model, T5ForConditionalGeneration):\n",
    "            text = [f'summarize: {t}' for t in text]\n",
    "\n",
    "        dataset = self._tokenize(text)\n",
    "        dl = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "\n",
    "        summaries = []\n",
    "\n",
    "        logger.info(f'Running summarizer on {len(dataset)} text sequences')\n",
    "        logger.info(f'Batch size = {mini_batch_size}')\n",
    "\n",
    "        cb = GeneratorCallback(num_beams, min_length, max_length, early_stopping, **kwargs)\n",
    "\n",
    "        preds,_ = super().get_preds(dl=dl, cbs=[cb])\n",
    "\n",
    "        preds = apply(lambda x: x.squeeze(0), preds)\n",
    "\n",
    "        for o in preds:\n",
    "            summaries.append(\n",
    "                [\n",
    "                    self.tokenizer.decode(\n",
    "                        o,\n",
    "                        skip_special_tokens=True,\n",
    "                        clean_up_tokenization_spaces=False,\n",
    "                    )\n",
    "                ].pop()\n",
    "            )\n",
    "\n",
    "        return summaries\n",
    "\n",
    "    def _tokenize(self, text: Union[List[str], str]) -> TensorDataset:\n",
    "        \"\"\" Batch tokenizes text and produces a `TensorDataset` with text \"\"\"\n",
    "\n",
    "        # Pre-trained Bart summarization model has a max length fo 1024 tokens for input\n",
    "        if isinstance(self.model, BartForConditionalGeneration):\n",
    "            tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=1024,\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "        else:\n",
    "            tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                add_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        # Bart doesn't use `token_type_ids`\n",
    "        dataset = TensorDataset(\n",
    "            tokenized_text[\"input_ids\"],\n",
    "            tokenized_text[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasySummarizer:\n",
    "    \"\"\"Summarization Module\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> summarizer = EasySummarizer()\n",
    "    >>> summarizer.summarize(text=\"Summarize this text\", model_name_or_path=\"t5-small\")\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.summarizers: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "\n",
    "    def summarize(\n",
    "        self,\n",
    "        text: Union[List[str], str],\n",
    "        model_name_or_path: str = \"t5-small\",\n",
    "        mini_batch_size: int = 32,\n",
    "        num_beams: int = 4,\n",
    "        min_length: int = 0,\n",
    "        max_length: int = 128,\n",
    "        early_stopping: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **model_name_or_path** - A String model id or path to a pre-trained model repository or custom trained model directory\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **num_beams** - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search.  Default to 4.\n",
    "        * **min_length** -  The min length of the sequence to be generated. Default to 0\n",
    "        * **max_length** - The max length of the sequence to be generated. Between min_length and infinity. Default to 128\n",
    "        * **early_stopping** - if set to True beam search is stopped when at least num_beams sentences finished per batch.\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n",
    "        \"\"\"\n",
    "        name = getattr(model_name_or_path, 'name', model_name_or_path)\n",
    "        if not self.summarizers[name]:\n",
    "            self.summarizers[name] = TransformersSummarizer.load(\n",
    "                name\n",
    "            )\n",
    "\n",
    "        summarizer = self.summarizers[name]\n",
    "        return summarizer.predict(\n",
    "            text=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            num_beams=num_beams,\n",
    "            min_length=min_length,\n",
    "            max_length=max_length,\n",
    "            early_stopping=early_stopping,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "# Text from encyclopedia Britannica on Einstein\n",
    "text = [\"\"\"Einstein’s education was disrupted by his father’s repeated failures at business. In 1894, after his company failed to get an important \n",
    "          contract to electrify the city of Munich, Hermann Einstein moved to Milan to work with a relative. Einstein was left at a boardinghouse in \n",
    "          Munich and expected to finish his education. Alone, miserable, and repelled by the looming prospect of military duty when he turned 16, Einstein \n",
    "          ran away six months later and landed on the doorstep of his surprised parents. His parents realized the enormous problems that he faced as a \n",
    "          school dropout and draft dodger with no employable skills. His prospects did not look promising.\n",
    "          Fortunately, Einstein could apply directly to the Eidgenössische Polytechnische Schule (“Swiss Federal Polytechnic School”; in 1911, \n",
    "          following expansion in 1909 to full university status, it was renamed the Eidgenössische Technische Hochschule, or “Swiss Federal \n",
    "          Institute of Technology”) in Zürich without the equivalent of a high school diploma if he passed its stiff entrance examinations. His marks \n",
    "          showed that he excelled in mathematics and physics, but he failed at French, chemistry, and biology. Because of his exceptional math scores, \n",
    "          he was allowed into the polytechnic on the condition that he first finish his formal schooling. He went to a special high school run by \n",
    "          Jost Winteler in Aarau, Switzerland, and graduated in 1896. He also renounced his German citizenship at that time. (He was stateless until 1901, \n",
    "          when he was granted Swiss citizenship.) He became lifelong friends with the Winteler family, with whom he had been boarding. (Winteler’s \n",
    "          daughter, Marie, was Einstein’s first love; Einstein’s sister, Maja, would eventually marry Winteler’s son Paul; and his close friend Michele \n",
    "          Besso would marry their eldest daughter, Anna.)\"\"\",\n",
    "       \"\"\"Einstein would write that two “wonders” deeply affected his early years. The first was his encounter with a compass at age five. \n",
    "          He was mystified that invisible forces could deflect the needle. This would lead to a lifelong fascination with invisible forces. \n",
    "          The second wonder came at age 12 when he discovered a book of geometry, which he devoured, calling it his 'sacred little geometry \n",
    "          book'. Einstein became deeply religious at age 12, even composing several songs in praise of God and chanting religious songs on \n",
    "          the way to school. This began to change, however, after he read science books that contradicted his religious beliefs. This challenge \n",
    "          to established authority left a deep and lasting impression. At the Luitpold Gymnasium, Einstein often felt out of place and victimized \n",
    "          by a Prussian-style educational system that seemed to stifle originality and creativity. One teacher even told him that he would \n",
    "          never amount to anything.\"\"\"]\n",
    "\n",
    "summarizer = EasySummarizer()\n",
    "summaries = summarizer.summarize(text = text, model_name_or_path=\"t5-small\", mini_batch_size=1, num_beams = 4, min_length=0, max_length=100, early_stopping=True)\n",
    "test_eq(summaries, ['Hermann Einstein was left at a boardinghouse and expected to finish his education . he ran away six months later and landed on the doorstep of his surprised parents . he could apply directly to the Eidgenössische Polytechnische Schule without the equivalent of a high school diploma .',\n",
    " 'Einstein was mystified that invisible forces could deflect the needle . the second wonder came at age 12 when he discovered a book of geometry . he became deeply religious at age 12 .'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "summaries = summarizer.summarize(text = text, model_name_or_path=\"facebook/bart-large-cnn\", mini_batch_size=1, num_beams = 2, min_length=40, max_length=300, early_stopping=True)\n",
    "test_eq(summaries, ['Einstein’s education was disrupted by his father’S repeated failures at business. In 1894, after his company failed to get an important contract, Einstein moved to Milan to work with a relative. Einstein was left at a boardinghouse in Munich and expected to finish his education.',\n",
    " 'Einstein would write that two ‘wonders’ deeply affected his early years. The first was his encounter with a compass at age five. The second wonder came at age 12 when he discovered a book of geometry.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from adaptnlp.model_hub import HFModelHub\n",
    "hub = HFModelHub()\n",
    "models = hub.search_model_by_task('summarization')\n",
    "model = models[-1]\n",
    "summaries = summarizer.summarize(text = text, model_name_or_path=model, mini_batch_size=1, num_beams = 2, min_length=40, max_length=300, early_stopping=True)\n",
    "test_eq(summaries, ['Swiss Federal Poly Polytechnic on the condition thatXtechnic and:”; in 1911, following expansion in 1909 to full university status, it wasX de eventually marry Winteler in Aarau, Switzerland, and graduated in 18X son Paul; and his close friend Michele Besso would marry their marry theirX de marry their their marry, Anna.))esteldest daughterX',\n",
    " 'would wonder came at this age 12 when de The second wonder came came at age 12Xul early years. The first was his encounter withacompass at age five. He was mys with invisible forces could deflect the needle. This would lead toa lifelong fascination with invisibleX: EinsteinX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
