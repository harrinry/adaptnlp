{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff75476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f44b3",
   "metadata": {},
   "source": [
    "# Data\n",
    "> The data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796705d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caad732",
   "metadata": {},
   "source": [
    "## The API\n",
    "\n",
    "AdaptNLP's data api takes inspiration from [fastai](https://docs.fast.ai)'s `DataBlock` API, as well has has partial compatibility with it. \n",
    "\n",
    "There is a medium and high-level API through `TaskDataset`, and individual task wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from transformers import AutoTokenizer, default_data_collator\n",
    "\n",
    "from fastcore.foundation import mask2idxs, L\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.xtras import Path, range_of\n",
    "from fastai.data.core import DataLoaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "import os, torch\n",
    "from functools import partial\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee02aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ColReader:\n",
    "    \"\"\"\n",
    "    Reads `cols` in `row` with potential `pref` and `suff`\n",
    "    Based on the fastai class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cols, # Some column names to use\n",
    "        pref:str='', # A prefix\n",
    "        suff:str='', # A suffix\n",
    "        label_delim:str=None, # A label delimiter\n",
    "    ):\n",
    "        self.pref = str(pref) + os.path.sep if isinstance(pref, Path) else pref\n",
    "        self.suff, self.label_delim = suff, label_delim\n",
    "        self.cols = L(cols)\n",
    "    \n",
    "    def _do_one(self, r, c):\n",
    "        o = r[c] if isinstance(c,int) else r[c] if c=='name' or c=='cat' else getattr(r,c)\n",
    "        if len(self.pref)==0 and len(self.suff)==0 and self.label_delim is None: return o\n",
    "        if self.label_delim is None: return f'{self.pref}{o}{self.suff}'\n",
    "        else: return o.split(self.label_delim) if len(o)>0 else []\n",
    "    \n",
    "    def __call__(self, o):\n",
    "        if len(self.cols) == 1: return self._do_one(o, self.cols[0])\n",
    "        return L(self._do_one(o,c) for c in self.cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b27246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Categorize:\n",
    "    \"\"\"\n",
    "    Collection of categories with reverse mapping in `o2i`\n",
    "    Based on the fastai class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        names, # An interable collection of items to create a vocab from\n",
    "        sort=True # Whether to make the items sorted\n",
    "    ):\n",
    "        names = L(names)\n",
    "        self.classes = L(o for o in names.unique() if o == o)\n",
    "        if sort: self.classes = self.classes.sorted()\n",
    "        self.o2i = dict(self.classes.val2idx())\n",
    "        \n",
    "    def map_objs(\n",
    "        self, \n",
    "        objs # Some iterable collection\n",
    "    ):\n",
    "        \"Map `objs` to IDs\"\n",
    "        return L(self.o2i[o] for o in objs)\n",
    "    \n",
    "    def map_ids(\n",
    "        self, \n",
    "        ids # Some ids correlating to `self.classes`\n",
    "    ):\n",
    "        \"Map `ids` to objects in vocab\"\n",
    "        return L(self.classes[o] for o in ids)\n",
    "    \n",
    "    def __call__(self, o): return int(self.o2i[o])\n",
    "    \n",
    "    def decode(self, o): return self.classes[o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def RandomSplitter(valid_pct=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Creates a function that splits some items between train and validation with `valid_pct` randomly\n",
    "    \"\"\"\n",
    "    def _inner(o):\n",
    "        if seed is not None: torch.manual_seed(seed)\n",
    "        rand_idx = L(list(torch.randperm(len(o)).numpy()))\n",
    "        cut = int(valid_pct * len(o))\n",
    "        return rand_idx[cut:], rand_idx[:cut]\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc93b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TaskDatasets:\n",
    "    \"\"\"\n",
    "    A set of datasets for a particular task, with a simple API.\n",
    "    \n",
    "    Note: This is the base API, `items` should be a set of regular text and model-ready labels,\n",
    "          including label or one-hot encoding being applied.\n",
    "    \"\"\"\n",
    "    @delegates(AutoTokenizer.from_pretrained)\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dset, # A train `Dataset` object\n",
    "        valid_dset, # A validation `Dataset` object\n",
    "        tokenizer_name:str = None, # The string name of a `HuggingFace` tokenizer or model. If `None`, will not tokenize the dataset.\n",
    "        tokenize:bool = True, # Whether to tokenize the dataset immediatly\n",
    "        **kwargs, # kwargs to go to `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        self.train = train_dset\n",
    "        self.valid = valid_dset\n",
    "        self.tokenizer = None\n",
    "        if tokenizer_name is not None: self.set_tokenizer(tokenizer_name, **kwargs)\n",
    "        if tokenize and self.tokenizer is not None: self._tokenize()\n",
    "        elif tokenize and self.tokenizer is None:\n",
    "            print(\"Tried to tokenize a dataset without a tokenizer. Please set a tokenizer with `set_tokenizer` and call `_tokenize()`\")\n",
    "\n",
    "    def __getitem__(self, idx): return self.train[idx]\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        \"Tokenize dataset in `self.items`\"\n",
    "        if not self.tokenizer: raise ValueError(\"Tried to tokenize a dataset without a tokenizer. Please add a tokenizer with `set_tokenizer(tokenizer_name` and try again\")\n",
    "        def _inner(item):return self.tokenizer(item['text'], padding=True, truncation=True)\n",
    "        self.train = self.train.map(_inner,batched=True,remove_columns = self.train.column_names)\n",
    "        self.valid = self.valid.map(_inner,batched=True,remove_columns = self.valid.column_names)\n",
    "    \n",
    "    @delegates(AutoTokenizer.from_pretrained)\n",
    "    def set_tokenizer(\n",
    "        self,\n",
    "        tokenizer_name:str, # A string name of a `HuggingFace` tokenizer or model\n",
    "        override_existing:bool = False, # Whether to override an existing tokenizer\n",
    "        **kwargs # kwargs to go to `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        \"Sets a new `AutoTokenizer` to `self.tokenizer`\"\n",
    "        if self.tokenizer and not override_existing:\n",
    "            print(f'Warning! You are trying to override an existing tokenizer: {self.tokenizer.name_or_path}. Pass `override_existing=True` to use a new tokenizer')\n",
    "            return\n",
    "        elif self.tokenizer and override_existing:\n",
    "            print(f'Setting new tokenizer to {tokenizer_name}')\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, **kwargs)\n",
    "        except:\n",
    "            raise ValueError(f'{tokenizer_name} is not a valid pretrained model on the HuggingFace Hub or a local model')\n",
    "    \n",
    "    @delegates(DataLoaders)\n",
    "    def dataloaders(\n",
    "        self, \n",
    "        batch_size=8, # A batch size\n",
    "        shuffle_train=True, # Whether to shuffle the training dataset\n",
    "        collate_fn = default_data_collator, # A custom collation function\n",
    "        **kwargs): # Torch DataLoader kwargs\n",
    "        \"Creates `DataLoaders` from the dataset\"\n",
    "        train_dl = DataLoader(self.train, shuffle=shuffle_train, collate_fn=collate_fn, batch_size=batch_size, **kwargs)\n",
    "        valid_dl = DataLoader(self.valid, shuffle=False, collate_fn=collate_fn, batch_size=batch_size, **kwargs)\n",
    "        return DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceClassificationDatasets(TaskDatasets):\n",
    "    \"\"\"\n",
    "    A set of datasets designed for sequence classification\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        items, # Some items we can pull x's and y's from\n",
    "        get_x = ColReader('text'), # A function taking in one item and extracting the text\n",
    "        get_y = ColReader('label'), # A function taking in one item and extracting the label(s)\n",
    "        splits = None, # Indexs to split the data from\n",
    "        tokenizer_name:str = None, # The string name of a HuggingFace tokenizer or model. If `None`, will not tokenize immediatly\n",
    "        tokenize:bool=True, # Whether to tokenize the dataset immediatly\n",
    "        **kwargs # kwargs to go to `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        xs = L(L(items).map(get_x)[0].values, use_list=True)\n",
    "        ys = L(L(items).map(get_y)[0].values, use_list=True)\n",
    "        self.categorize = Categorize(ys)\n",
    "        ys = L([self.categorize(y) for y in ys], use_list=True)\n",
    "        train_xs, train_ys = xs[splits[0]], ys[splits[0]]\n",
    "        valid_xs, valid_ys = xs[splits[1]], ys[splits[1]]\n",
    "        \n",
    "        train_dset = Dataset.from_dict({\n",
    "            'text':train_xs,\n",
    "            'label':train_ys\n",
    "        })\n",
    "        \n",
    "        valid_dset = Dataset.from_dict({\n",
    "            'text':valid_xs,\n",
    "            'label':valid_ys\n",
    "        })\n",
    "        \n",
    "        super().__init__(train_dset, valid_dset, tokenizer_name, tokenize, **kwargs)\n",
    "        \n",
    "    \n",
    "    @delegates(AutoTokenizer.from_pretrained)\n",
    "    @classmethod\n",
    "    def from_df(\n",
    "        cls,\n",
    "        df:pd.DataFrame, # A Pandas Dataframe or Path to a DataFrame\n",
    "        text_col:str = 'text', # Name of the column the text is stored\n",
    "        label_col:str = 'labels', # Name of the column the label(s) are stored\n",
    "        splits = None, # Indexes to split the data with\n",
    "        tokenizer_name:str = None, # The string name of a huggingFace tokenizer or model. If `None`, will not tokenize the dataset\n",
    "        tokenize:bool = True, # Whether to tokenize the dataset immediatly\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"Builds `SequenceClassificationDatasets` from a `DataFrame` or file path\"\n",
    "        get_x = ColReader(text_col)\n",
    "        get_y = ColReader(label_col)\n",
    "        if splits is None: splits = RandomSplitter(0.2)(range_of(df))\n",
    "        return cls(df, get_x, get_y, splits, tokenizer_name, tokenize, **kwargs)\n",
    "    \n",
    "    @delegates(DataLoaders)\n",
    "    def dataloaders(\n",
    "        self, \n",
    "        batch_size=8, # A batch size\n",
    "        shuffle_train=True, # Whether to shuffle the training dataset\n",
    "        collate_fn = default_data_collator, # A custom collation function\n",
    "        **kwargs): # Torch DataLoader kwargs\n",
    "        dls = super().dataloaders(batch_size, shuffle_train, collate_fn, **kwargs)\n",
    "        dls[0].categorize = self.categorize\n",
    "        return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67445879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568e2009685a435b851e1709c1644e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727c4ce8d1f5446aadc96bca4827df32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "from fastai.data.external import URLs, untar_data\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "dset = SequenceClassificationDatasets.from_df(\n",
    "    df,\n",
    "    'text',\n",
    "    'label',\n",
    "    tokenizer_name = \"bert-base-uncased\",\n",
    "    tokenize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42611b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _group_texts(examples, block_size):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LanguageModelDatasets(TaskDatasets):\n",
    "    \"\"\"\n",
    "    A set of datasets designed for language model fine-tuning\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        items, # Some items we can pull x's and y's from\n",
    "        get_x = ColReader('text'), # A function taking in one item and extracting the text\n",
    "        block_size:int = 512, # A block size to split up the data with\n",
    "        splits = None, # Indexs to split the data from\n",
    "        tokenizer_name:str = None, # The string name of a HuggingFace tokenizer or model. If `None`, will not tokenize immediatly\n",
    "        tokenize:bool=True, # Whether to tokenize the dataset immediatly\n",
    "        **kwargs # kwargs to go to `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        xs = L(L(items).map(get_x)[0].values, use_list=True)\n",
    "        train_xs = xs[splits[0]]\n",
    "        valid_xs = xs[splits[1]]\n",
    "        \n",
    "        train_dset = Dataset.from_dict({\n",
    "            'text':train_xs.items\n",
    "        })\n",
    "        \n",
    "        valid_dset = Dataset.from_dict({\n",
    "            'text':valid_xs.items\n",
    "        })\n",
    "        \n",
    "        super().__init__(train_dset, valid_dset, tokenizer_name, tokenize, **kwargs)\n",
    "        self.block_size = block_size\n",
    "        f = partial(_group_texts, block_size=self.block_size)\n",
    "        self.train = self.train.map(f, batched=True)\n",
    "        self.valid = self.valid.map(f, batched=True)\n",
    "        \n",
    "    @delegates(AutoTokenizer.from_pretrained)\n",
    "    @classmethod\n",
    "    def from_df(\n",
    "        cls,\n",
    "        df:pd.DataFrame, # A Pandas Dataframe or Path to a DataFrame\n",
    "        text_col:str = 'text', # Name of the column the text is stored\n",
    "        splits = None, # Indexes to split the data with\n",
    "        block_size:int = 512, # A block size to split up the data with\n",
    "        tokenizer_name:str = None, # The string name of a huggingFace tokenizer or model. If `None`, will not tokenize the dataset\n",
    "        tokenize:bool = True, # Whether to tokenize the dataset immediatly\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"Builds `SequenceClassificationDatasets` from a `DataFrame` or file path\"\n",
    "        get_x = ColReader(text_col)\n",
    "        if splits is None: splits = RandomSplitter(0.2)(range_of(df))\n",
    "        return cls(df, get_x, block_size, splits, tokenizer_name, tokenize, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bad70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9f3ba14ee943389e17dae3a221da0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af0723e0480413292493e5bcf7000a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85656159fe3453a9f41a45365aba082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08fe2cb17414076809c4d10f429d12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "dsets = LanguageModelDatasets.from_df(\n",
    "    df,\n",
    "    'text',\n",
    "    tokenizer_name = \"bert-base-uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f20b68c",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fa343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_file_utils.ipynb.\n",
      "Converted 01_callback.ipynb.\n",
      "Converted 02_model_hub.ipynb.\n",
      "Converted 03_model.ipynb.\n",
      "Converted 04_embeddings.ipynb.\n",
      "Converted 04a_tutorial.embeddings.ipynb.\n",
      "Converted 05_token_classification.ipynb.\n",
      "Converted 05a_tutorial.token_tagging.ipynb.\n",
      "Converted 06_sequence_classification.ipynb.\n",
      "Converted 06a_tutorial.easy_sequence_classifier.ipynb.\n",
      "Converted 07_summarization.ipynb.\n",
      "Converted 07a_tutorial.summarization.ipynb.\n",
      "Converted 08_translation.ipynb.\n",
      "Converted 08a_tutorial.translation.ipynb.\n",
      "Converted 09_text_generation.ipynb.\n",
      "Converted 09a_tutorial.easy_text_generator.ipynb.\n",
      "Converted 10_question_answering.ipynb.\n",
      "Converted 10a_tutorial.question_answering.ipynb.\n",
      "Converted 13a_transformers.squad_metrics.ipynb.\n",
      "Converted 14_result.ipynb.\n",
      "Converted 15_training.data.ipynb.\n",
      "Converted 16_training.tuner.ipynb.\n",
      "Converted 20_tutorial.tuner.sequence_classification.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23aca68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
