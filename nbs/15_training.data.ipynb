{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff75476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f44b3",
   "metadata": {},
   "source": [
    "# Data\n",
    "> The data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796705d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caad732",
   "metadata": {},
   "source": [
    "## The API\n",
    "\n",
    "AdaptNLP's data api takes inspiration from [fastai](https://docs.fast.ai)'s `DataBlock` API, as well has has partial compatibility with it. \n",
    "\n",
    "There is a medium and high-level API through `TaskDataset`, and individual task wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from fastcore.foundation import mask2idxs, L\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.xtras import Path, range_of\n",
    "from fastai.data.core import DataLoaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "import os, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee02aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ColReader:\n",
    "    \"\"\"\n",
    "    Reads `cols` in `row` with potential `pref` and `suff`\n",
    "    Based on the fastai class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cols, # Some column names to use\n",
    "        pref:str='', # A prefix\n",
    "        suff:str='', # A suffix\n",
    "        label_delim:str=None, # A label delimiter\n",
    "    ):\n",
    "        self.pref = str(pref) + os.path.sep if isinstance(pref, Path) else pref\n",
    "        self.suff, self.label_delim = suff, label_delim\n",
    "        self.cols = L(cols)\n",
    "    \n",
    "    def _do_one(self, r, c):\n",
    "        o = r[c] if isinstance(c,int) else r[c] if c=='name' or c=='cat' else getattr(r,c)\n",
    "        if len(self.pref)==0 and len(self.suff)==0 and self.label_delim is None: return o\n",
    "        if self.label_delim is None: return f'{self.pref}{o}{self.suff}'\n",
    "        else: return o.split(self.label_delim) if len(o)>0 else []\n",
    "    \n",
    "    def __call__(self, o):\n",
    "        if len(self.cols) == 1: return self._do_one(o, self.cols[0])\n",
    "        return L(self._do_one(o,c) for c in self.cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b27246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Categorize:\n",
    "    \"\"\"\n",
    "    Collection of categories with reverse mapping in `o2i`\n",
    "    Based on the fastai class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        names, # An interable collection of items to create a vocab from\n",
    "        sort=True # Whether to make the items sorted\n",
    "    ):\n",
    "        names = L(names)\n",
    "        self.classes = L(o for o in names.unique() if o == o)\n",
    "        if sort: self.classes = self.classes.sorted()\n",
    "        self.o2i = dict(self.classes.val2idx())\n",
    "        \n",
    "    def map_objs(\n",
    "        self, \n",
    "        objs # Some iterable collection\n",
    "    ):\n",
    "        \"Map `objs` to IDs\"\n",
    "        return L(self.o2i[o] for o in objs)\n",
    "    \n",
    "    def map_ids(\n",
    "        self, \n",
    "        ids # Some ids correlating to `self.classes`\n",
    "    ):\n",
    "        \"Map `ids` to objects in vocab\"\n",
    "        return L(self.classes[o] for o in ids)\n",
    "    \n",
    "    def __call__(self, o): return torch.tensor(self.o2i[o])\n",
    "    \n",
    "    def decode(self, o): return self.classes[o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def RandomSplitter(valid_pct=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Creates a function that splits some items between train and validation with `valid_pct` randomly\n",
    "    \"\"\"\n",
    "    def _inner(o):\n",
    "        if seed is not None: torch.manual_seed(seed)\n",
    "        rand_idx = L(list(torch.randperm(len(o)).numpy()))\n",
    "        cut = int(valid_pct * len(o))\n",
    "        return rand_idx[cut:], rand_idx[:cut]\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc93b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TaskDatasets:\n",
    "    \"\"\"\n",
    "    A set of datasets for a particular task, with a simple API.\n",
    "    \n",
    "    Note: This is the base API, `items` should be a set of regular text and model-ready labels,\n",
    "          including label or one-hot encoding being applied.\n",
    "    \"\"\"\n",
    "    @delegates(AutoTokenizer.from_pretrained)\n",
    "    def __init__(\n",
    "        self,\n",
    "        items, # An array of (input, label)\n",
    "        splits = None, # Indexes to split the data with\n",
    "        tokenizer_name:str = None, # The string name of a `HuggingFace` tokenizer or model. If `None`, will not tokenize the dataset.\n",
    "        tokenize:bool = True, # Whether to tokenize the dataset immediatly\n",
    "        **kwargs, # kwargs to go to `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        self.items = L(items)\n",
    "        splits = L([slice(None), []] if splits is None else splits).map(mask2idxs)\n",
    "        self.train_idxs, self.valid_idxs = splits[0], splits[1]\n",
    "        self.tokenizer = None\n",
    "        if tokenizer_name is not None: self.set_tokenizer(tokenizer_name, **kwargs)\n",
    "        if tokenize and self.tokenizer is not None: self._tokenize()\n",
    "        elif tokenize and self.tokenizer is None:\n",
    "            print(\"Tried to tokenize a dataset without a tokenizer. Please set a tokenizer with `set_tokenizer` and call `_tokenize()`\")\n",
    "\n",
    "    def __getitem__(self, idx): return self.items[idx]\n",
    "    \n",
    "    @property\n",
    "    def train(self): return self.items[self.train_idxs]\n",
    "    \n",
    "    @property\n",
    "    def valid(self): return self.items[self.train_idxs]\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        \"Tokenize dataset in `self.items`\"\n",
    "        if not self.tokenizer: raise ValueError(\"Tried to tokenize a dataset without a tokenizer. Please add a tokenizer with `set_tokenizer(tokenizer_name` and try again\")\n",
    "        def _inner(item): \n",
    "            if len(item) > 1:\n",
    "                data = self.tokenizer(item[0], padding=True, truncation=True)\n",
    "                data['labels'] = item[1]\n",
    "                return data\n",
    "            else: return self.tokenizer(item[0], padding=True, truncation=True)\n",
    "        self.items = self.items.map(_inner)\n",
    "    \n",
    "    @delegates(AutoTokenizer.from_pretrained)\n",
    "    def set_tokenizer(\n",
    "        self,\n",
    "        tokenizer_name:str, # A string name of a `HuggingFace` tokenizer or model\n",
    "        override_existing:bool = False, # Whether to override an existing tokenizer\n",
    "        **kwargs # kwargs to go to `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        \"Sets a new `AutoTokenizer` to `self.tokenizer`\"\n",
    "        if self.tokenizer and not override_existing:\n",
    "            print(f'Warning! You are trying to override an existing tokenizer: {self.tokenizer.name_or_path}. Pass `override_existing=True` to use a new tokenizer')\n",
    "            return\n",
    "        elif self.tokenizer and override_existing:\n",
    "            print(f'Setting new tokenizer to {tokenizer_name}')\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, **kwargs)\n",
    "        except:\n",
    "            raise ValueError(f'{tokenizer_name} is not a valid pretrained model on the HuggingFace Hub or a local model')\n",
    "    \n",
    "    @delegates(DataLoaders)\n",
    "    def dataloaders(\n",
    "        self, \n",
    "        batch_size=8, # A batch size\n",
    "        shuffle_train=True, # Whether to shuffle the training dataset\n",
    "        collate_fn = None, # A custom collation function\n",
    "        **kwargs): # Torch DataLoader kwargs\n",
    "        \"Creates `DataLoaders` from the dataset\"\n",
    "        if collate_fn is None:\n",
    "            from transformers import DataCollatorWithPadding\n",
    "            collate_fn = DataCollatorWithPadding(self.tokenizer)\n",
    "        train_dl = DataLoader(self.train, shuffle=shuffle_train, collate_fn=collate_fn, batch_size=batch_size, **kwargs)\n",
    "        valid_dl = DataLoader(self.valid, shuffle=False, collate_fn=collate_fn, batch_size=batch_size, **kwargs)\n",
    "        return DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceClassificationDatasets(TaskDatasets):\n",
    "    \"\"\"\n",
    "    A set of datasets designed for sequence classification\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        items, # Some items we can pull x's and y's from\n",
    "        get_x = ColReader('text'), # A function taking in one item and extracting the text\n",
    "        get_y = ColReader('label'), # A function taking in one item and extracting the label(s)\n",
    "        splits = None, # Indexs to split the data from\n",
    "        tokenizer_name:str = None, # The string name of a HuggingFace tokenizer or model. If `None`, will not tokenize immediatly\n",
    "        tokenize:bool=True, # Whether to tokenize the dataset immediatly\n",
    "        **kwargs # kwargs to go to `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        xs = L(L(items).map(get_x)[0].values, use_list=True)\n",
    "        ys = L(L(items).map(get_y)[0].values, use_list=True)\n",
    "        self.categorize = Categorize(ys)\n",
    "        ys = [self.categorize(y) for y in ys]\n",
    "        items = L(zip(xs,ys))\n",
    "        super().__init__(items, splits, tokenizer_name, tokenize, **kwargs)\n",
    "        \n",
    "    \n",
    "    @delegates(AutoTokenizer.from_pretrained)\n",
    "    @classmethod\n",
    "    def from_df(\n",
    "        cls,\n",
    "        df:Union[pd.DataFrame, Path], # A Pandas Dataframe or Path to a DataFrame\n",
    "        text_col:str = 'text', # Name of the column the text is stored\n",
    "        label_col:str = 'labels', # Name of the column the label(s) are stored\n",
    "        splits = None, # Indexes to split the data with\n",
    "        tokenizer_name:str = None, # The string name of a huggingFace tokenizer or model. If `None`, will not tokenize the dataset\n",
    "        tokenize:bool = True, # Whether to tokenize the dataset immediatly\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"Builds `SequenceClassificationDatasets` from a `DataFrame` or file path\"\n",
    "        if not isinstance(df, pd.DataFrame): df = pd.read_csv(df)\n",
    "        get_x = ColReader(text_col)\n",
    "        get_y = ColReader(label_col)\n",
    "        if splits is None: splits = RandomSplitter(0.2)(range_of(df))\n",
    "        return cls(df, get_x, get_y, splits, tokenizer_name, tokenize, **kwargs)\n",
    "    \n",
    "    @delegates(DataLoaders)\n",
    "    def dataloaders(\n",
    "        self, \n",
    "        batch_size=8, # A batch size\n",
    "        shuffle_train=True, # Whether to shuffle the training dataset\n",
    "        collate_fn = None, # A custom collation function\n",
    "        **kwargs): # Torch DataLoader kwargs\n",
    "        dls = super().dataloaders(batch_size, shuffle_train, collate_fn, **kwargs)\n",
    "        dls[0].categorize = self.categorize\n",
    "        return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67445879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1037, 5186, 2092, 1011, 2081, 2143, 1012, 1996, 3772, 1010, 5896, 1998, 4950, 1011, 2147, 2024, 2035, 2034, 1011, 3446, 1012, 1996, 2189, 2003, 2204, 1010, 2205, 1010, 2295, 2009, 2003, 3262, 2220, 1999, 1996, 2143, 1010, 2043, 2477, 2024, 2145, 4659, 15138, 2100, 1012, 2045, 2024, 2053, 2428, 18795, 2015, 1999, 1996, 3459, 1010, 2295, 2195, 5344, 2097, 2022, 5220, 1012, 1996, 2972, 3459, 2515, 2019, 6581, 3105, 2007, 1996, 5896, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2021, 2009, 2003, 2524, 2000, 3422, 1010, 2138, 2045, 2003, 2053, 2204, 2203, 2000, 1037, 3663, 2066, 1996, 2028, 3591, 1012, 2009, 2003, 2085, 19964, 2000, 7499, 1996, 2329, 2005, 4292, 18221, 1998, 7486, 2114, 2169, 2060, 1010, 1998, 2059, 10311, 2135, 14443, 2068, 2046, 2048, 3032, 1012, 2045, 2003, 2070, 7857, 1999, 2023, 3193, 1010, 2021, 2009, 1005, 1055, 2036, 2995, 2008, 2053, 2028, 3140, 18221, 1998, 7486, 1999, 1996, 2555, 2000, 11094, 16416, 2102, 2169, 2060, 2004, 2027, 2106, 2105, 1996, 2051, 1997, 13571, 1012, 2009, 3849, 2062, 3497, 2008, 1996, 2329, 3432, 2387, 1996, 13136, 2090, 1996, 11822, 1998, 2020, 12266, 2438, 2000, 18077, 2068, 2000, 2037, 2219, 4515, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2765, 2003, 2008, 2045, 2003, 2172, 18186, 1998, 29582, 3012, 1999, 1996, 3663, 1998, 2023, 2003, 2200, 16010, 2000, 3342, 1998, 2000, 2156, 2006, 1996, 3898, 1012, 2021, 2009, 2003, 2196, 4993, 2004, 1037, 2304, 1011, 1998, 1011, 2317, 2553, 1012, 2045, 2003, 2918, 2791, 1998, 11760, 2006, 2119, 3903, 1010, 1998, 2036, 1996, 3246, 2005, 2689, 1999, 1996, 3920, 4245, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2045, 2003, 18434, 1997, 1037, 4066, 1010, 1999, 1996, 2203, 1010, 2043, 16405, 3217, 2038, 2000, 2191, 1037, 2524, 3601, 2090, 1037, 2158, 2040, 2038, 9868, 2014, 2166, 1010, 2021, 2036, 5621, 3866, 2014, 1010, 1998, 2014, 2155, 2029, 2038, 4487, 6499, 7962, 2098, 2014, 1010, 2059, 2101, 2272, 2559, 2005, 2014, 1012, 2021, 2011, 2008, 2391, 1010, 2016, 2038, 2053, 5724, 2008, 2003, 2302, 2307, 3255, 2005, 2014, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2023, 2143, 7883, 1996, 4471, 2008, 2119, 7486, 1998, 18221, 2031, 2037, 6542, 19399, 1010, 1998, 2036, 2008, 2119, 2064, 2022, 10667, 25201, 1998, 11922, 2111, 1012, 1996, 4507, 1997, 13571, 3084, 2008, 2613, 6648, 2035, 1996, 2062, 16255, 8450, 1010, 2144, 2045, 2064, 2196, 2022, 2613, 16088, 2408, 1996, 2634, 1013, 4501, 3675, 1012, 1999, 2008, 3168, 1010, 2009, 2003, 2714, 2000, 1000, 2720, 1004, 3680, 1045, 10532, 1000, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1999, 1996, 2203, 1010, 2057, 2020, 5580, 2000, 2031, 2464, 1996, 2143, 1010, 2130, 2295, 1996, 5813, 2001, 27724, 2075, 1012, 2065, 1996, 2866, 1998, 2149, 2071, 3066, 2007, 2037, 2219, 15215, 1997, 14398, 2007, 2023, 2785, 1997, 3581, 2791, 1010, 2027, 2052, 5121, 2022, 2488, 2125, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': tensor(1)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "from fastai.data.external import URLs, untar_data\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "dsets = SequenceClassificationDatasets.from_df(\n",
    "    df,\n",
    "    'text',\n",
    "    'label',\n",
    "    tokenizer_name = \"bert-base-uncased\",\n",
    "    tokenize=True\n",
    ")\n",
    "dsets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f20b68c",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fa343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_file_utils.ipynb.\n",
      "Converted 01_callback.ipynb.\n",
      "Converted 02_model_hub.ipynb.\n",
      "Converted 03_model.ipynb.\n",
      "Converted 04_embeddings.ipynb.\n",
      "Converted 04a_tutorial.embeddings.ipynb.\n",
      "Converted 05_token_classification.ipynb.\n",
      "Converted 05a_tutorial.token_tagging.ipynb.\n",
      "Converted 06_sequence_classification.ipynb.\n",
      "Converted 06a_tutorial.easy_sequence_classifier.ipynb.\n",
      "Converted 07_summarization.ipynb.\n",
      "Converted 07a_tutorial.summarization.ipynb.\n",
      "Converted 08_translation.ipynb.\n",
      "Converted 08a_tutorial.translation.ipynb.\n",
      "Converted 09_text_generation.ipynb.\n",
      "Converted 09a_tutorial.easy_text_generator.ipynb.\n",
      "Converted 10_question_answering.ipynb.\n",
      "Converted 10a_tutorial.question_answering.ipynb.\n",
      "Converted 13a_transformers.squad_metrics.ipynb.\n",
      "Converted 14_result.ipynb.\n",
      "Converted 15_training.data.ipynb.\n",
      "Converted 16_training.tuner.ipynb.\n",
      "Converted 20_tutorial.tuner.sequence_classification.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23aca68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
