{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb49e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training.language_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab3def",
   "metadata": {},
   "source": [
    "# Language Model Tuning\n",
    "> Data and Tuning API for Language Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f2a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc604f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from fastcore.basics import mk_class\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.xtras import Path, range_of\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.data.transforms import get_files\n",
    "\n",
    "from adaptnlp.training.core import *\n",
    "from adaptnlp.training.arrow_utils import TextNoNewLineDatasetReader\n",
    "\n",
    "from adaptnlp.inference.text_generation import TransformersTextGenerator\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, default_data_collator\n",
    "from datasets import Dataset\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995f2f0",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e3be962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _group_texts(examples, block_size):\n",
    "    # Concatenate all texts, based on code by Transformers\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12201b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _tokenize(item, tokenizer, tokenize_kwargs): return tokenizer(item['text'], **tokenize_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7946bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LanguageModelDatasets(TaskDatasets):\n",
    "    \"\"\"\n",
    "    A set of datasets designed for language model fine-tuning\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dset,\n",
    "        valid_dset,\n",
    "        tokenizer_name,\n",
    "        tokenize,\n",
    "        tokenize_kwargs,\n",
    "        auto_kwargs,\n",
    "        remove_columns,\n",
    "        block_size,\n",
    "        masked_lm\n",
    "    ):  \n",
    "        \"Constructs TaskDatasets, should not be called explicitly\"\n",
    "        super().__init__(\n",
    "            train_dset, \n",
    "            valid_dset, \n",
    "            tokenizer_name, \n",
    "            tokenize,\n",
    "            _tokenize, \n",
    "            tokenize_kwargs, \n",
    "            auto_kwargs\n",
    "        )\n",
    "        self.masked_lm = masked_lm\n",
    "        self.block_size = block_size\n",
    "        f = partial(_group_texts, block_size=self.block_size)\n",
    "        self.train = self.train.map(f, batched=True)\n",
    "        self.valid = self.valid.map(f, batched=True)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dfs(\n",
    "        cls,\n",
    "        train_df:pd.DataFrame, # A Pandas Dataframe\n",
    "        text_col:str, # The name of the text column\n",
    "        tokenizer_name:str, # The name of the tokenizer\n",
    "        block_size:int=128, # The size of each block\n",
    "        masked_lm:bool=False, # Whether the language model is a MLM\n",
    "        valid_df:pd.DataFrame=None, # An optional validation DataFrame\n",
    "        split_func:callable=None, # Optionally a splitting function similar to RandomSplitter\n",
    "        split_pct:float=.1, # What % to split the df between training and validation\n",
    "        tokenize_kwargs:dict={}, # kwargs for the tokenize function\n",
    "        auto_kwargs:dict={} # kwargs for the AutoTokenizer.from_pretrained constructor\n",
    "    ):\n",
    "        \"Builds `LanguageModelDatasets` from a `DataFrame` or file path\"\n",
    "        if split_func is None: split_func = RandomSplitter(split_pct)\n",
    "        if valid_df is None:\n",
    "            train_idxs, valid_idxs = split_func(range_of(train_df))\n",
    "            valid_df = train_df.iloc[valid_idxs]\n",
    "            train_df = train_df.iloc[train_idxs]            \n",
    "        \n",
    "        train_df = train_df[[text_col]]\n",
    "        valid_df = valid_df[[text_col]]\n",
    "        \n",
    "        train_df = train_df.rename(columns={text_col:'text'})\n",
    "        valid_df = valid_df.rename(columns={text_col:'text'})\n",
    "        \n",
    "        train_dset = Dataset.from_dict(train_df.to_dict('list'))\n",
    "        valid_dset = Dataset.from_dict(valid_df.to_dict('list'))\n",
    "        \n",
    "        dsets = TaskDatasets(\n",
    "            train_dset,\n",
    "            valid_dset,\n",
    "            tokenizer_name,\n",
    "            False,\n",
    "            _tokenize,\n",
    "            tokenize_kwargs=tokenize_kwargs,\n",
    "            auto_kwargs=auto_kwargs\n",
    "        )\n",
    "\n",
    "        f = partial(_group_texts, block_size=512)\n",
    "        t = partial(_tokenize, tokenizer=dsets.tokenizer, tokenize_kwargs=tokenize_kwargs)\n",
    "        dsets.train = dsets.train.map(t, batched=True, remove_columns=['text'])\n",
    "        dsets.valid = dsets.valid.map(t, batched=True, remove_columns=['text'])\n",
    "\n",
    "        dsets.train = dsets.train.map(f, batched=True)\n",
    "        dsets.valid = dsets.valid.map(f, batched=True)\n",
    "        return dsets\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csvs(\n",
    "        cls,\n",
    "        train_csv:Path, # A training csv file\n",
    "        text_col:str, # The name of the text column\n",
    "        tokenizer_name:str, # The name of the tokenizer\n",
    "        block_size:int=128, # The size of each block\n",
    "        masked_lm:bool=False, # Whether the language model is a MLM\n",
    "        valid_csv:Path=None, # An optional validation csv\n",
    "        split_func:callable=None, # Optionally a splitting function similar to RandomSplitter\n",
    "        split_pct:float=.1, # What % to split the df between training and validation\n",
    "        tokenize_kwargs:dict={}, # kwargs for the tokenize function\n",
    "        auto_kwargs:dict={}, # kwargs for the AutoTokenizer.from_pretrained constructor\n",
    "        **kwargs, # kwargs for `pd.read_csv`\n",
    "    ):\n",
    "        \"Builds `LanguageModelDatasets` from a single csv or set of csvs. A convience constructor for `from_dfs`\"\n",
    "        train_df = pd.read_csv(train_csv, **kwargs)\n",
    "        if valid_csv is not None: valid_df = pd.read_csv(valid_csv, **kwargs)\n",
    "        else: valid_df = None\n",
    "        return cls.from_dfs(\n",
    "            train_df, \n",
    "            text_col, \n",
    "            tokenizer_name, \n",
    "            block_size, \n",
    "            masked_lm,\n",
    "            valid_df,\n",
    "            split_func,\n",
    "            split_pct,\n",
    "            tokenize_kwargs,\n",
    "            auto_kwargs\n",
    "        )  \n",
    "        \n",
    "    @classmethod\n",
    "    def from_folders(\n",
    "        cls,\n",
    "        train_path:Path, # The path to the training data\n",
    "        tokenizer_name:str, # The name of the tokenizer\n",
    "        block_size:int=128, # The size of each block\n",
    "        masked_lm:bool=False, # Whether the language model is a MLM\n",
    "        valid_path:Path=None, # An optional validation path\n",
    "        split_func:callable=None, # Optionally a splitting function similar to RandomSplitter\n",
    "        split_pct:float=.1, # What % to split the df between training and validation\n",
    "        tokenize_kwargs:dict={}, # kwargs for the tokenize function\n",
    "        auto_kwargs:dict={} # kwargs for the AutoTokenizer.from_pretrained constructor\n",
    "    ):\n",
    "        \"Builds `LanguageModelDatasets` from a folder or group of folders\"\n",
    "        train_txts = get_files(train_path, extensions='.txt')\n",
    "        if valid_path is not None:\n",
    "            valid_txts = get_files(valid_path, extensions='.txt')\n",
    "        else:\n",
    "            if split_func is not None:\n",
    "                split_func = RandomSplitter(split_pct)\n",
    "            train_idxs, valid_idxs = split_func(train_txts)\n",
    "            valid_txts = train_txts[valid_idxs]\n",
    "            train_txts = train_txts[train_idxs]\n",
    "        train_txts = [str(x) for x in train_txts]\n",
    "        valid_txts = [str(x) for x in valid_txts]\n",
    "        train_dset = TextNoNewLineDatasetReader(train_txts).read()\n",
    "        valid_dset = TextNoNewLineDatasetReader(valid_txts).read()\n",
    "        \n",
    "        dsets = TaskDatasets(\n",
    "            train_dset,\n",
    "            valid_dset,\n",
    "            tokenizer_name,\n",
    "            False,\n",
    "            _tokenize,\n",
    "            tokenize_kwargs=tokenize_kwargs,\n",
    "            auto_kwargs=auto_kwargs\n",
    "        )\n",
    "\n",
    "        f = partial(_group_texts, block_size=512)\n",
    "        t = partial(_tokenize, tokenizer=dsets.tokenizer, tokenize_kwargs=tokenize_kwargs)\n",
    "        dsets.train = dsets.train.map(t, batched=True, remove_columns=['text'])\n",
    "        dsets.valid = dsets.valid.map(t, batched=True, remove_columns=['text'])\n",
    "\n",
    "        dsets.train = dsets.train.map(f, batched=True)\n",
    "        dsets.valid = dsets.valid.map(f, batched=True)\n",
    "        return dsets\n",
    "    \n",
    "    @delegates(DataLoaders)\n",
    "    def dataloaders(\n",
    "        self, \n",
    "        batch_size=8, # A batch size\n",
    "        shuffle_train=True, # Whether to shuffle the training dataset\n",
    "        collate_fn = default_data_collator, # A custom collation function\n",
    "        mlm_probability:float = 0.15, # Token masking probablity for Masked Language Models\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"Build DataLoaders from `self`\"\n",
    "        if self.masked_lm: collate_fn = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm_probability=mlm_probability)\n",
    "        return super().dataloaders(batch_size, shuffle_train, collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f88a4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from transformers import AutoModelForMaskedLM, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "mk_class('LMType', **{o:o.lower() for o in ['Masked', 'Causal', 'Seq2Seq']},\n",
    "        doc=\"All valid language model classes with typo-proofing\")\n",
    "\n",
    "_constructors = {\n",
    "    'masked':AutoModelForMaskedLM.from_pretrained,\n",
    "    'causal':AutoModelForCausalLM.from_pretrained,\n",
    "    'seq2seq':AutoModelForSeq2SeqLM.from_pretrained\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97b271ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4d9be3e7894ffea61e499d9e679157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486b0feff04d42c9836de411754ad05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d275ad1475e4f08929dd4fae7bbe31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba30e50b86964095a7ed85e90cf5d23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from fastai.data.external import URLs, untar_data\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "\n",
    "dset = LanguageModelDatasets.from_dfs(\n",
    "    df,\n",
    "    'text',\n",
    "    tokenizer_name = \"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "test_eq(len(dset.train[0]), 4) \n",
    "test_eq(dset.train[0].keys(), ['attention_mask', 'input_ids', 'labels', 'token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "686ca6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bce94eb6914739b08267bc2ac12753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3863fa711dd54322b210fa5ef3d27086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876a3945ff404ff1888b775635ed4493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb66629843f4ae59027affa1a59376a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "train_df = df.iloc[:800]\n",
    "valid_df = df.iloc[800:]\n",
    "\n",
    "dset = LanguageModelDatasets.from_dfs(\n",
    "    train_df,\n",
    "    valid_df=valid_df, \n",
    "    text_col='text',\n",
    "    tokenizer_name = \"bert-base-uncased\"\n",
    ")\n",
    "test_eq(len(dset.train[0]), 4) \n",
    "test_eq(dset.train[0].keys(), ['attention_mask', 'input_ids', 'labels', 'token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LanguageModelTuner(AdaptiveTuner):\n",
    "    \"\"\"\n",
    "    An `AdaptiveTuner` with good defaults for Language Model fine-tuning\n",
    "    **Valid kwargs and defaults:**\n",
    "      - `lr`:float = 0.001\n",
    "      - `splitter`:function = `trainable_params`\n",
    "      - `cbs`:list = None\n",
    "      - `path`:Path = None\n",
    "      - `model_dir`:Path = 'models'\n",
    "      - `wd`:float = None\n",
    "      - `wd_bn_bias`:bool = False\n",
    "      - `train_bn`:bool = True\n",
    "      - `moms`: tuple(float) = (0.95, 0.85, 0.95)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dls:DataLoaders, # A set of DataLoaders or AdaptiveDataLoaders\n",
    "        model_name, # A HuggingFace model\n",
    "        tokenizer = None, # A HuggingFace tokenizer\n",
    "        language_model_type:LMType = LMType.Causal, # The type of language model to use\n",
    "        loss_func = CrossEntropyLossFlat(), # A loss function\n",
    "        metrics = [Perplexity()], # Metrics to monitor the training with\n",
    "        opt_func = Adam, # A fastai or torch Optimizer\n",
    "        additional_cbs = None, # Additional Callbacks to have always tied to the Tuner,\n",
    "        expose_fastai_api = False, # Whether to expose the fastai API\n",
    "        **kwargs, # kwargs for `Learner.__init__`\n",
    "    ):\n",
    "        additional_cbs = listify(additional_cbs)\n",
    "        for arg in 'dls,model,loss_func,metrics,opt_func,cbs,expose_fastai'.split(','):\n",
    "            if arg in kwargs.keys(): kwargs.pop(arg) # Pop all existing kwargs\n",
    "\n",
    "        if language_model_type is None: raise ValueError(\"Please specify the type of language model you want to use, such as `masked` or `causal`\")\n",
    "        if language_model_type not in _constructors.keys():\n",
    "            raise ValueError(\n",
    "                \"\"\"\n",
    "                Please enter a valid Langauge Model Type of:\n",
    "                  * `masked` or `LMType.Masked`\n",
    "                  * `causal` or `LMType.Causal`\n",
    "                  * `seq2seq` or `LMType.Seq2Seq`\n",
    "                \"\"\"\n",
    "            )\n",
    "        try:\n",
    "            model = _constructors[language_model_type](model_name)\n",
    "        except Exception as e:\n",
    "            message = e.args[0]\n",
    "            m = f\"Was not able to create a {language_model_type} instance of {model_name}. Please use a valid model for your task:\"\n",
    "            m += message\n",
    "            e.args = [m]\n",
    "            raise e\n",
    "        \n",
    "        if tokenizer is None: tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = \"<PAD>\"\n",
    "\n",
    "        super().__init__(\n",
    "            expose_fastai_api,\n",
    "            dls = dls,\n",
    "            model = model,\n",
    "            tokenizer = tokenizer,\n",
    "            loss_func = loss_func,\n",
    "            metrics = metrics,\n",
    "            opt_func = opt_func,\n",
    "            cbs=additional_cbs,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text:Union[List[str], str], # Some text or list of texts to do inference with\n",
    "        bs:int=64, # A batch size to use for multiple texts\n",
    "        num_tokens_to_produce:int=50, # Number of tokens to generate\n",
    "        **kwargs, # Optional arguments for `PretrainedModel.generate`\n",
    "    ):\n",
    "        \"Predict some `text` for sequence classification with the currently loaded model\"\n",
    "        if getattr(self, '_inferencer', None) is None:\n",
    "            \n",
    "            if self.tokenizer.pad_token is None: \n",
    "                self.tokenizer.pad_token=\"<PAD>\"\n",
    "            self._inferencer = TransformersTextGenerator(self.tokenizer,self.model)\n",
    "        return self._inferencer.predict(text, bs, num_tokens_to_produce, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c9bdb1",
   "metadata": {},
   "source": [
    "## Export - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adac362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_callback.ipynb.\n",
      "Converted 02_model_hub.ipynb.\n",
      "Converted 03_model.ipynb.\n",
      "Converted 04_embeddings.ipynb.\n",
      "Converted 04a_tutorial.embeddings.ipynb.\n",
      "Converted 05_token_classification.ipynb.\n",
      "Converted 05a_tutorial.token_tagging.ipynb.\n",
      "Converted 06_sequence_classification.ipynb.\n",
      "Converted 06a_tutorial.easy_sequence_classifier.ipynb.\n",
      "Converted 07_summarization.ipynb.\n",
      "Converted 07a_tutorial.summarization.ipynb.\n",
      "Converted 08_translation.ipynb.\n",
      "Converted 08a_tutorial.translation.ipynb.\n",
      "Converted 09_text_generation.ipynb.\n",
      "Converted 09a_tutorial.easy_text_generator.ipynb.\n",
      "Converted 10_question_answering.ipynb.\n",
      "Converted 10a_tutorial.question_answering.ipynb.\n",
      "Converted 13a_inference.utils.ipynb.\n",
      "Converted 14_result.ipynb.\n",
      "Converted 14_training.core.ipynb.\n",
      "Converted 15_training.sequence_classification.ipynb.\n",
      "Converted 16_training.language_model.ipynb.\n",
      "Converted 17_training.arrow_utils.ipynb.\n",
      "Converted 20_tutorial.tuner.sequence_classification.ipynb.\n",
      "Converted 21_tutorial.training.language_model.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e3433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
