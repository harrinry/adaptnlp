{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Title (change me)\n",
    "> Default description (change me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Union, Dict\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from adaptnlp import EasyDocumentEmbeddings\n",
    "\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.data import Corpus\n",
    "from flair.embeddings import DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.visual.training_curves import Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceClassifierTrainer:\n",
    "    \"\"\"Sequence Classifier Trainer\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> sc_trainer = SequenceClassifierTrainer(corpus=\"/Path/to/data/dir\")\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **corpus** - A flair corpus data model or `Path`/string to a directory with train.csv/test.csv/dev.csv\n",
    "    * **encoder** - A `EasyDocumentEmbeddings` object if training with a flair prediction head or `Path`/string if training with Transformer's prediction models\n",
    "    * **column_name_map** - Required if corpus is not a `Corpus` object, it's a dictionary specifying the indices of the text and label columns of the csv i.e. {1:\"text\",2:\"label\"}\n",
    "    * **corpus_in_memory** - Boolean for whether to store corpus embeddings in memory\n",
    "    * **predictive_head** - For now either \"flair\" or \"transformers\" for the prediction head\n",
    "    * **&ast;&ast;kwargs** - Keyword arguments for Flair's `TextClassifier` model class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        corpus: Union[Corpus, Path, str],\n",
    "        encoder: Union[EasyDocumentEmbeddings, Path, str],\n",
    "        column_name_map: None,\n",
    "        corpus_in_memory: bool = True,\n",
    "        predictive_head: str = \"flair\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if isinstance(corpus, Corpus):\n",
    "            self.corpus = corpus\n",
    "        else:\n",
    "            if isinstance(corpus, str):\n",
    "                corpus = Path(corpus)\n",
    "            if not column_name_map:\n",
    "                raise ValueError(\n",
    "                    \"If not instantiating with `Corpus` object, must pass in `column_name_map` argument to specify text/label indices\"\n",
    "                )\n",
    "            self.corpus = CSVClassificationCorpus(\n",
    "                corpus,\n",
    "                column_name_map,\n",
    "                skip_header=True,\n",
    "                delimiter=\",\",\n",
    "                in_memory=corpus_in_memory,\n",
    "            )\n",
    "\n",
    "        # Verify predictive head is within available heads\n",
    "        self.available_predictive_head = [\"flair\", \"transformers\"]\n",
    "        if predictive_head not in self.available_predictive_head:\n",
    "            raise ValueError(\n",
    "                f\"predictive_head param must be one of the following: {self.available_predictive_head}\"\n",
    "            )\n",
    "        self.predictive_head = predictive_head\n",
    "\n",
    "        # Verify correct corresponding encoder is used with predictive head (This can be structured with better design in the future)\n",
    "        if isinstance(encoder, EasyDocumentEmbeddings):\n",
    "            if predictive_head == \"transformers\":\n",
    "                raise ValueError(\n",
    "                    \"If using `transformers` predictive head, pass in the path to the transformer's model\"\n",
    "                )\n",
    "            else:\n",
    "                self.encoder = encoder\n",
    "        else:\n",
    "            if isinstance(encoder, str):\n",
    "                encoder = Path(encoder)\n",
    "            self.encoder = encoder\n",
    "\n",
    "        # Create the label dictionary on init (store to keep from constantly generating label_dict) should we use dev/test set instead assuming all labels are provided?\n",
    "        self.label_dict = self.corpus.make_label_dictionary()\n",
    "\n",
    "        # Save trainer kwargs dict for reinitializations\n",
    "        self.trainer_kwargs = kwargs\n",
    "\n",
    "        # Load trainer with initial setup\n",
    "        self._initial_setup(self.label_dict, **kwargs)\n",
    "\n",
    "    def _initial_setup(self, label_dict: Dict, **kwargs):\n",
    "        if self.predictive_head == \"flair\":\n",
    "\n",
    "            # Get Document embeddings from `embeddings`\n",
    "            document_embeddings: DocumentRNNEmbeddings = self.encoder.rnn_embeddings\n",
    "\n",
    "            # Create the text classifier\n",
    "            classifier = TextClassifier(\n",
    "                document_embeddings,\n",
    "                label_dictionary=label_dict,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            # Initialize the text classifier trainer\n",
    "            self.trainer = ModelTrainer(classifier, self.corpus)\n",
    "\n",
    "        # TODO: In internal transformers package, create ****ForSequenceClassification adaptations\n",
    "        elif self.predictive_head == \"transformers\":\n",
    "            with open(self.encoder / \"config.json\") as config_f:\n",
    "                configs = json.load(config_f)\n",
    "                model_name = configs[\"architectures\"][-1]\n",
    "            if model_name == \"BertForMaskedLM\":\n",
    "                pass\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        output_dir: Union[Path, str],\n",
    "        learning_rate: float = 0.07,\n",
    "        mini_batch_size: int = 32,\n",
    "        anneal_factor: float = 0.5,\n",
    "        patience: int = 5,\n",
    "        max_epochs: int = 150,\n",
    "        plot_weights: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Train the Sequence Classifier\n",
    "\n",
    "        * **output_dir** - The output directory where the model predictions and checkpoints will be written.\n",
    "        * **learning_rate** - The initial learning rate\n",
    "        * **mini_batch_size** - Batch size for the dataloader\n",
    "        * **anneal_factor** - The factor by which the learning rate is annealed\n",
    "        * **patience** - Patience is the number of epochs with no improvement the Trainer waits until annealing the learning rate\n",
    "        * **max_epochs** - Maximum number of epochs to train. Terminates training if this number is surpassed.\n",
    "        * **plot_weights** - Bool to plot weights or not\n",
    "        * **kwargs** - Keyword arguments for the rest of Flair's `Trainer.train()` hyperparameters\n",
    "        \"\"\"\n",
    "        if isinstance(output_dir, str):\n",
    "            output_dir = Path(output_dir)\n",
    "\n",
    "        # Start the training\n",
    "        self.trainer.train(\n",
    "            output_dir,\n",
    "            learning_rate=learning_rate,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            anneal_factor=anneal_factor,\n",
    "            patience=patience,\n",
    "            max_epochs=max_epochs,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Plot weight traces\n",
    "        if plot_weights:\n",
    "            plotter = Plotter()\n",
    "            plotter.plot_weights(output_dir / \"weights.txt\")\n",
    "\n",
    "    def find_learning_rate(\n",
    "        self,\n",
    "        output_dir: Union[Path, str],\n",
    "        file_name: str = \"learning_rate.tsv\",\n",
    "        start_learning_rate: float = 1e-8,\n",
    "        end_learning_rate: float = 10,\n",
    "        iterations: int = 100,\n",
    "        mini_batch_size: int = 32,\n",
    "        stop_early: bool = True,\n",
    "        smoothing_factor: float = 0.7,\n",
    "        plot_learning_rate: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Uses Leslie's cyclical learning rate finding method to generate and save the loss x learning rate plot\n",
    "\n",
    "        This method returns a suggested learning rate using the static method `LMFineTuner.suggest_learning_rate()`\n",
    "        which is implicitly run in this method.\n",
    "\n",
    "        * **output_dir** - Path to dir for learning rate file to be saved\n",
    "        * **file_name** - Name of learning rate .tsv file\n",
    "        * **start_learning_rate** - Initial learning rate to start cyclical learning rate finder method\n",
    "        * **end_learning_rate** - End learning rate to stop exponential increase of the learning rate\n",
    "        * **iterations** - Number of optimizer iterations for the ExpAnnealLR scheduler\n",
    "        * **mini_batch_size** - Batch size for dataloader\n",
    "        * **stop_early** - Bool for stopping early once loss diverges\n",
    "        * **smoothing_factor** - Smoothing factor on moving average of losses\n",
    "        * **adam_epsilon** - Epsilon for Adam optimizer.\n",
    "        * **weight_decay** - Weight decay if we apply some.\n",
    "        * **kwargs** - Additional keyword arguments for the Adam optimizer\n",
    "        **return** - Learning rate as a float\n",
    "        \"\"\"\n",
    "        # 7. find learning rate\n",
    "        learning_rate_tsv = self.trainer.find_learning_rate(\n",
    "            base_path=output_dir,\n",
    "            file_name=file_name,\n",
    "            start_learning_rate=start_learning_rate,\n",
    "            end_learning_rate=end_learning_rate,\n",
    "            iterations=iterations,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            stop_early=stop_early,\n",
    "            smoothing_factor=smoothing_factor,\n",
    "        )\n",
    "\n",
    "        # Reinitialize optimizer and parameters by reinitializing trainer\n",
    "        self._initial_setup(self.label_dict, **self.trainer_kwargs)\n",
    "\n",
    "        if plot_learning_rate:\n",
    "            plotter = Plotter()\n",
    "            plotter.plot_learning_rate(learning_rate_tsv)\n",
    "\n",
    "        # Use the automated learning rate finder\n",
    "        with open(learning_rate_tsv) as lr_f:\n",
    "            lr_tsv = list(csv.reader(lr_f, delimiter=\"\\t\"))\n",
    "        losses = np.array([float(row[-1]) for row in lr_tsv[1:]])\n",
    "        lrs = np.array([float(row[-2]) for row in lr_tsv[1:]])\n",
    "        lr_to_use = self.suggested_learning_rate(losses, lrs, **kwargs)\n",
    "        print(f\"Recommended Learning Rate {lr_to_use}\")\n",
    "        return lr_to_use\n",
    "\n",
    "    @staticmethod\n",
    "    def suggested_learning_rate(\n",
    "        losses: np.array,\n",
    "        lrs: np.array,\n",
    "        lr_diff: int = 15,\n",
    "        loss_threshold: float = 0.2,\n",
    "        adjust_value: float = 1,\n",
    "    ) -> float:\n",
    "        # This seems redundant unless we can make this configured for each trainer/finetuner\n",
    "        \"\"\"\n",
    "        Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method\n",
    "\n",
    "        * **losses** - Numpy array of losses\n",
    "        * **lrs** - Numpy array of exponentially increasing learning rates (must match dim of `losses`)\n",
    "        * **lr_diff** - Learning rate Interval of slide ruler\n",
    "        * **loss_threshold** - Threshold of loss difference on interval where the sliding stops\n",
    "        * **adjust_value** - Coefficient for adjustment\n",
    "        **return** - the optimal learning rate as a float\n",
    "        \"\"\"\n",
    "        # Get loss values and their corresponding gradients, and get lr values\n",
    "        assert lr_diff < len(losses)\n",
    "        loss_grad = np.gradient(losses)\n",
    "\n",
    "        # Search for index in gradients where loss is lowest before the loss spike\n",
    "        # Initialize right and left idx using the lr_diff as a spacing unit\n",
    "        # Set the local min lr as -1 to signify if threshold is too low\n",
    "        r_idx = -1\n",
    "        l_idx = r_idx - lr_diff\n",
    "        local_min_lr = lrs[l_idx]\n",
    "        while (l_idx >= -len(losses)) and (\n",
    "            abs(loss_grad[r_idx] - loss_grad[l_idx]) > loss_threshold\n",
    "        ):\n",
    "            local_min_lr = lrs[l_idx]\n",
    "            r_idx -= 1\n",
    "            l_idx -= 1\n",
    "\n",
    "        lr_to_use = local_min_lr * adjust_value\n",
    "\n",
    "        return lr_to_use"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
