{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp transformers.finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Title (change me)\n",
    "> Default description (change me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Contains code used/modified by AdaptNLP author from transformers\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "from typing import Union, List\n",
    "import datetime\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import csv\n",
    "import copy\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm as tqdm_base\n",
    "\n",
    "from flair.visual.training_curves import Plotter\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer,\n",
    "    CamembertConfig,\n",
    "    CamembertForMaskedLM,\n",
    "    CamembertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForMaskedLM,\n",
    "    DistilBertTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    OpenAIGPTConfig,\n",
    "    OpenAIGPTLMHeadModel,\n",
    "    OpenAIGPTTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    "    AlbertConfig,\n",
    "    AlbertForMaskedLM,\n",
    "    AlbertTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tqdm(*args, **kwargs):\n",
    "    if hasattr(tqdm_base, \"_instances\"):\n",
    "        for instance in list(tqdm_base._instances):\n",
    "            tqdm_base._decr_instances(instance)\n",
    "    return tqdm_base(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        model_type: str,\n",
    "        overwrite_cache: bool,\n",
    "        file_path: str = \"train\",\n",
    "        block_size: int = 512,\n",
    "    ):\n",
    "        assert os.path.isfile(file_path)\n",
    "\n",
    "        block_size = block_size - (\n",
    "            tokenizer.max_len - tokenizer.max_len_single_sentence\n",
    "        )\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, model_type + \"_cached_lm_\" + str(block_size) + \"_\" + filename\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            print(\"Opening file\")\n",
    "            with open(file_path, encoding=\"utf-8\") as f:\n",
    "                print(f\"Reading file {file_path}\")\n",
    "                if file_path.endswith(\".txt\") or file_path.endswith(\".raw\"):\n",
    "                    text = f.read()\n",
    "                elif file_path.endswith(\".csv\"):\n",
    "                    reader = csv.reader(f)\n",
    "                    try:\n",
    "                        csv_idx = next(reader).index(\"text\")\n",
    "                        text = \". \".join([row[csv_idx] for row in reader])\n",
    "                    except ValueError:\n",
    "                        logger.info(\"No header row provided with 'text' column\")\n",
    "                        text = \"\"\n",
    "                else:\n",
    "                    text = \"\"\n",
    "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "            print(\"iterating through tokenized text\")\n",
    "            for i in tqdm(\n",
    "                range(0, len(tokenized_text) - block_size + 1, block_size)\n",
    "            ):  # Truncate in block of block_size\n",
    "                self.examples.append(\n",
    "                    tokenizer.build_inputs_with_special_tokens(\n",
    "                        tokenized_text[i : i + block_size]\n",
    "                    )\n",
    "                )\n",
    "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
    "            # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "            # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LMFineTunerManual:\n",
    "    \"\"\"\n",
    "     A Language Model Fine Tuner object you can set language model configurations and then train and evaluate\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> finetuner = adaptnlp.LMFineTuner()\n",
    "    >>> finetuner.train()\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **train_data_file** - The input training data file (a text file).\n",
    "    * **eval_data_file** - An optional input evaluation data file to evaluate the perplexity on (a text file).\n",
    "    * **model_type** - The model architecture to be trained or fine-tuned.\n",
    "    * **model_name_or_path** - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\n",
    "    * **mlm** - Train with masked-language modeling loss instead of language modeling.\n",
    "    * **mlm_probability** - Ratio of tokens to mask for masked language modeling loss\n",
    "    * **config_name** - Optional Transformers pretrained config name or path if not the same as model_name_or_path. If both are None, initialize a new config.\n",
    "    * **tokenizer_name** - Optional Transformers pretrained tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new tokenizer.\n",
    "    * **cache_dir** - Optional directory to store the pre-trained models downloaded from s3 (If None, will go to default dir)\n",
    "    * **block_size** - Optional input sequence length after tokenization.\n",
    "                        The training dataset will be truncated in block of this size for training.\"\n",
    "                        `-1` will default to the model max input length for single sentence inputs (take into account special tokens).\n",
    "    * **no_cuda** - Avoid using CUDA when available\n",
    "    * **overwrite_cache** - Overwrite the cached training and evaluation sets\n",
    "    * **seed** - random seed for initialization\n",
    "    * **fp16** - Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\n",
    "    * **fp16_opt_level** - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\n",
    "    * **local_rank** - For distributed training: local_rank\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data_file: str,\n",
    "        eval_data_file: str = None,\n",
    "        model_type: str = \"bert\",\n",
    "        model_name_or_path: str = None,\n",
    "        mlm: bool = True,\n",
    "        mlm_probability: float = 0.15,\n",
    "        config_name: str = None,\n",
    "        tokenizer_name: str = None,\n",
    "        cache_dir: str = None,\n",
    "        block_size: int = -1,\n",
    "        no_cuda: bool = False,\n",
    "        overwrite_cache: bool = False,\n",
    "        seed: int = 42,\n",
    "        fp16: bool = False,\n",
    "        fp16_opt_level: str = \"O1\",\n",
    "        local_rank: int = -1,\n",
    "    ):\n",
    "\n",
    "        self.train_data_file = train_data_file\n",
    "        self.eval_data_file = eval_data_file\n",
    "        self.model_type = model_type\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.mlm = mlm\n",
    "        self.mlm_probability = mlm_probability\n",
    "        self.config_name = config_name\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.cache_dir = cache_dir\n",
    "        self.block_size = block_size\n",
    "        self.no_cuda = no_cuda\n",
    "        self.overwrite_cache = overwrite_cache\n",
    "        self.seed = seed\n",
    "        self.fp16 = fp16\n",
    "        self.fp16_opt_level = fp16_opt_level\n",
    "        self.local_rank = local_rank\n",
    "\n",
    "        self.MODEL_CLASSES = {\n",
    "            \"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "            \"openai-gpt\": (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "            \"bert\": (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "            \"roberta\": (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
    "            \"distilbert\": (\n",
    "                DistilBertConfig,\n",
    "                DistilBertForMaskedLM,\n",
    "                DistilBertTokenizer,\n",
    "            ),\n",
    "            \"camembert\": (CamembertConfig, CamembertForMaskedLM, CamembertTokenizer),\n",
    "            \"albert\": (AlbertConfig, AlbertForMaskedLM, AlbertTokenizer),\n",
    "        }\n",
    "\n",
    "        self._initial_setup()\n",
    "\n",
    "    def _initial_setup(self):\n",
    "\n",
    "        #  Setup model type and output directory\n",
    "        if (\n",
    "            self.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"]\n",
    "            and not self.mlm\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using with the mlm parameter set as `True`\"\n",
    "                \"for (masked language modeling).\"\n",
    "            )\n",
    "\n",
    "        if self.eval_data_file is None:\n",
    "            raise ValueError(\n",
    "                \"Cannot do evaluation without an evaluation data file. Either supply a file to eval_data_file parameter or continue without evaluating\"\n",
    "            )\n",
    "\n",
    "        #  Setup CUDA, GPU, and distributed training\n",
    "        if self.local_rank == -1 or self.no_cuda:\n",
    "            device = torch.device(\n",
    "                \"cuda\" if torch.cuda.is_available() and not self.no_cuda else \"cpu\"\n",
    "            )\n",
    "            self.n_gpu = 0 if self.no_cuda else torch.cuda.device_count()\n",
    "        else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "            torch.cuda.set_device(self.local_rank)\n",
    "            device = torch.device(\"cuda\", self.local_rank)\n",
    "            torch.distributed.init_process_group(backend=\"nccl\")\n",
    "            self.n_gpu = 1\n",
    "        self.device = device\n",
    "\n",
    "        # Setup logging and seed\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "            datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "            level=logging.INFO if self.local_rank in [-1, 0] else logging.WARN,\n",
    "        )\n",
    "        logger.warning(\n",
    "            \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "            self.local_rank,\n",
    "            device,\n",
    "            self.n_gpu,\n",
    "            bool(self.local_rank != -1),\n",
    "            self.fp16,\n",
    "        )\n",
    "        self._set_seed()\n",
    "\n",
    "        # Load pretrained model and tokenizer\n",
    "        if self.local_rank not in [-1, 0]:\n",
    "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "        config_class, model_class, tokenizer_class = self.MODEL_CLASSES[self.model_type]\n",
    "        if self.config_name:\n",
    "            self.config = config_class.from_pretrained(\n",
    "                self.config_name, cache_dir=self.cache_dir\n",
    "            )\n",
    "        elif self.model_name_or_path:\n",
    "            self.config = config_class.from_pretrained(\n",
    "                self.model_name_or_path, cache_dir=self.cache_dir\n",
    "            )\n",
    "        else:\n",
    "            self.config = config_class()\n",
    "\n",
    "        if self.tokenizer_name:\n",
    "            self.tokenizer = tokenizer_class.from_pretrained(\n",
    "                self.tokenizer_name, cache_dir=self.cache_dir\n",
    "            )\n",
    "        elif self.model_name_or_path:\n",
    "            self.tokenizer = tokenizer_class.from_pretrained(\n",
    "                self.model_name_or_path, cache_dir=self.cache_dir\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"You are instantiating a new {tokenizer_class.__name__} tokenizer from scratch. Are you sure this is what you meant to do? \\n To specifiy a pretrained tokenizer name, pass in a tokenizer_name argument\"\n",
    "            )\n",
    "\n",
    "        if self.block_size <= 0:\n",
    "            self.block_size = self.tokenizer.max_len_single_sentence\n",
    "            # Our input block size will be the max possible for the model\n",
    "        else:\n",
    "            self.block_size = min(\n",
    "                self.block_size, self.tokenizer.max_len_single_sentence\n",
    "            )\n",
    "\n",
    "        if self.model_name_or_path:\n",
    "            self.model = model_class.from_pretrained(\n",
    "                self.model_name_or_path,\n",
    "                from_tf=bool(\".ckpt\" in self.model_name_or_path),\n",
    "                config=self.config,\n",
    "                cache_dir=self.cache_dir,\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"Training new model from scratch\")\n",
    "            self.model = model_class(config=self.config)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        if self.local_rank == 0:\n",
    "            torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "    def train_one_cycle(\n",
    "        self,\n",
    "        output_dir: str,\n",
    "        should_continue: bool = False,\n",
    "        overwrite_output_dir: bool = False,\n",
    "        evaluate_during_training: bool = False,\n",
    "        per_gpu_train_batch_size: int = 4,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        learning_rate: float = 5e-5,\n",
    "        weight_decay: float = 0.0,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        max_grad_norm: float = 1.0,\n",
    "        num_train_epochs: float = 1.0,\n",
    "        max_steps: int = -1,\n",
    "        warmup_steps: int = 0,\n",
    "        logging_steps: int = 50,\n",
    "        save_steps: int = 50,\n",
    "        save_total_limit: int = 3,\n",
    "        use_tensorboard: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        * **output_dir** - The output directory where the model predictions and checkpoints will be written.\n",
    "        * **should_continue** - Whether to continue training from latest checkpoint in `output_dir`\n",
    "        * **overwrite_output_dir** - Overwrite the content of output directory `output_dir`\n",
    "        * **evaluate_during_training** - Run evaluation during training at each `logging_step`.\n",
    "        * **per_gpu_train_batch_size** - Batch size per GPU/CPU for training. (If `evaluate_during_training` is True, this is also the eval batch size\n",
    "        * **gradient_accumulation_steps** - Number of updates steps to accumulate before performing a backward/update pass\n",
    "        * **learning_rate** - The initial learning rate for Adam optimizer.\n",
    "        * **weight_decay** - Weight decay if we apply some.\n",
    "        * **adam_epsilon** - Epsilon for Adam optimizer.\n",
    "        * **max_grad_norm** - Max gradient norm. Duh\n",
    "        * **num_train_epochs** - Total number of training epochs to perform.\n",
    "        * **max_steps** - If > 0: set total number of training steps to perform. Override `num_train_epochs`.\n",
    "        * **warmup_steps** - Linear warmup over warmup_steps.\n",
    "        * **logging_steps** - Number of steps until logging occurs.\n",
    "        * **save_steps** - Number of steps until checkpoint is saved in `output_dir`\n",
    "        * **save_total_limit** - Limit the total amount of checkpoints, delete the older checkpoints in the `output_dir`, does not delete by default\n",
    "        * **use_tensorboard** - Only useable if tensorboard is installed\n",
    "        **return** - None\n",
    "        \"\"\"\n",
    "        # Check to overwrite\n",
    "        if (\n",
    "            os.path.exists(output_dir)\n",
    "            and os.listdir(output_dir)\n",
    "            and not overwrite_output_dir\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({output_dir}) already exists and is not empty. Set overwrite_output_dir as `True` to overcome.\"\n",
    "            )\n",
    "\n",
    "        # Check if continuing training from checkpoint\n",
    "        if should_continue:\n",
    "            sorted_checkpoints = self._sorted_checkpoints(\n",
    "                checkpoint_prefix=\"checkpoint\", output_dir=output_dir\n",
    "            )\n",
    "            if len(sorted_checkpoints) == 0:\n",
    "                raise ValueError(\n",
    "                    f\"Trying to continue training but no checkpoint was found in {output_dir}, set `should_continue` argument to False if training from scratch\"\n",
    "                )\n",
    "            else:\n",
    "                self.model_name_or_path = sorted_checkpoints[-1]\n",
    "                self._initial_setup()\n",
    "\n",
    "        # Get locals for training args\n",
    "        init_locals = copy.deepcopy(locals())\n",
    "        init_locals.pop(\"self\")\n",
    "\n",
    "        # Start logger\n",
    "        logger.info(\"Training/evaluation parameters %s\", str(locals()))\n",
    "\n",
    "        ##############\n",
    "        ## Training ##\n",
    "        ##############\n",
    "        if self.local_rank not in [-1, 0]:\n",
    "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "        # Load Dataset\n",
    "        train_dataset = self.load_and_cache_examples(evaluate=False)\n",
    "\n",
    "        if self.local_rank == 0:\n",
    "            torch.distributed.barrier()\n",
    "\n",
    "        if self.local_rank in [-1, 0] and use_tensorboard:\n",
    "            try:\n",
    "                from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "                tb_writer = SummaryWriter()\n",
    "            except ImportError:\n",
    "                logger.warning(\n",
    "                    \"WARNING! Tensorboard is a required dependency...`use_tensorboard` is now set as False\"\n",
    "                )\n",
    "                use_tensorboard = False\n",
    "                pass\n",
    "\n",
    "        # Train the model\n",
    "\n",
    "        train_batch_size = per_gpu_train_batch_size * max(1, self.n_gpu)\n",
    "\n",
    "        def collate(examples: List[torch.Tensor]):\n",
    "            if self.tokenizer._pad_token is None:\n",
    "                return pad_sequence(examples, batch_first=True)\n",
    "            return pad_sequence(\n",
    "                examples, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        train_sampler = (\n",
    "            RandomSampler(train_dataset)\n",
    "            if self.local_rank == -1\n",
    "            else DistributedSampler(train_dataset)\n",
    "        )\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=train_batch_size,\n",
    "            collate_fn=collate,\n",
    "        )\n",
    "\n",
    "        if max_steps > 0:\n",
    "            t_total = max_steps\n",
    "            num_train_epochs = (\n",
    "                max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n",
    "            )\n",
    "        else:\n",
    "            t_total = (\n",
    "                len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "            )\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon\n",
    "        )\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=learning_rate,\n",
    "            epochs=int(num_train_epochs),\n",
    "            steps_per_epoch=int(t_total),\n",
    "        )\n",
    "\n",
    "        # Check if saved optimizer or scheduler states exist\n",
    "        if (\n",
    "            self.model_name_or_path\n",
    "            and os.path.isfile(os.path.join(self.model_name_or_path, \"optimizer.pt\"))\n",
    "            and os.path.isfile(os.path.join(self.model_name_or_path, \"scheduler.pt\"))\n",
    "        ):\n",
    "            # Load in optimizer and scheduler states\n",
    "            optimizer.load_state_dict(\n",
    "                torch.load(os.path.join(self.model_name_or_path, \"optimizer.pt\"))\n",
    "            )\n",
    "            scheduler.load_state_dict(\n",
    "                torch.load(os.path.join(self.model_name_or_path, \"scheduler.pt\"))\n",
    "            )\n",
    "\n",
    "        if self.fp16:\n",
    "            try:\n",
    "                from apex import amp\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n",
    "                )\n",
    "            self.model, optimizer = amp.initialize(\n",
    "                self.model, optimizer, opt_level=self.fp16_opt_level\n",
    "            )\n",
    "\n",
    "        # multi-gpu training (should be after apex fp16 initialization)\n",
    "        if self.n_gpu > 1:\n",
    "            self.model = torch.nn.DataParallel(self.model)\n",
    "\n",
    "        # Distributed training (should be after apex fp16 initialization)\n",
    "        if self.local_rank != -1:\n",
    "            self.model = torch.nn.parallel.DistributedDataParallel(\n",
    "                self.model,\n",
    "                device_ids=[self.local_rank],\n",
    "                output_device=self.local_rank,\n",
    "                find_unused_parameters=True,\n",
    "            )\n",
    "\n",
    "        # Train!\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "        logger.info(\"  Num Epochs = %d\", num_train_epochs)\n",
    "        logger.info(\"  Instantaneous batch size per GPU = %d\", per_gpu_train_batch_size)\n",
    "        logger.info(\n",
    "            \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "            train_batch_size\n",
    "            * gradient_accumulation_steps\n",
    "            * (torch.distributed.get_world_size() if self.local_rank != -1 else 1),\n",
    "        )\n",
    "        logger.info(\"  Gradient Accumulation steps = %d\", gradient_accumulation_steps)\n",
    "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "        global_step = 0\n",
    "        epochs_trained = 0\n",
    "        steps_trained_in_current_epoch = 0\n",
    "        # Check if continuing training from a checkpoint\n",
    "        if self.model_name_or_path and os.path.exists(self.model_name_or_path):\n",
    "            try:\n",
    "                # set global_step to gobal_step of last saved checkpoint from model path\n",
    "                checkpoint_suffix = self.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "                global_step = int(checkpoint_suffix)\n",
    "                epochs_trained = global_step // (\n",
    "                    len(train_dataloader) // gradient_accumulation_steps\n",
    "                )\n",
    "                steps_trained_in_current_epoch = global_step % (\n",
    "                    len(train_dataloader) // gradient_accumulation_steps\n",
    "                )\n",
    "\n",
    "                logger.info(\n",
    "                    \"  Continuing training from checkpoint, will skip to saved global_step\"\n",
    "                )\n",
    "                logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "                logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "                logger.info(\n",
    "                    \"  Will skip the first %d steps in the first epoch\",\n",
    "                    steps_trained_in_current_epoch,\n",
    "                )\n",
    "            except ValueError:\n",
    "                logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "        tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "        model_to_resize = (\n",
    "            self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_resize.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        train_iterator = trange(\n",
    "            epochs_trained,\n",
    "            int(num_train_epochs),\n",
    "            desc=\"Epoch\",\n",
    "            disable=self.local_rank not in [-1, 0],\n",
    "        )\n",
    "        self._set_seed()  # Added here for reproducibility\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(\n",
    "                train_dataloader,\n",
    "                desc=\"Iteration\",\n",
    "                disable=self.local_rank not in [-1, 0],\n",
    "            )\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                # Skip past any already trained steps if resuming training\n",
    "                if steps_trained_in_current_epoch > 0:\n",
    "                    steps_trained_in_current_epoch -= 1\n",
    "                    continue\n",
    "\n",
    "                inputs, labels = self.mask_tokens(batch) if self.mlm else (batch, batch)\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                self.model.train()\n",
    "                outputs = (\n",
    "                    self.model(inputs, masked_lm_labels=labels)\n",
    "                    if self.mlm\n",
    "                    else self.model(inputs, labels=labels)\n",
    "                )\n",
    "                loss = outputs[\n",
    "                    0\n",
    "                ]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "                if self.n_gpu > 1:\n",
    "                    loss = (\n",
    "                        loss.mean()\n",
    "                    )  # mean() to average on multi-gpu parallel training\n",
    "                if gradient_accumulation_steps > 1:\n",
    "                    loss = loss / gradient_accumulation_steps\n",
    "\n",
    "                if self.fp16:\n",
    "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                        scaled_loss.backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                    if self.fp16:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            amp.master_params(optimizer), max_grad_norm\n",
    "                        )\n",
    "                    else:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), max_grad_norm\n",
    "                        )\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    self.model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    if (\n",
    "                        self.local_rank in [-1, 0]\n",
    "                        and logging_steps > 0\n",
    "                        and global_step % logging_steps == 0\n",
    "                    ):\n",
    "                        # Log metrics\n",
    "                        if (\n",
    "                            self.local_rank == -1 and evaluate_during_training\n",
    "                        ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                            results = self.evaluate(\n",
    "                                output_dir=output_dir,\n",
    "                                per_gpu_eval_batch_size=per_gpu_train_batch_size,\n",
    "                            )\n",
    "                            if use_tensorboard:\n",
    "                                for key, value in results.items():\n",
    "                                    tb_writer.add_scalar(\n",
    "                                        f\"eval_{key}\", value, global_step\n",
    "                                    )\n",
    "                        if use_tensorboard:\n",
    "                            tb_writer.add_scalar(\n",
    "                                \"lr\", scheduler.get_lr()[0], global_step\n",
    "                            )\n",
    "                            tb_writer.add_scalar(\n",
    "                                \"loss\",\n",
    "                                (tr_loss - logging_loss) / logging_steps,\n",
    "                                global_step,\n",
    "                            )\n",
    "                        logging_loss = tr_loss\n",
    "\n",
    "                    if (\n",
    "                        self.local_rank in [-1, 0]\n",
    "                        and save_steps > 0\n",
    "                        and global_step % save_steps == 0\n",
    "                    ):\n",
    "                        checkpoint_prefix = \"checkpoint\"\n",
    "                        # Save model checkpoint\n",
    "                        # TODO: os.makedirs bug when output_dir exists\n",
    "                        ckpt_output_dir = os.path.join(\n",
    "                            output_dir, f\"{checkpoint_prefix}-{global_step}\"\n",
    "                        )\n",
    "                        os.makedirs(ckpt_output_dir, exist_ok=True)\n",
    "                        model_to_save = (\n",
    "                            self.model.module\n",
    "                            if hasattr(self.model, \"module\")\n",
    "                            else self.model\n",
    "                        )  # Take care of distributed/parallel training\n",
    "                        model_to_save.save_pretrained(ckpt_output_dir)\n",
    "                        self.tokenizer.save_pretrained(ckpt_output_dir)\n",
    "\n",
    "                        torch.save(\n",
    "                            init_locals,\n",
    "                            os.path.join(ckpt_output_dir, \"training_args.bin\"),\n",
    "                        )\n",
    "                        logger.info(\"Saving model checkpoint to %s\", ckpt_output_dir)\n",
    "\n",
    "                        self._rotate_checkpoints(\n",
    "                            checkpoint_prefix,\n",
    "                            save_total_limit=save_total_limit,\n",
    "                            output_dir=output_dir,\n",
    "                        )\n",
    "\n",
    "                        torch.save(\n",
    "                            optimizer.state_dict(),\n",
    "                            os.path.join(ckpt_output_dir, \"optimizer.pt\"),\n",
    "                        )\n",
    "                        torch.save(\n",
    "                            scheduler.state_dict(),\n",
    "                            os.path.join(ckpt_output_dir, \"scheduler.pt\"),\n",
    "                        )\n",
    "                        logger.info(\n",
    "                            \"Saving optimizer and scheduler states to %s\",\n",
    "                            ckpt_output_dir,\n",
    "                        )\n",
    "\n",
    "                if max_steps > 0 and global_step > max_steps:\n",
    "                    epoch_iterator.close()\n",
    "                    break\n",
    "            if max_steps > 0 and global_step > max_steps:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "\n",
    "        if self.local_rank in [-1, 0] and use_tensorboard:\n",
    "            tb_writer.close()\n",
    "\n",
    "        tr_loss = tr_loss / global_step\n",
    "\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "        # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "        if self.local_rank == -1 or torch.distributed.get_rank() == 0:\n",
    "            # Create output directory if needed\n",
    "            if self.local_rank in [-1, 0]:\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "            # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "            # They can then be reloaded using `from_pretrained()`\n",
    "            model_to_save = (\n",
    "                self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "            )  # Take care of distributed/parallel training\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            # Good practice: save your training arguments together with the trained model\n",
    "            torch.save(init_locals, os.path.join(output_dir, \"training_args.bin\"))\n",
    "\n",
    "            # Load a trained model and vocabulary that you have fine-tuned\n",
    "            config_class, model_class, tokenizer_class = self.MODEL_CLASSES[\n",
    "                self.model_type\n",
    "            ]\n",
    "            self.model = model_class.from_pretrained(output_dir)\n",
    "            self.tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        output_dir: str,\n",
    "        should_continue: bool = False,\n",
    "        overwrite_output_dir: bool = False,\n",
    "        evaluate_during_training: bool = False,\n",
    "        per_gpu_train_batch_size: int = 4,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        learning_rate: float = 5e-5,\n",
    "        weight_decay: float = 0.0,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        max_grad_norm: float = 1.0,\n",
    "        num_train_epochs: float = 1.0,\n",
    "        max_steps: int = -1,\n",
    "        warmup_steps: int = 0,\n",
    "        logging_steps: int = 50,\n",
    "        save_steps: int = 50,\n",
    "        save_total_limit: int = 3,\n",
    "        use_tensorboard: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        * **output_dir** - The output directory where the model predictions and checkpoints will be written.\n",
    "        * **should_continue** - Whether to continue training from latest checkpoint in `output_dir`\n",
    "        * **overwrite_output_dir** - Overwrite the content of output directory `output_dir`\n",
    "        * **evaluate_during_training** - Run evaluation during training at each `logging_step`.\n",
    "        * **per_gpu_train_batch_size** - Batch size per GPU/CPU for training. (If `evaluate_during_training` is True, this is also the eval batch size\n",
    "        * **gradient_accumulation_steps** - Number of updates steps to accumulate before performing a backward/update pass\n",
    "        * **learning_rate** - The initial learning rate for Adam optimizer.\n",
    "        * **weight_decay** - Weight decay if we apply some.\n",
    "        * **adam_epsilon** - Epsilon for Adam optimizer.\n",
    "        * **max_grad_norm** - Max gradient norm. Duh\n",
    "        * **num_train_epochs** - Total number of training epochs to perform.\n",
    "        * **max_steps** - If > 0: set total number of training steps to perform. Override `num_train_epochs`.\n",
    "        * **warmup_steps** - Linear warmup over warmup_steps.\n",
    "        * **logging_steps** - Number of steps until logging occurs.\n",
    "        * **save_steps** - Number of steps until checkpoint is saved in `output_dir`\n",
    "        * **save_total_limit** - Limit the total amount of checkpoints, delete the older checkpoints in the `output_dir`, does not delete by default\n",
    "        * **use_tensorboard** - Only useable if tensorboard is installed\n",
    "        **return** - None\n",
    "        \"\"\"\n",
    "        # Check to overwrite\n",
    "        if (\n",
    "            os.path.exists(output_dir)\n",
    "            and os.listdir(output_dir)\n",
    "            and not overwrite_output_dir\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({output_dir}) already exists and is not empty. Set overwrite_output_dir as `True` to overcome.\"\n",
    "            )\n",
    "\n",
    "        # Check if continuing training from checkpoint\n",
    "        if should_continue:\n",
    "            sorted_checkpoints = self._sorted_checkpoints(\n",
    "                checkpoint_prefix=\"checkpoint\", output_dir=output_dir\n",
    "            )\n",
    "            if len(sorted_checkpoints) == 0:\n",
    "                raise ValueError(\n",
    "                    f\"Trying to continue training but no checkpoint was found in {output_dir}, set `should_continue` argument to False if training from scratch\"\n",
    "                )\n",
    "            else:\n",
    "                self.model_name_or_path = sorted_checkpoints[-1]\n",
    "                self._initial_setup()\n",
    "\n",
    "        # Get locals for training args\n",
    "        init_locals = copy.deepcopy(locals())\n",
    "        init_locals.pop(\"self\")\n",
    "\n",
    "        # Start logger\n",
    "        logger.info(\"Training/evaluation parameters %s\", str(locals()))\n",
    "\n",
    "        ##############\n",
    "        ## Training ##\n",
    "        ##############\n",
    "        if self.local_rank not in [-1, 0]:\n",
    "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "        # Load Dataset\n",
    "        train_dataset = self.load_and_cache_examples(evaluate=False)\n",
    "\n",
    "        if self.local_rank == 0:\n",
    "            torch.distributed.barrier()\n",
    "\n",
    "        if self.local_rank in [-1, 0] and use_tensorboard:\n",
    "            try:\n",
    "                from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "                tb_writer = SummaryWriter()\n",
    "            except ImportError:\n",
    "                logger.warning(\n",
    "                    \"WARNING! Tensorboard is a required dependency...`use_tensorboard` is now set as False\"\n",
    "                )\n",
    "                use_tensorboard = False\n",
    "                pass\n",
    "\n",
    "        # Train the model\n",
    "\n",
    "        train_batch_size = per_gpu_train_batch_size * max(1, self.n_gpu)\n",
    "\n",
    "        def collate(examples: List[torch.Tensor]):\n",
    "            if self.tokenizer._pad_token is None:\n",
    "                return pad_sequence(examples, batch_first=True)\n",
    "            return pad_sequence(\n",
    "                examples, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        train_sampler = (\n",
    "            RandomSampler(train_dataset)\n",
    "            if self.local_rank == -1\n",
    "            else DistributedSampler(train_dataset)\n",
    "        )\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=train_batch_size,\n",
    "            collate_fn=collate,\n",
    "        )\n",
    "\n",
    "        if max_steps > 0:\n",
    "            t_total = max_steps\n",
    "            num_train_epochs = (\n",
    "                max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n",
    "            )\n",
    "        else:\n",
    "            t_total = (\n",
    "                len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "            )\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "\n",
    "        # Check if saved optimizer or scheduler states exist\n",
    "        if (\n",
    "            self.model_name_or_path\n",
    "            and os.path.isfile(os.path.join(self.model_name_or_path, \"optimizer.pt\"))\n",
    "            and os.path.isfile(os.path.join(self.model_name_or_path, \"scheduler.pt\"))\n",
    "        ):\n",
    "            # Load in optimizer and scheduler states\n",
    "            optimizer.load_state_dict(\n",
    "                torch.load(os.path.join(self.model_name_or_path, \"optimizer.pt\"))\n",
    "            )\n",
    "            scheduler.load_state_dict(\n",
    "                torch.load(os.path.join(self.model_name_or_path, \"scheduler.pt\"))\n",
    "            )\n",
    "\n",
    "        if self.fp16:\n",
    "            try:\n",
    "                from apex import amp\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n",
    "                )\n",
    "            self.model, optimizer = amp.initialize(\n",
    "                self.model, optimizer, opt_level=self.fp16_opt_level\n",
    "            )\n",
    "\n",
    "        # multi-gpu training (should be after apex fp16 initialization)\n",
    "        if self.n_gpu > 1:\n",
    "            self.model = torch.nn.DataParallel(self.model)\n",
    "\n",
    "        # Distributed training (should be after apex fp16 initialization)\n",
    "        if self.local_rank != -1:\n",
    "            self.model = torch.nn.parallel.DistributedDataParallel(\n",
    "                self.model,\n",
    "                device_ids=[self.local_rank],\n",
    "                output_device=self.local_rank,\n",
    "                find_unused_parameters=True,\n",
    "            )\n",
    "\n",
    "        # Train!\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "        logger.info(\"  Num Epochs = %d\", num_train_epochs)\n",
    "        logger.info(\"  Instantaneous batch size per GPU = %d\", per_gpu_train_batch_size)\n",
    "        logger.info(\n",
    "            \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "            train_batch_size\n",
    "            * gradient_accumulation_steps\n",
    "            * (torch.distributed.get_world_size() if self.local_rank != -1 else 1),\n",
    "        )\n",
    "        logger.info(\"  Gradient Accumulation steps = %d\", gradient_accumulation_steps)\n",
    "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "        global_step = 0\n",
    "        epochs_trained = 0\n",
    "        steps_trained_in_current_epoch = 0\n",
    "        # Check if continuing training from a checkpoint\n",
    "        if self.model_name_or_path and os.path.exists(self.model_name_or_path):\n",
    "            try:\n",
    "                # set global_step to gobal_step of last saved checkpoint from model path\n",
    "                checkpoint_suffix = self.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "                global_step = int(checkpoint_suffix)\n",
    "                epochs_trained = global_step // (\n",
    "                    len(train_dataloader) // gradient_accumulation_steps\n",
    "                )\n",
    "                steps_trained_in_current_epoch = global_step % (\n",
    "                    len(train_dataloader) // gradient_accumulation_steps\n",
    "                )\n",
    "\n",
    "                logger.info(\n",
    "                    \"  Continuing training from checkpoint, will skip to saved global_step\"\n",
    "                )\n",
    "                logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "                logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "                logger.info(\n",
    "                    \"  Will skip the first %d steps in the first epoch\",\n",
    "                    steps_trained_in_current_epoch,\n",
    "                )\n",
    "            except ValueError:\n",
    "                logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "        tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "        model_to_resize = (\n",
    "            self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_resize.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        train_iterator = trange(\n",
    "            epochs_trained,\n",
    "            int(num_train_epochs),\n",
    "            desc=\"Epoch\",\n",
    "            disable=self.local_rank not in [-1, 0],\n",
    "        )\n",
    "        self._set_seed()  # Added here for reproducibility\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(\n",
    "                train_dataloader,\n",
    "                desc=\"Iteration\",\n",
    "                disable=self.local_rank not in [-1, 0],\n",
    "            )\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                # Skip past any already trained steps if resuming training\n",
    "                if steps_trained_in_current_epoch > 0:\n",
    "                    steps_trained_in_current_epoch -= 1\n",
    "                    continue\n",
    "\n",
    "                inputs, labels = self.mask_tokens(batch) if self.mlm else (batch, batch)\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                self.model.train()\n",
    "                outputs = (\n",
    "                    self.model(inputs, masked_lm_labels=labels)\n",
    "                    if self.mlm\n",
    "                    else self.model(inputs, labels=labels)\n",
    "                )\n",
    "                loss = outputs[\n",
    "                    0\n",
    "                ]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "                if self.n_gpu > 1:\n",
    "                    loss = (\n",
    "                        loss.mean()\n",
    "                    )  # mean() to average on multi-gpu parallel training\n",
    "                if gradient_accumulation_steps > 1:\n",
    "                    loss = loss / gradient_accumulation_steps\n",
    "\n",
    "                if self.fp16:\n",
    "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                        scaled_loss.backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                    if self.fp16:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            amp.master_params(optimizer), max_grad_norm\n",
    "                        )\n",
    "                    else:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), max_grad_norm\n",
    "                        )\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    self.model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    if (\n",
    "                        self.local_rank in [-1, 0]\n",
    "                        and logging_steps > 0\n",
    "                        and global_step % logging_steps == 0\n",
    "                    ):\n",
    "                        # Log metrics\n",
    "                        if (\n",
    "                            self.local_rank == -1 and evaluate_during_training\n",
    "                        ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                            results = self.evaluate(\n",
    "                                output_dir=output_dir,\n",
    "                                per_gpu_eval_batch_size=per_gpu_train_batch_size,\n",
    "                            )\n",
    "                            if use_tensorboard:\n",
    "                                for key, value in results.items():\n",
    "                                    tb_writer.add_scalar(\n",
    "                                        f\"eval_{key}\", value, global_step\n",
    "                                    )\n",
    "                        if use_tensorboard:\n",
    "                            tb_writer.add_scalar(\n",
    "                                \"lr\", scheduler.get_lr()[0], global_step\n",
    "                            )\n",
    "                            tb_writer.add_scalar(\n",
    "                                \"loss\",\n",
    "                                (tr_loss - logging_loss) / logging_steps,\n",
    "                                global_step,\n",
    "                            )\n",
    "                        logging_loss = tr_loss\n",
    "\n",
    "                    if (\n",
    "                        self.local_rank in [-1, 0]\n",
    "                        and save_steps > 0\n",
    "                        and global_step % save_steps == 0\n",
    "                    ):\n",
    "                        checkpoint_prefix = \"checkpoint\"\n",
    "                        # Save model checkpoint\n",
    "                        # TODO: os.makedirs bug when output_dir exists\n",
    "                        ckpt_output_dir = os.path.join(\n",
    "                            output_dir, f\"{checkpoint_prefix}-{global_step}\"\n",
    "                        )\n",
    "                        os.makedirs(ckpt_output_dir, exist_ok=True)\n",
    "                        model_to_save = (\n",
    "                            self.model.module\n",
    "                            if hasattr(self.model, \"module\")\n",
    "                            else self.model\n",
    "                        )  # Take care of distributed/parallel training\n",
    "                        model_to_save.save_pretrained(ckpt_output_dir)\n",
    "                        self.tokenizer.save_pretrained(ckpt_output_dir)\n",
    "\n",
    "                        torch.save(\n",
    "                            init_locals,\n",
    "                            os.path.join(ckpt_output_dir, \"training_args.bin\"),\n",
    "                        )\n",
    "                        logger.info(\"Saving model checkpoint to %s\", ckpt_output_dir)\n",
    "\n",
    "                        self._rotate_checkpoints(\n",
    "                            checkpoint_prefix,\n",
    "                            save_total_limit=save_total_limit,\n",
    "                            output_dir=output_dir,\n",
    "                        )\n",
    "\n",
    "                        torch.save(\n",
    "                            optimizer.state_dict(),\n",
    "                            os.path.join(ckpt_output_dir, \"optimizer.pt\"),\n",
    "                        )\n",
    "                        torch.save(\n",
    "                            scheduler.state_dict(),\n",
    "                            os.path.join(ckpt_output_dir, \"scheduler.pt\"),\n",
    "                        )\n",
    "                        logger.info(\n",
    "                            \"Saving optimizer and scheduler states to %s\",\n",
    "                            ckpt_output_dir,\n",
    "                        )\n",
    "\n",
    "                if max_steps > 0 and global_step > max_steps:\n",
    "                    epoch_iterator.close()\n",
    "                    break\n",
    "            if max_steps > 0 and global_step > max_steps:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "\n",
    "        if self.local_rank in [-1, 0] and use_tensorboard:\n",
    "            tb_writer.close()\n",
    "\n",
    "        tr_loss = tr_loss / global_step\n",
    "\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "        # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "        if self.local_rank == -1 or torch.distributed.get_rank() == 0:\n",
    "            # Create output directory if needed\n",
    "            if self.local_rank in [-1, 0]:\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "            # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "            # They can then be reloaded using `from_pretrained()`\n",
    "            model_to_save = (\n",
    "                self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "            )  # Take care of distributed/parallel training\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            # Good practice: save your training arguments together with the trained model\n",
    "            torch.save(init_locals, os.path.join(output_dir, \"training_args.bin\"))\n",
    "\n",
    "            # Load a trained model and vocabulary that you have fine-tuned\n",
    "            config_class, model_class, tokenizer_class = self.MODEL_CLASSES[\n",
    "                self.model_type\n",
    "            ]\n",
    "            self.model = model_class.from_pretrained(output_dir)\n",
    "            self.tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def evaluate_all_checkpoints(\n",
    "        self,\n",
    "        output_dir: str,\n",
    "        per_gpu_eval_batch_size: int = 4,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        * **output_dir** - The output directory where the model predictions and checkpoints will be written.\n",
    "        * **per_gpu_eval_batch_size** - Batch size per GPU/CPU for evaluation.\n",
    "        **return** - Results in a dictionary\n",
    "        \"\"\"\n",
    "        # Evaluation\n",
    "        results = {}\n",
    "        if self.local_rank in [-1, 0]:\n",
    "            # checkpoints = [output_dir]\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c)\n",
    "                for c in sorted(\n",
    "                    glob.glob(output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True)\n",
    "                )\n",
    "            )\n",
    "            print(checkpoints)\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(\n",
    "                logging.WARN\n",
    "            )  # Reduce logging\n",
    "            logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "            for checkpoint in checkpoints:\n",
    "                global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "                prefix = (\n",
    "                    checkpoint.split(\"/\")[-1]\n",
    "                    if checkpoint.find(\"checkpoint\") != -1\n",
    "                    else \"\"\n",
    "                )\n",
    "\n",
    "                config_class, model_class, tokenizer_class = self.MODEL_CLASSES[\n",
    "                    self.model_type\n",
    "                ]\n",
    "                self.model = model_class.from_pretrained(checkpoint)\n",
    "                self.model.to(self.device)\n",
    "                result = self.evaluate(\n",
    "                    output_dir=output_dir,\n",
    "                    per_gpu_eval_batch_size=per_gpu_eval_batch_size,\n",
    "                    prefix=prefix,\n",
    "                )\n",
    "                result = dict((k + f\"_{global_step}\", v) for k, v in result.items())\n",
    "                results.update(result)\n",
    "        return results\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        output_dir: str,\n",
    "        per_gpu_eval_batch_size: int = 4,\n",
    "        prefix: str = \"\",\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        * **output_dir** - The output directory where the model predictions and checkpoints will be written.\n",
    "        * **per_gpu_eval_batch_size** - Batch size per GPU/CPU for evaluation.\n",
    "        * **prefix** - Prefix of checkpoint i.e. \"checkpoint-50\"\n",
    "        **return** - Results in a dictionary\n",
    "        \"\"\"\n",
    "        # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "        eval_output_dir = output_dir\n",
    "\n",
    "        eval_dataset = self.load_and_cache_examples(evaluate=True)\n",
    "\n",
    "        if self.local_rank in [-1, 0]:\n",
    "            os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "        eval_batch_size = per_gpu_eval_batch_size * max(1, self.n_gpu)\n",
    "\n",
    "        # Note that DistributedSampler samples randomly\n",
    "        def collate(examples: List[torch.Tensor]):\n",
    "            if self.tokenizer._pad_token is None:\n",
    "                return pad_sequence(examples, batch_first=True)\n",
    "            return pad_sequence(\n",
    "                examples, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        eval_sampler = SequentialSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(\n",
    "            eval_dataset,\n",
    "            sampler=eval_sampler,\n",
    "            batch_size=eval_batch_size,\n",
    "            collate_fn=collate,\n",
    "        )\n",
    "\n",
    "        # multi-gpu evaluate\n",
    "        if self.n_gpu > 1:\n",
    "            self.model = torch.nn.DataParallel(self.model)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(f\"***** Running evaluation {prefix} *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "        logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            inputs, labels = self.mask_tokens(batch) if self.mlm else (batch, batch)\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = (\n",
    "                    self.model(inputs, masked_lm_labels=labels)\n",
    "                    if self.mlm\n",
    "                    else self.model(inputs, labels=labels)\n",
    "                )\n",
    "                lm_loss = outputs[0]\n",
    "                eval_loss += lm_loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "        result = {\"perplexity\": perplexity}\n",
    "\n",
    "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(f\"***** Eval results {prefix} *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def load_and_cache_examples(self, evaluate=False):\n",
    "        return TextDataset(\n",
    "            self.tokenizer,\n",
    "            self.model_type,\n",
    "            self.overwrite_cache,\n",
    "            file_path=self.eval_data_file if evaluate else self.train_data_file,\n",
    "            block_size=self.block_size,\n",
    "        )\n",
    "\n",
    "    def _set_seed(self):\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        if self.n_gpu > 0:\n",
    "            torch.cuda.manual_seed_all(self.seed)\n",
    "\n",
    "    def _sorted_checkpoints(\n",
    "        self, checkpoint_prefix: str, output_dir: str, use_mtime=False\n",
    "    ) -> List[str]:\n",
    "        ordering_and_checkpoint_path = []\n",
    "\n",
    "        glob_checkpoints = glob.glob(os.path.join(output_dir, f\"{checkpoint_prefix}-*\"))\n",
    "\n",
    "        for path in glob_checkpoints:\n",
    "            if use_mtime:\n",
    "                ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "            else:\n",
    "                regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "                if regex_match and regex_match.groups():\n",
    "                    ordering_and_checkpoint_path.append(\n",
    "                        (int(regex_match.groups()[0]), path)\n",
    "                    )\n",
    "\n",
    "        checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "        checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "        return checkpoints_sorted\n",
    "\n",
    "    def _rotate_checkpoints(\n",
    "        self,\n",
    "        checkpoint_prefix: str,\n",
    "        save_total_limit: int,\n",
    "        output_dir: str,\n",
    "        use_mtime: bool = False,\n",
    "    ) -> None:\n",
    "        if not save_total_limit:\n",
    "            return\n",
    "        if save_total_limit <= 0:\n",
    "            return\n",
    "\n",
    "        # Check if we should delete older checkpoint(s)\n",
    "        checkpoints_sorted = self._sorted_checkpoints(\n",
    "            checkpoint_prefix, output_dir, use_mtime\n",
    "        )\n",
    "        if len(checkpoints_sorted) <= save_total_limit:\n",
    "            return\n",
    "        number_of_checkpoints_to_delete = max(\n",
    "            0, len(checkpoints_sorted) - save_total_limit\n",
    "        )\n",
    "        checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "        for checkpoint in checkpoints_to_be_deleted:\n",
    "            logger.info(\n",
    "                f\"Deleting older checkpoint [{checkpoint}] due to save_total_limit\"\n",
    "            )\n",
    "            shutil.rmtree(checkpoint)\n",
    "\n",
    "    def mask_tokens(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Set `mlm` param as False\"\n",
    "            )\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "            for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(\n",
    "            torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0\n",
    "        )\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = (\n",
    "            torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        )\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(\n",
    "            self.tokenizer.mask_token\n",
    "        )\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = (\n",
    "            torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n",
    "            & masked_indices\n",
    "            & ~indices_replaced\n",
    "        )\n",
    "        random_words = torch.randint(\n",
    "            len(self.tokenizer), labels.shape, dtype=torch.long\n",
    "        )\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "    def find_learning_rate(\n",
    "        self,\n",
    "        output_dir: Union[Path, str],\n",
    "        file_name: str = \"learning_rate.tsv\",\n",
    "        start_learning_rate: float = 1e-7,\n",
    "        end_learning_rate: float = 10,\n",
    "        iterations: int = 100,\n",
    "        mini_batch_size: int = 8,\n",
    "        stop_early: bool = True,\n",
    "        smoothing_factor: float = 0.7,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        weight_decay: float = 0.0,\n",
    "        **kwargs,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Uses Leslie's cyclical learning rate finding method to generate and save the loss x learning rate plot\n",
    "\n",
    "        This method returns a suggested learning rate using the static method `LMFineTuner.suggest_learning_rate()`\n",
    "        which is implicitly run in this method.\n",
    "\n",
    "        * **output_dir** - Path to dir for learning rate file to be saved\n",
    "        * **file_name** - Name of learning rate .tsv file\n",
    "        * **start_learning_rate** - Initial learning rate to start cyclical learning rate finder method\n",
    "        * **end_learning_rate** - End learning rate to stop exponential increase of the learning rate\n",
    "        * **iterations** - Number of optimizer iterations for the ExpAnnealLR scheduler\n",
    "        * **mini_batch_size** - Batch size for dataloader\n",
    "        * **stop_early** - Bool for stopping early once loss diverges\n",
    "        * **smoothing_factor** - Smoothing factor on moving average of losses\n",
    "        * **adam_epsilon** - Epsilon for Adam optimizer.\n",
    "        * **weight_decay** - Weight decay if we apply some.\n",
    "        * **kwargs** - Additional keyword arguments for the Adam optimizer\n",
    "        **return** - Learning rate as a float\n",
    "        \"\"\"\n",
    "        best_loss = None\n",
    "        moving_avg_loss = 0\n",
    "\n",
    "        # cast string to Path\n",
    "        if type(output_dir) is str:\n",
    "            output_dir = Path(output_dir)\n",
    "        from flair.training_utils import (\n",
    "            init_output_file,\n",
    "            log_line,\n",
    "        )\n",
    "\n",
    "        learning_rate_tsv = init_output_file(output_dir, file_name)\n",
    "\n",
    "        with open(learning_rate_tsv, \"a\") as f:\n",
    "            f.write(\"ITERATION\\tTIMESTAMP\\tLEARNING_RATE\\tTRAIN_LOSS\\n\")\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay) for transformer's AdamW optimzer\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=start_learning_rate,\n",
    "            eps=adam_epsilon,\n",
    "            **kwargs,\n",
    "        )\n",
    "        ## Original SGD optimizer Flair uses commnted out\n",
    "        # optimizer = AdamW(self.model.parameters(), lr=start_learning_rate, **kwargs)\n",
    "        # from torch.optim.sgd import SGD\n",
    "        # optimizer = SGD(\n",
    "        #    self.model.parameters(), lr=start_learning_rate, **kwargs\n",
    "        # )\n",
    "\n",
    "        # Flair's original EXPAnnealLR scheduler\n",
    "        from flair.optim import ExpAnnealLR\n",
    "\n",
    "        scheduler = ExpAnnealLR(optimizer, end_learning_rate, iterations)\n",
    "\n",
    "        model_state = self.model.state_dict()\n",
    "        self.model.train()\n",
    "\n",
    "        train_dataset = self.load_and_cache_examples(evaluate=False)\n",
    "\n",
    "        step = 0\n",
    "        while step < iterations:\n",
    "            train_sampler = RandomSampler(train_dataset)\n",
    "            batch_loader = DataLoader(\n",
    "                train_dataset, sampler=train_sampler, batch_size=mini_batch_size\n",
    "            )\n",
    "            for batch in batch_loader:\n",
    "                step += 1\n",
    "\n",
    "                # forward pass\n",
    "                inputs, labels = self.mask_tokens(batch) if self.mlm else (batch, batch)\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                self.model.train()\n",
    "                outputs = (\n",
    "                    self.model(inputs, masked_lm_labels=labels)\n",
    "                    if self.mlm\n",
    "                    else self.model(inputs, labels=labels)\n",
    "                )\n",
    "                loss = outputs[0]\n",
    "\n",
    "                # update optimizer and scheduler\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step(step)\n",
    "\n",
    "                print(scheduler.get_lr())\n",
    "                learning_rate = scheduler.get_lr()[0]\n",
    "\n",
    "                loss_item = loss.item()\n",
    "                if step == 1:\n",
    "                    best_loss = loss_item\n",
    "                else:\n",
    "                    if smoothing_factor > 0:\n",
    "                        moving_avg_loss = (\n",
    "                            smoothing_factor * moving_avg_loss\n",
    "                            + (1 - smoothing_factor) * loss_item\n",
    "                        )\n",
    "                        loss_item = moving_avg_loss / (\n",
    "                            1 - smoothing_factor ** (step + 1)\n",
    "                        )\n",
    "                    if loss_item < best_loss:\n",
    "                        best_loss = loss\n",
    "\n",
    "                if step > iterations:\n",
    "                    break\n",
    "\n",
    "                if stop_early and (loss_item > 4 * best_loss or torch.isnan(loss)):\n",
    "                    log_line(logger)\n",
    "                    logger.info(\"loss diverged - stopping early!\")\n",
    "                    step = iterations\n",
    "                    break\n",
    "\n",
    "                with open(str(learning_rate_tsv), \"a\") as f:\n",
    "                    f.write(\n",
    "                        f\"{step}\\t{datetime.datetime.now():%H:%M:%S}\\t{learning_rate}\\t{loss_item}\\n\"\n",
    "                    )\n",
    "\n",
    "            self.model.load_state_dict(model_state)\n",
    "            self.model.to(self.device)\n",
    "\n",
    "        log_line(logger)\n",
    "        logger.info(\n",
    "            f\"learning rate finder finished - plot {learning_rate_tsv} \\n Reinitalizing model's parameters and optimizer\"\n",
    "        )\n",
    "        log_line(logger)\n",
    "\n",
    "        # Reinitialize transformers model's parameters (This could be optimized)\n",
    "        self._initial_setup()\n",
    "\n",
    "        plotter = Plotter()\n",
    "        plotter.plot_learning_rate(Path(learning_rate_tsv))\n",
    "\n",
    "        # Use the automated learning rate finder\n",
    "        with open(learning_rate_tsv) as lr_f:\n",
    "            lr_tsv = list(csv.reader(lr_f, delimiter=\"\\t\"))\n",
    "        losses = np.array([float(row[-1]) for row in lr_tsv[1:]])\n",
    "        lrs = np.array([float(row[-2]) for row in lr_tsv[1:]])\n",
    "        lr_to_use = self.suggest_learning_rate(losses, lrs)\n",
    "        print(f\"Recommended Learning Rate {lr_to_use}\")\n",
    "        return lr_to_use\n",
    "\n",
    "    @staticmethod\n",
    "    def suggest_learning_rate(\n",
    "        losses: np.array,\n",
    "        lrs: np.array,\n",
    "        lr_diff: int = 30,\n",
    "        loss_threshold: float = 0.05,\n",
    "        adjust_value: float = 1,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method\n",
    "\n",
    "        * **losses** - Numpy array of losses\n",
    "        * **lrs** - Numpy array of exponentially increasing learning rates (must match dim of `losses`)\n",
    "        * **lr_diff** - Learning rate Interval of slide ruler\n",
    "        * **loss_threshold** - Threshold of loss difference on interval where the sliding stops\n",
    "        * **adjust_value** - Coefficient for adjustment\n",
    "        **return** - the optimal learning rate as a float\n",
    "        \"\"\"\n",
    "        # Get loss values and their corresponding gradients, and get lr values\n",
    "        assert lr_diff < len(losses)\n",
    "        loss_grad = np.gradient(losses)\n",
    "\n",
    "        # Search for index in gradients where loss is lowest before the loss spike\n",
    "        # Initialize right and left idx using the lr_diff as a spacing unit\n",
    "        # Set the local min lr as -1 to signify if threshold is too low\n",
    "        r_idx = -1\n",
    "        l_idx = r_idx - lr_diff\n",
    "        local_min_lr = lrs[l_idx]\n",
    "        while (l_idx >= -len(losses)) and (\n",
    "            abs(loss_grad[r_idx] - loss_grad[l_idx]) > loss_threshold\n",
    "        ):\n",
    "            local_min_lr = lrs[l_idx]\n",
    "            r_idx -= 1\n",
    "            l_idx -= 1\n",
    "\n",
    "        lr_to_use = local_min_lr * adjust_value\n",
    "\n",
    "        return lr_to_use\n",
    "\n",
    "    def freeze_to(self, n: int) -> None:\n",
    "        \"\"\"Freeze first n layers of model\n",
    "\n",
    "        * **n** - Starting from initial layer, freeze all layers up to nth layer inclusively\n",
    "        \"\"\"\n",
    "        layers = list(self.model.parameters())\n",
    "        # Freeze up to n layers\n",
    "        for param in layers[:n]:\n",
    "            param.requires_grad = False\n",
    "        for param in layers[n:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        \"\"\"Freeze last classification layer group only\"\"\"\n",
    "        layers_len = len(list(self.model.cls.parameters()))\n",
    "        self.freeze_to(-layers_len)\n",
    "\n",
    "    def unfreeze(self) -> None:\n",
    "        \"\"\"Unfreeze all layers\"\"\"\n",
    "        self.freeze_to(0)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
