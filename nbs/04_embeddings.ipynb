{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "> AdaptNLP Embeddings Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import test_eq\n",
    "from fastcore.xtras import is_listy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import logging, torch\n",
    "from typing import List, Dict, Union\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "from fastcore.basics import mk_class\n",
    "from fastcore.xtras import dict2obj\n",
    "from fastcore.dispatch import typedispatch\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import (\n",
    "    Embeddings,\n",
    "    WordEmbeddings,\n",
    "    StackedEmbeddings,\n",
    "    FlairEmbeddings,\n",
    "    DocumentPoolEmbeddings,\n",
    "    DocumentRNNEmbeddings,\n",
    "    TransformerWordEmbeddings,\n",
    ")\n",
    "\n",
    "from adaptnlp.model_hub import FlairModelHub, HFModelHub, FlairModelResult, HFModelResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_flair_hub = FlairModelHub()\n",
    "_hf_hub = HFModelHub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@typedispatch\n",
    "def _make_sentences(text:str, as_list=False) -> Union[List[Sentence], Sentence]:\n",
    "    return [Sentence(text)] if as_list else Sentence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_sentences = 'a,b,c'.split(',')\n",
    "out = _make_sentences(test_sentences)\n",
    "tst_out = [Sentence('a'), Sentence('b'), Sentence('c')]\n",
    "for o,t in zip(out, tst_out):\n",
    "    test_eq(o[0].text, t[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@typedispatch\n",
    "def _make_sentences(text:list, as_list=False) -> Union[List[Sentence], Sentence]:\n",
    "    if all(isinstance(t,str) for t in text):\n",
    "        return [Sentence(t) for t in text]\n",
    "    elif all(isinstance(t, Sentence) for t in text):\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_sentence = 'My name is Zach'\n",
    "out = _make_sentences(test_sentence, as_list=True)\n",
    "tst_out = [Sentence(test_sentence)]\n",
    "for o,t in zip(out, tst_out):\n",
    "    test_eq(o[0].text, t[0].text)\n",
    "test_eq(is_listy(out), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@typedispatch\n",
    "def _make_sentences(text:Sentence, as_list=False) -> Union[List[Sentence], Sentence]:\n",
    "    return [text] if as_list else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_sentence = Sentence('Me')\n",
    "out = _make_sentences(test_sentence)\n",
    "test_eq(test_sentence[0].text, out[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@typedispatch\n",
    "def _get_embedding_model(model_name_or_path:HFModelResult) -> TransformerWordEmbeddings:\n",
    "    return TransformerWordEmbeddings(model_name_or_path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@typedispatch\n",
    "def _get_embedding_model(model_name_or_path:FlairModelResult) -> Union[FlairEmbeddings, WordEmbeddings]:\n",
    "    nm = model_name_or_path.name\n",
    "    try:\n",
    "        return WordEmbeddings(nm.strip('flairNLP/'))\n",
    "    except:\n",
    "        return FlairEmbeddings(nm.strip('flairNLP/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@typedispatch\n",
    "def _get_embedding_model(model_name_or_path:str) -> Union[TransformerWordEmbeddings, WordEmbeddings, FlairEmbeddings]:\n",
    "    res = _flair_hub.search_model_by_name(model_name_or_path, user_uploaded=True)\n",
    "    if len(res) < 1:\n",
    "        # No models found\n",
    "        res = _hf_hub.search_model_by_name(model_name_or_path, user_uploaded=True)\n",
    "        if len(res) < 1:\n",
    "            raise ValueError(f'Embeddings not found for the model key: {model_name_or_path}, check documentation or custom model path to verify specified model')\n",
    "        else:\n",
    "            return TransformerWordEmbeddings(res[0].name) # Returning the first should always be the non-fast option\n",
    "    else:\n",
    "        nm = res[0].name\n",
    "        try:\n",
    "            return WordEmbeddings(nm.strip('flairNLP/'))\n",
    "        except:\n",
    "            return FlairEmbeddings(nm.strip('flairNLP/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.basics import mk_class\n",
    "mk_class('DetailLevel', **{o:o.lower() for o in 'High,Medium,Low'.split(',')},\n",
    "         doc=\"All possible naming conventions for DetailLevel with typo-proofing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EmbeddingResult:\n",
    "    \"\"\"\n",
    "    A result class designed for Embedding models\n",
    "    \"\"\"\n",
    "    def __init__(self, sentence:Sentence):\n",
    "        self._sentence = sentence\n",
    "    \n",
    "    @property\n",
    "    def sentence_embeddings(self) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        All embeddings in `sentence` (if available)\n",
    "        \"\"\"\n",
    "        return self._sentence.get_embedding()\n",
    "    \n",
    "    @property\n",
    "    def token_embeddings(self) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        All embeddings from the individual tokens in `sentence` with original order in shape (n, embed_dim)\n",
    "        \"\"\"\n",
    "        return torch.stack([tok.get_embedding() for tok in self._sentence], dim=0)\n",
    "\n",
    "    @property\n",
    "    def tokenized_inputs(self) -> str:\n",
    "        \"\"\"\n",
    "        The original tokenized inputs\n",
    "        \"\"\"\n",
    "        return self._sentence.to_tokenized_string()\n",
    "    \n",
    "    @property\n",
    "    def inputs(self) -> str:\n",
    "        \"\"\"\n",
    "        The original input\n",
    "        \"\"\"\n",
    "        return self._sentence.to_original_text()\n",
    "    \n",
    "    def to_dict(self, detail_level:DetailLevel=DetailLevel.Low):\n",
    "        o = OrderedDict()\n",
    "        o.update({'inputs':self.inputs,\n",
    "                  'sentence_embeddings':self.sentence_embeddings,\n",
    "                 'token_embeddings':self.token_embeddings})\n",
    "        if detail_level == 'medium' or detail_level == 'high':\n",
    "            # Return embeddings/word pairs and indicies, and the tokenized input\n",
    "            o.update({\n",
    "                tok.text:{\n",
    "                    'embeddings':tok.get_embedding(),\n",
    "                    'word_idx':tok.idx\n",
    "                } for tok in self._sentence\n",
    "            })\n",
    "            o.update({\n",
    "                'tokenized_inputs':self.tokenized_inputs\n",
    "            })\n",
    "        if detail_level == 'high':\n",
    "            # Return embeddings/word pairs, indicies, and the original Sentence object\n",
    "            o.update({tok.text:{\n",
    "                'embeddings':tok.get_embedding(),\n",
    "                'word_idx':tok.idx\n",
    "                } for tok in self._sentence})\n",
    "            o.update({'sentence':self._sentence})\n",
    "        return o\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = f\"{self.__class__.__name__}:\" + \" {\"\n",
    "        s += f'\\n\\tInputs: {self.inputs}'\n",
    "        if self.token_embeddings is not None: s += f'\\n\\tToken Embeddings Shape: {self.token_embeddings.shape}'\n",
    "        if self.sentence_embeddings is not None: s += f'\\n\\tSentence Embeddings Shape: {self.sentence_embeddings.shape}'\n",
    "        return s + '\\n}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _format_results(embeds:list, detail_level:DetailLevel=None):\n",
    "    \"\"\"\n",
    "    Generates either a list of `EmbeddingResult`s or a single based upon `detail_level` and their length\n",
    "    \"\"\"\n",
    "    res = [EmbeddingResult(embed) for embed in embeds]\n",
    "    return [o.to_dict(detail_level) for o in res] if detail_level is not None else res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyWordEmbeddings:\n",
    "    \"\"\"Word embeddings from the latest language models\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> embeddings = adaptnlp.EasyWordEmbeddings()\n",
    "    >>> embeddings.embed_text(\"text you want embeddings for\", model_name_or_path=\"bert-base-cased\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models: Dict[Embeddings] = defaultdict(bool)\n",
    "\n",
    "    def embed_text(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        model_name_or_path: Union[str, HFModelResult, FlairModelResult] = \"bert-base-cased\",\n",
    "        detail_level:DetailLevel = DetailLevel.Low,\n",
    "        raw:bool = False\n",
    "    ) -> List[EmbeddingResult]:\n",
    "        \"\"\"Produces embeddings for text\n",
    "\n",
    "        **Parameters**:\n",
    "        * `text` - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * `model_name_or_path` - The hosted model name key, model path, or an instance of either `HFModelResult` or `FlairModelResult`\n",
    "        * `detail_level` - A level of detail to return. By default is None, which returns a EmbeddingResult, otherwise will return a dict\n",
    "        * `raw` - A boolean of whether to skip generating an EmbeddingResult or dictionary. Mostly for dev, default is False\n",
    "        \n",
    "        **Return**:\n",
    "        * A list of either EmbeddingResult's or dictionaries with information\n",
    "        \"\"\"\n",
    "        # Convert into sentences\n",
    "        sentences = _make_sentences(text)\n",
    "\n",
    "        # Load correct Embeddings module\n",
    "        if model_name_or_path not in self.models.keys():\n",
    "            self.models[model_name_or_path] = _get_embedding_model(model_name_or_path)\n",
    "        embedding = self.models[model_name_or_path]\n",
    "        embeds = embedding.embed(sentences)\n",
    "        \n",
    "        return _format_results(embeds, detail_level) if not raw else embeds\n",
    "\n",
    "    def embed_all(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        model_names_or_paths:List[str] = [],\n",
    "        detail_level:DetailLevel=DetailLevel.Low,\n",
    "        raw:bool = False,\n",
    "    ) -> List[EmbeddingResult]:\n",
    "        \"\"\"Embeds text with all embedding models loaded\n",
    "\n",
    "        **Parameters**:\n",
    "        * `text` - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * `model_names_or_paths` -  A list of model names\n",
    "        * `detail_level` - A level of detail to return. By default is None, which returns a EmbeddingResult, otherwise will return a dict\n",
    "        * `raw` - A boolean of whether to skip generating an EmbeddingResult or dictionary. Mostly for dev, default is False\n",
    "        \n",
    "        **Return**:\n",
    "        * A list of either EmbeddingResult's or dictionaries with information\n",
    "        \"\"\"\n",
    "        # Convert into sentences\n",
    "        sentences = _make_sentences(text)\n",
    "\n",
    "        if model_names_or_paths:\n",
    "            for embedding_name in model_names_or_paths:\n",
    "                sentences = self.embed_text(\n",
    "                    sentences, model_name_or_path=embedding_name, raw=True\n",
    "                )\n",
    "        else:\n",
    "            for embedding_name in self.models.keys():\n",
    "                sentences = self.embed_text(\n",
    "                    sentences, model_name_or_path=embedding_name, raw=True\n",
    "                )\n",
    "        return _format_results(sentences, detail_level) if not raw else embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import torch\n",
    "embeddings = EasyWordEmbeddings()\n",
    "res = embeddings.embed_text(\"text you want embeddings for\", model_name_or_path=\"bert-base-cased\")\n",
    "test_eq(res[0]['token_embeddings'].shape, torch.Size([5, 768]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "res = embeddings.embed_all(['text you want embeddings for', 'My name is Zach'],\n",
    "                           ['bert-base-cased', 'xlnet-base-cased'])\n",
    "test_eq(res[0]['token_embeddings'].shape, torch.Size([5,1536]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from transformers import pipeline\n",
    "import timeit\n",
    "\n",
    "from fastcore.test import test, test_close\n",
    "import operator as op\n",
    "\n",
    "# Ensure that speeds are comparible with that of raw HuggingFace\n",
    "\n",
    "def performance_test(\n",
    "    func_a:str, # A string of a function to time using AdaptNLP\n",
    "    func_b:str, # A string of a function to time using HuggingFace,\n",
    "    globals_a:dict, # A dict of globals `func_a` should expect\n",
    "    globals_b:dict, # A dict of globals `func_b` should expect\n",
    "    iterations:int=10, # Number of iterations\n",
    "    repeat:int=5, # Number of times to repeat `number` iterations,\n",
    "    thresh:float=0.01 # Threshold for `test_close`\n",
    "):\n",
    "    \"Tests the performance of `func_a` vs `func_b` and checks they either are close, or a is faster\"\n",
    "    time_adapt = timeit.repeat(func_a, globals=globals_a, number=iterations, repeat=repeat)\n",
    "    time_hf = timeit.repeat(func_b, globals=globals_b, number=iterations, repeat=repeat)\n",
    "    avg_a, avg_b = sum(time_adapt)/repeat, sum(time_hf)/repeat\n",
    "    # Test the performance\n",
    "    try:\n",
    "        test(avg_a, avg_b, op.lt)\n",
    "    except:\n",
    "        try:\n",
    "            test_close(avg_a, avg_b, thresh)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"\"\"\n",
    "            HuggingFace was comparitively faster than AdaptNLP. Functions used:\n",
    "            -------------------------------------------------------------------\n",
    "            \n",
    "            AdaptNLP:\n",
    "            \\tFunction:{func_a}\n",
    "            \n",
    "            -------------------------------------------------------------------\n",
    "            \n",
    "            HuggingFace:\n",
    "            \\tFunction:{func_b}\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "text = \"text you want embeddings for\"\n",
    "pipe = pipeline('feature-extraction', 'bert-base-cased')\n",
    "func_a = '_ = _embeddings.embed_text(\"text you want embeddings for\", model_name_or_path=\"bert-base-cased\")'\n",
    "glob_a = {'_embeddings':embeddings}\n",
    "func_b = '_ = _pipe(_text)'\n",
    "glob_b = {'_pipe':pipe, '_text':text}\n",
    "number=10\n",
    "\n",
    "performance_test(func_a, func_b, glob_a, glob_b, iterations=number, thresh=0.1) # Keep thresh within a tenth of a second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyStackedEmbeddings:\n",
    "    \"\"\"Word Embeddings that have been concatenated and \"stacked\" as specified by flair\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> embeddings = adaptnlp.EasyStackedEmbeddings(\"bert-base-cased\", \"gpt2\", \"xlnet-base-cased\")\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * `embeddings` - Non-keyword variable number of strings specifying the embeddings you want to stack\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *embeddings: str):\n",
    "        print(\"May need a couple moments to instantiate...\")\n",
    "        self.embedding_stack = []\n",
    "\n",
    "        # Load correct Embeddings module\n",
    "        for model_name_or_path in embeddings:\n",
    "            self.embedding_stack.append(_get_embedding_model(model_name_or_path))\n",
    "\n",
    "        assert len(self.embedding_stack) != 0\n",
    "        self.stacked_embeddings = StackedEmbeddings(embeddings=self.embedding_stack)\n",
    "\n",
    "    def embed_text(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        detail_level:DetailLevel = DetailLevel.Low,\n",
    "        raw:bool = False\n",
    "    ) -> List[EmbeddingResult]:\n",
    "        \"\"\"Stacked embeddings\n",
    "\n",
    "        **Parameters**:\n",
    "        * `text` - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * `detail_level` - A level of detail to return. By default is None, which returns a EmbeddingResult, otherwise will return a dict\n",
    "        * `raw` - A boolean of whether to skip generating an EmbeddingResult or dictionary. Mostly for dev, default is False\n",
    "\n",
    "        **Return**:\n",
    "        * A list of either EmbeddingResult's or dictionaries with information\n",
    "        \"\"\"\n",
    "        # Convert into sentences\n",
    "        sentences = _make_sentences(text, as_list=True)\n",
    "\n",
    "        # Unlike flair embeddings modules, stacked embeddings do not return a list of sentences\n",
    "        self.stacked_embeddings.embed(sentences)\n",
    "        \n",
    "        return _format_results(sentences, detail_level) if not raw else embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May need a couple moments to instantiate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "embeddings = EasyStackedEmbeddings(\"bert-base-cased\", \"xlnet-base-cased\")\n",
    "sentences = embeddings.embed_text(\"This is Albert.  My last name is Einstein.  I like physics and atoms.\")\n",
    "test_eq(sentences[0]['token_embeddings'].shape, torch.Size([16,1536]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyDocumentEmbeddings:\n",
    "    \"\"\"Document Embeddings generated by pool and rnn methods applied to the word embeddings of text\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> embeddings = adaptnlp.EasyDocumentEmbeddings(\"bert-base-cased\", \"xlnet-base-cased\", methods[\"rnn\"])\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * `embeddings` - Non-keyword variable number of strings referring to model names or paths\n",
    "    * `methods` - A list of strings to specify which document embeddings to use i.e. [\"rnn\", \"pool\"] (avoids unncessary loading of models if only using one)\n",
    "    * `configs` - A dictionary of configurations for flair's rnn and pool document embeddings\n",
    "    ```python\n",
    "    >>> example_configs = {\"pool_configs\": {\"fine_tune_mode\": \"linear\", \"pooling\": \"mean\", },\n",
    "    ...                   \"rnn_configs\": {\"hidden_size\": 512,\n",
    "    ...                                   \"rnn_layers\": 1,\n",
    "    ...                                   \"reproject_words\": True,\n",
    "    ...                                   \"reproject_words_dimension\": 256,\n",
    "    ...                                   \"bidirectional\": False,\n",
    "    ...                                   \"dropout\": 0.5,\n",
    "    ...                                   \"word_dropout\": 0.0,\n",
    "    ...                                   \"locked_dropout\": 0.0,\n",
    "    ...                                   \"rnn_type\": \"GRU\",\n",
    "    ...                                   \"fine_tune\": True, },\n",
    "    ...                  }\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    __allowed_methods = [\"rnn\", \"pool\"]\n",
    "    __allowed_configs = (\"pool_configs\", \"rnn_configs\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *embeddings: str,\n",
    "        methods: List[str] = [\"rnn\", \"pool\"],\n",
    "        configs: Dict = {\n",
    "            \"pool_configs\": {\"fine_tune_mode\": \"linear\", \"pooling\": \"mean\"},\n",
    "            \"rnn_configs\": {\n",
    "                \"hidden_size\": 512,\n",
    "                \"rnn_layers\": 1,\n",
    "                \"reproject_words\": True,\n",
    "                \"reproject_words_dimension\": 256,\n",
    "                \"bidirectional\": False,\n",
    "                \"dropout\": 0.5,\n",
    "                \"word_dropout\": 0.0,\n",
    "                \"locked_dropout\": 0.0,\n",
    "                \"rnn_type\": \"GRU\",\n",
    "                \"fine_tune\": True,\n",
    "            },\n",
    "        },\n",
    "    ):\n",
    "        print(\"May need a couple moments to instantiate...\")\n",
    "        self.embedding_stack = []\n",
    "\n",
    "        # Check methods\n",
    "        for m in methods:\n",
    "            assert m in self.__class__.__allowed_methods\n",
    "\n",
    "        # Set configs for pooling and rnn parameters\n",
    "        for k, v in configs.items():\n",
    "            assert k in self.__class__.__allowed_configs\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        # Load correct Embeddings module\n",
    "        for model_name_or_path in embeddings:\n",
    "            self.embedding_stack.append(_get_embedding_model(model_name_or_path))\n",
    "\n",
    "        assert len(self.embedding_stack) != 0\n",
    "        if \"pool\" in methods:\n",
    "            self.pool_embeddings = DocumentPoolEmbeddings(\n",
    "                self.embedding_stack, **self.pool_configs\n",
    "            )\n",
    "            print(\"Pooled embedding loaded\")\n",
    "        if \"rnn\" in methods:\n",
    "            self.rnn_embeddings = DocumentRNNEmbeddings(\n",
    "                self.embedding_stack, **self.rnn_configs\n",
    "            )\n",
    "            print(\"RNN embeddings loaded\")\n",
    "\n",
    "    def embed_pool(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        detail_level:DetailLevel = DetailLevel.Low,\n",
    "    ) -> List[EmbeddingResult]:\n",
    "        \"\"\"Generate stacked embeddings with `DocumentPoolEmbeddings`\n",
    "\n",
    "        **Parameters**:\n",
    "        * `text` - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * `detail_level` - A level of detail to return. By default is None, which returns a EmbeddingResult, otherwise will return a dict\n",
    "\n",
    "        **Return**:\n",
    "        * A list of either EmbeddingResult's or dictionaries with information\n",
    "        \"\"\"\n",
    "        sentences = _make_sentences(text, as_list=True)\n",
    "        self.pool_embeddings.embed(sentences)\n",
    "        return _format_results(sentences, detail_level)\n",
    "\n",
    "    def embed_rnn(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        detail_level:DetailLevel = DetailLevel.Low,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Generate stacked embeddings with `DocumentRNNEmbeddings`\n",
    "\n",
    "        **Parameters**:\n",
    "        * `text` - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * `detail_level` - A level of detail to return. By default is None, which returns a EmbeddingResult, otherwise will return a dict\n",
    "\n",
    "        **Return**:\n",
    "        * A list of either EmbeddingResult's or dictionaries with information\n",
    "        \"\"\"\n",
    "        sentences = _make_sentences(text, as_list=True)\n",
    "        self.rnn_embeddings.embed(sentences)\n",
    "        return _format_results(sentences, detail_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May need a couple moments to instantiate...\n",
      "Pooled embedding loaded\n",
      "RNN embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "embeddings = EasyDocumentEmbeddings(\"bert-base-cased\", \"xlnet-base-cased\")\n",
    "res = embeddings.embed_pool(\"This is Albert.  My last name is Einstein.  I like physics and atoms.\")\n",
    "test_eq(res[0]['sentence_embeddings'].shape, torch.Size([1536]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "res = embeddings.embed_rnn(\"This is Albert.  My last name is Einstein.  I like physics and atoms.\")\n",
    "test_eq(res[0]['sentence_embeddings'].shape, torch.Size([512]))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
