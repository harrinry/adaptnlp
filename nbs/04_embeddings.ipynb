{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "> AdaptNLP Embeddings Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import test_eq\n",
    "from fastcore.xtras import is_listy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging, torch\n",
    "from typing import List, Dict, Union\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "from fastcore.basics import mk_class\n",
    "from fastcore.xtras import dict2obj\n",
    "from fastcore.dispatch import typedispatch\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import (\n",
    "    Embeddings,\n",
    "    WordEmbeddings,\n",
    "    StackedEmbeddings,\n",
    "    FlairEmbeddings,\n",
    "    DocumentPoolEmbeddings,\n",
    "    DocumentRNNEmbeddings,\n",
    "    TransformerWordEmbeddings,\n",
    ")\n",
    "\n",
    "from adaptnlp.model_hub import FlairModelHub, HFModelHub, FlairModelResult, HFModelResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_flair_hub = FlairModelHub()\n",
    "_hf_hub = HFModelHub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@typedispatch\n",
    "def _make_sentences(text:str, as_list=False) -> Union[List[Sentence], Sentence]:\n",
    "    return [Sentence(text)] if as_list else Sentence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_sentences = 'a,b,c'.split(',')\n",
    "out = _make_sentences(test_sentences)\n",
    "tst_out = [Sentence('a'), Sentence('b'), Sentence('c')]\n",
    "for o,t in zip(out, tst_out):\n",
    "    test_eq(o[0].text, t[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@typedispatch\n",
    "def _make_sentences(text:list, as_list=False) -> Union[List[Sentence], Sentence]:\n",
    "    if all(isinstance(t,str) for t in text):\n",
    "        return [Sentence(t) for t in text]\n",
    "    elif all(isinstance(t, Sentence) for t in text):\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_sentence = 'My name is Zach'\n",
    "out = _make_sentences(test_sentence, as_list=True)\n",
    "tst_out = [Sentence(test_sentence)]\n",
    "for o,t in zip(out, tst_out):\n",
    "    test_eq(o[0].text, t[0].text)\n",
    "test_eq(is_listy(out), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@typedispatch\n",
    "def _make_sentences(text:Sentence, as_list=False) -> Union[List[Sentence], Sentence]:\n",
    "    return [text] if as_list else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_sentence = Sentence('Me')\n",
    "out = _make_sentences(test_sentence)\n",
    "test_eq(test_sentence[0].text, out[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _get_embedding_model(model_name_or_path:Union[str, HFModelResult, FlairModelResult]) -> Union[FlairEmbeddings, WordEmbeddings, TransformerWordEmbeddings, Sentence]:\n",
    "    \"Load the proper `Embeddings` model from `model_name_or_path`\"\n",
    "    if isinstance(model_name_or_path, FlairModelResult): \n",
    "        nm = model_name_or_path.name\n",
    "        try:\n",
    "            return WordEmbeddings(nm.strip('flairNLP/'))\n",
    "        except:\n",
    "            return FlairEmbeddings(nm.strip('flairNLP/'))\n",
    "    \n",
    "    elif isinstance(model_name_or_path, HFModelResult): return TransformerWordEmbeddings(model_name_or_path.name)\n",
    "    else:\n",
    "        res = _flair_hub.search_model_by_name(model_name_or_path, user_uploaded=True)\n",
    "        if len(res) < 1:\n",
    "            # No models found\n",
    "            res = _hf_hub.search_model_by_name(model_name_or_path, user_uploaded=True)\n",
    "            if len(res) < 1:\n",
    "                raise ValueError(f'Embeddings not found for the model key: {model_name_or_path}, check documentation or custom model path to verify specified model')\n",
    "            else:\n",
    "                return TransformerWordEmbeddings(res[0].name) # Returning the first should always be the non-fast option\n",
    "        else:\n",
    "            nm = res[0].name\n",
    "            try:\n",
    "                return WordEmbeddings(nm.strip('flairNLP/'))\n",
    "            except:\n",
    "                return FlairEmbeddings(nm.strip('flairNLP/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mk_class('DetailLevel', **{o:o.lower() for o in 'High,Medium,Low'.split(',')},\n",
    "         doc=\"All possible naming conventions for DetailLevel with typo-proofing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EmbeddingResult:\n",
    "    \"\"\"\n",
    "    A result class designed for Embedding models\n",
    "    \"\"\"\n",
    "    def __init__(self, sentence:Sentence):\n",
    "        self._sentence = sentence\n",
    "    \n",
    "    @property\n",
    "    def sentence_embeddings(self) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        All embeddings in `sentence` (if available)\n",
    "        \"\"\"\n",
    "        return self._sentence.get_embedding()\n",
    "    \n",
    "    @property\n",
    "    def token_embeddings(self) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        All embeddings from the individual tokens in `sentence` with original order in shape (n, embed_dim)\n",
    "        \"\"\"\n",
    "        return torch.stack([tok.get_embedding() for tok in self._sentence], dim=0)\n",
    "\n",
    "    @property\n",
    "    def tokenized_inputs(self) -> str:\n",
    "        \"\"\"\n",
    "        The original tokenized inputs\n",
    "        \"\"\"\n",
    "        return self._sentence.to_tokenized_string()\n",
    "    \n",
    "    @property\n",
    "    def inputs(self) -> str:\n",
    "        \"\"\"\n",
    "        The original input\n",
    "        \"\"\"\n",
    "        return self._sentence.to_original_text()\n",
    "    \n",
    "    def to_dict(self, detail_level:DetailLevel=DetailLevel.Low):\n",
    "        o = OrderedDict()\n",
    "        o.update({'inputs':self.inputs,\n",
    "                  'sentence_embeddings':self.sentence_embeddings,\n",
    "                 'token_embeddings':self.token_embeddings})\n",
    "        if detail_level == 'medium' or detail_level == 'high':\n",
    "            # Return embeddings/word pairs and indicies, and the tokenized input\n",
    "            o.update({\n",
    "                tok.text:{\n",
    "                    'embeddings':tok.get_embedding(),\n",
    "                    'word_idx':tok.idx\n",
    "                } for tok in self._sentence\n",
    "            })\n",
    "            o.update({\n",
    "                'tokenized_inputs':self.tokenized_inputs\n",
    "            })\n",
    "        if detail_level == 'high':\n",
    "            # Return embeddings/word pairs, indicies, and the original Sentence object\n",
    "            o.update({tok.text:{\n",
    "                'embeddings':tok.get_embedding(),\n",
    "                'word_idx':tok.idx\n",
    "                } for tok in self._sentence})\n",
    "            o.update({'sentence':self._sentence})\n",
    "        return o\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = f\"{self.__class__.__name__}:\" + \" {\"\n",
    "        s += f'\\n\\tInputs: {self.inputs}'\n",
    "        if self.token_embeddings is not None: s += f'\\n\\tToken Embeddings Shape: {self.token_embeddings.shape}'\n",
    "        if self.sentence_embeddings is not None: s += f'\\n\\tSentence Embeddings Shape: {self.sentence_embeddings.shape}'\n",
    "        return s + '\\n}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _format_results(embeds:list, detail_level:DetailLevel=None):\n",
    "    \"\"\"\n",
    "    Generates either a list of `EmbeddingResult`s or a single based upon `detail_level` and their length\n",
    "    \"\"\"\n",
    "    res = [EmbeddingResult(embed) for embed in embeds]\n",
    "    return [o.to_dict(detail_level) for o in res] if detail_level is not None else res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyWordEmbeddings:\n",
    "    \"\"\"Word embeddings from the latest language models\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> embeddings = adaptnlp.EasyWordEmbeddings()\n",
    "    >>> embeddings.embed_text(\"text you want embeddings for\", model_name_or_path=\"bert-base-cased\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models: Dict[Embeddings] = defaultdict(bool)\n",
    "\n",
    "    def embed_text(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        model_name_or_path: Union[str, HFModelResult, FlairModelResult] = \"bert-base-cased\",\n",
    "        detail_level:DetailLevel = DetailLevel.Low,\n",
    "        raw:bool = False\n",
    "    ) -> List[EmbeddingResult]:\n",
    "        \"\"\"Produces embeddings for text\n",
    "\n",
    "        **Parameters**:\n",
    "        * `text` - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * `model_name_or_path` - The hosted model name key, model path, or an instance of either `HFModelResult` or `FlairModelResult`\n",
    "        * `detail_level` - A level of detail to return. By default is None, which returns a EmbeddingResult, otherwise will return a dict\n",
    "        * `raw` - A boolean of whether to skip generating an EmbeddingResult or dictionary. Mostly for dev, default is False\n",
    "        \n",
    "        **Return**:\n",
    "        * A list of either EmbeddingResult's or dictionaries with information\n",
    "        \"\"\"\n",
    "        # Convert into sentences\n",
    "        sentences = _make_sentences(text)\n",
    "\n",
    "        # Load correct Embeddings module\n",
    "        self.models[model_name_or_path] = _get_embedding_model(model_name_or_path)\n",
    "        embedding = self.models[model_name_or_path]\n",
    "        embeds = embedding.embed(sentences)\n",
    "        \n",
    "        return _format_results(embeds, detail_level) if not raw else embeds\n",
    "\n",
    "    def embed_all(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        model_names_or_paths:List[str] = [],\n",
    "        detail_level:DetailLevel=DetailLevel.Low,\n",
    "        raw:bool = False,\n",
    "    ) -> List[EmbeddingResult]:\n",
    "        \"\"\"Embeds text with all embedding models loaded\n",
    "\n",
    "        **Parameters**:\n",
    "        * `text` - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * `model_names_or_paths` -  A list of model names\n",
    "        * `detail_level` - A level of detail to return. By default is None, which returns a EmbeddingResult, otherwise will return a dict\n",
    "        * `raw` - A boolean of whether to skip generating an EmbeddingResult or dictionary. Mostly for dev, default is False\n",
    "        \n",
    "        **Return**:\n",
    "        * A list of either EmbeddingResult's or dictionaries with information\n",
    "        \"\"\"\n",
    "        # Convert into sentences\n",
    "        sentences = _make_sentences(text)\n",
    "\n",
    "        if model_names_or_paths:\n",
    "            for embedding_name in model_names_or_paths:\n",
    "                sentences = self.embed_text(\n",
    "                    sentences, model_name_or_path=embedding_name, raw=True\n",
    "                )\n",
    "        else:\n",
    "            for embedding_name in self.models.keys():\n",
    "                sentences = self.embed_text(\n",
    "                    sentences, model_name_or_path=embedding_name, raw=True\n",
    "                )\n",
    "        return _format_results(sentences, detail_level) if not raw else embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import torch\n",
    "embeddings = EasyWordEmbeddings()\n",
    "res = embeddings.embed_text(\"text you want embeddings for\", model_name_or_path=\"bert-base-cased\")\n",
    "test_eq(res[0]['token_embeddings'].shape, torch.Size([5, 768]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('inputs', 'text you want embeddings for'),\n",
       "              ('sentence_embeddings', tensor([])),\n",
       "              ('token_embeddings',\n",
       "               tensor([[-7.9327e-01, -4.1105e-01, -3.8680e-01,  ..., -3.2171e-01,\n",
       "                         3.0939e-01,  4.4718e-01],\n",
       "                       [ 8.5489e-01, -3.2192e-01,  3.8047e-01,  ...,  9.5253e-01,\n",
       "                        -1.2863e-01,  5.8812e-01],\n",
       "                       [-2.1384e-02,  4.0351e-01, -1.6811e-01,  ...,  3.7693e-01,\n",
       "                        -1.0005e+00,  1.1159e-01],\n",
       "                       [ 4.2309e-01,  1.2025e+00, -7.9792e-01,  ...,  1.2090e-01,\n",
       "                         1.4435e+00,  1.2092e-03],\n",
       "                       [-1.8945e-01,  7.2156e-02, -6.4450e-01,  ..., -4.3372e-01,\n",
       "                        -1.9544e-01, -2.6331e-01]], device='cuda:0'))])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "res = embeddings.embed_all(['text you want embeddings for', 'My name is Zach'],\n",
    "                           ['bert-base-cased', 'xlnet-base-cased'])\n",
    "test_eq(res[0]['token_embeddings'].shape, torch.Size([5,1536]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('inputs', 'text you want embeddings for'),\n",
       "              ('sentence_embeddings', tensor([])),\n",
       "              ('token_embeddings',\n",
       "               tensor([[-0.7933, -0.4110, -0.3868,  ..., -0.3953, -0.7718,  0.1248],\n",
       "                       [ 0.8549, -0.3219,  0.3805,  ..., -0.7219,  0.8176,  0.1700],\n",
       "                       [-0.0214,  0.4035, -0.1681,  ..., -0.7486,  0.8592,  0.6758],\n",
       "                       [ 0.4231,  1.2025, -0.7979,  ..., -0.6044,  0.1577,  0.8191],\n",
       "                       [-0.1894,  0.0722, -0.6445,  ..., -1.1703, -0.3907,  0.2863]],\n",
       "                      device='cuda:0'))]),\n",
       " OrderedDict([('inputs', 'My name is Zach'),\n",
       "              ('sentence_embeddings', tensor([])),\n",
       "              ('token_embeddings',\n",
       "               tensor([[-0.6113,  0.4488,  0.1556,  ..., -0.6987, -0.6393, -0.4213],\n",
       "                       [-0.4969,  0.8494,  0.0863,  ..., -0.2641, -0.5141,  0.4813],\n",
       "                       [-0.5020, -0.2093,  0.2883,  ..., -0.8309,  0.6171,  0.1118],\n",
       "                       [-0.2088, -1.2357,  0.7745,  ..., -0.5751,  0.9572,  0.2226]],\n",
       "                      device='cuda:0'))])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyStackedEmbeddings:\n",
    "    \"\"\"Word Embeddings that have been concatenated and \"stacked\" as specified by flair\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> embeddings = adaptnlp.EasyStackedEmbeddings(\"bert-base-cased\", \"gpt2\", \"xlnet-base-cased\")\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * `embeddings` - Non-keyword variable number of strings specifying the embeddings you want to stack\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *embeddings: str):\n",
    "        print(\"May need a couple moments to instantiate...\")\n",
    "        self.embedding_stack = []\n",
    "\n",
    "        # Load correct Embeddings module\n",
    "        for model_name_or_path in embeddings:\n",
    "            self.embedding_stack.append(_get_embedding_model(model_name_or_path))\n",
    "\n",
    "        assert len(self.embedding_stack) != 0\n",
    "        self.stacked_embeddings = StackedEmbeddings(embeddings=self.embedding_stack)\n",
    "\n",
    "    def embed_text(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        detail_level:DetailLevel = DetailLevel.Low,\n",
    "        raw:bool = False\n",
    "    ) -> List[EmbeddingResult]:\n",
    "        \"\"\"Stacked embeddings\n",
    "\n",
    "        **Parameters**:\n",
    "        * `text` - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * `detail_level` - A level of detail to return. By default is None, which returns a EmbeddingResult, otherwise will return a dict\n",
    "        * `raw` - A boolean of whether to skip generating an EmbeddingResult or dictionary. Mostly for dev, default is False\n",
    "\n",
    "        **Return**:\n",
    "        * A list of either EmbeddingResult's or dictionaries with information\n",
    "        \"\"\"\n",
    "        # Convert into sentences\n",
    "        sentences = _make_sentences(text, as_list=True)\n",
    "\n",
    "        # Unlike flair embeddings modules, stacked embeddings do not return a list of sentences\n",
    "        self.stacked_embeddings.embed(sentences)\n",
    "        \n",
    "        return _format_results(sentences, detail_level) if not raw else embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May need a couple moments to instantiate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "embeddings = EasyStackedEmbeddings(\"bert-base-cased\", \"xlnet-base-cased\")\n",
    "sentences = embeddings.embed_text(\"This is Albert.  My last name is Einstein.  I like physics and atoms.\")\n",
    "test_eq(sentences[0]['token_embeddings'].shape, torch.Size([16,1536]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('inputs',\n",
       "               'This is Albert.  My last name is Einstein.  I like physics and atoms.'),\n",
       "              ('sentence_embeddings', tensor([])),\n",
       "              ('token_embeddings',\n",
       "               tensor([[-0.6795, -0.2041,  1.0153,  ...,  0.4284, -1.7024, -0.2833],\n",
       "                       [-0.1609, -0.2013,  0.9313,  ...,  0.1025, -0.6252, -0.4562],\n",
       "                       [-0.0846, -0.2399,  0.2524,  ...,  0.1644,  0.0220,  0.1787],\n",
       "                       ...,\n",
       "                       [ 0.2307,  0.0850, -0.3529,  ...,  0.3358,  0.3527,  0.3798],\n",
       "                       [-0.3223,  0.3806, -0.7739,  ..., -0.0810,  0.4421,  0.2971],\n",
       "                       [ 0.2760, -0.0849, -0.0120,  ...,  0.2688,  0.4413,  0.1424]],\n",
       "                      device='cuda:0'))])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyDocumentEmbeddings:\n",
    "    \"\"\"Document Embeddings generated by pool and rnn methods applied to the word embeddings of text\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> embeddings = adaptnlp.EasyDocumentEmbeddings(\"bert-base-cased\", \"xlnet-base-cased\", methods[\"rnn\"])\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * `embeddings` - Non-keyword variable number of strings referring to model names or paths\n",
    "    * `methods` - A list of strings to specify which document embeddings to use i.e. [\"rnn\", \"pool\"] (avoids unncessary loading of models if only using one)\n",
    "    * `configs` - A dictionary of configurations for flair's rnn and pool document embeddings\n",
    "    ```python\n",
    "    >>> example_configs = {\"pool_configs\": {\"fine_tune_mode\": \"linear\", \"pooling\": \"mean\", },\n",
    "    ...                   \"rnn_configs\": {\"hidden_size\": 512,\n",
    "    ...                                   \"rnn_layers\": 1,\n",
    "    ...                                   \"reproject_words\": True,\n",
    "    ...                                   \"reproject_words_dimension\": 256,\n",
    "    ...                                   \"bidirectional\": False,\n",
    "    ...                                   \"dropout\": 0.5,\n",
    "    ...                                   \"word_dropout\": 0.0,\n",
    "    ...                                   \"locked_dropout\": 0.0,\n",
    "    ...                                   \"rnn_type\": \"GRU\",\n",
    "    ...                                   \"fine_tune\": True, },\n",
    "    ...                  }\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    __allowed_methods = [\"rnn\", \"pool\"]\n",
    "    __allowed_configs = (\"pool_configs\", \"rnn_configs\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *embeddings: str,\n",
    "        methods: List[str] = [\"rnn\", \"pool\"],\n",
    "        configs: Dict = {\n",
    "            \"pool_configs\": {\"fine_tune_mode\": \"linear\", \"pooling\": \"mean\"},\n",
    "            \"rnn_configs\": {\n",
    "                \"hidden_size\": 512,\n",
    "                \"rnn_layers\": 1,\n",
    "                \"reproject_words\": True,\n",
    "                \"reproject_words_dimension\": 256,\n",
    "                \"bidirectional\": False,\n",
    "                \"dropout\": 0.5,\n",
    "                \"word_dropout\": 0.0,\n",
    "                \"locked_dropout\": 0.0,\n",
    "                \"rnn_type\": \"GRU\",\n",
    "                \"fine_tune\": True,\n",
    "            },\n",
    "        },\n",
    "    ):\n",
    "        print(\"May need a couple moments to instantiate...\")\n",
    "        self.embedding_stack = []\n",
    "\n",
    "        # Check methods\n",
    "        for m in methods:\n",
    "            assert m in self.__class__.__allowed_methods\n",
    "\n",
    "        # Set configs for pooling and rnn parameters\n",
    "        for k, v in configs.items():\n",
    "            assert k in self.__class__.__allowed_configs\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        # Load correct Embeddings module\n",
    "        for model_name_or_path in embeddings:\n",
    "            self.embedding_stack.append(_get_embedding_model(model_name_or_path))\n",
    "\n",
    "        assert len(self.embedding_stack) != 0\n",
    "        if \"pool\" in methods:\n",
    "            self.pool_embeddings = DocumentPoolEmbeddings(\n",
    "                self.embedding_stack, **self.pool_configs\n",
    "            )\n",
    "            print(\"Pooled embedding loaded\")\n",
    "        if \"rnn\" in methods:\n",
    "            self.rnn_embeddings = DocumentRNNEmbeddings(\n",
    "                self.embedding_stack, **self.rnn_configs\n",
    "            )\n",
    "            print(\"RNN embeddings loaded\")\n",
    "\n",
    "    def embed_pool(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        detail_level:DetailLevel = DetailLevel.Low,\n",
    "    ) -> List[EmbeddingResult]:\n",
    "        \"\"\"Generate stacked embeddings with `DocumentPoolEmbeddings`\n",
    "\n",
    "        **Parameters**:\n",
    "        * `text` - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * `detail_level` - A level of detail to return. By default is None, which returns a EmbeddingResult, otherwise will return a dict\n",
    "\n",
    "        **Return**:\n",
    "        * A list of either EmbeddingResult's or dictionaries with information\n",
    "        \"\"\"\n",
    "        sentences = _make_sentences(text, as_list=True)\n",
    "        self.pool_embeddings.embed(sentences)\n",
    "        return _format_results(sentences, detail_level)\n",
    "\n",
    "    def embed_rnn(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        detail_level:DetailLevel = DetailLevel.Low,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Generate stacked embeddings with `DocumentRNNEmbeddings`\n",
    "\n",
    "        **Parameters**:\n",
    "        * `text` - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * `detail_level` - A level of detail to return. By default is None, which returns a EmbeddingResult, otherwise will return a dict\n",
    "\n",
    "        **Return**:\n",
    "        * A list of either EmbeddingResult's or dictionaries with information\n",
    "        \"\"\"\n",
    "        sentences = _make_sentences(text, as_list=True)\n",
    "        self.rnn_embeddings.embed(sentences)\n",
    "        return _format_results(sentences, detail_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May need a couple moments to instantiate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled embedding loaded\n",
      "RNN embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "embeddings = EasyDocumentEmbeddings(\"bert-base-cased\", \"xlnet-base-cased\")\n",
    "res = embeddings.embed_pool(\"This is Albert.  My last name is Einstein.  I like physics and atoms.\")\n",
    "test_eq(res[0]['sentence_embeddings'].shape, torch.Size([1536]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('inputs',\n",
       "               'This is Albert.  My last name is Einstein.  I like physics and atoms.'),\n",
       "              ('sentence_embeddings',\n",
       "               tensor([-0.2397,  0.2154,  0.1053,  ..., -0.0263, -0.2341,  0.0857],\n",
       "                      device='cuda:0', grad_fn=<CatBackward>)),\n",
       "              ('token_embeddings',\n",
       "               tensor([[-0.6795, -0.2041,  1.0153,  ...,  0.4284, -1.7024, -0.2833],\n",
       "                       [-0.1609, -0.2013,  0.9313,  ...,  0.1025, -0.6252, -0.4562],\n",
       "                       [-0.0846, -0.2399,  0.2524,  ...,  0.1644,  0.0220,  0.1787],\n",
       "                       ...,\n",
       "                       [ 0.2307,  0.0850, -0.3529,  ...,  0.3358,  0.3527,  0.3798],\n",
       "                       [-0.3223,  0.3806, -0.7739,  ..., -0.0810,  0.4421,  0.2971],\n",
       "                       [ 0.2760, -0.0849, -0.0120,  ...,  0.2688,  0.4413,  0.1424]],\n",
       "                      device='cuda:0'))])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "res = embeddings.embed_rnn(\"This is Albert.  My last name is Einstein.  I like physics and atoms.\")\n",
    "test_eq(res[0]['sentence_embeddings'].shape, torch.Size([512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('inputs',\n",
       "               'This is Albert.  My last name is Einstein.  I like physics and atoms.'),\n",
       "              ('sentence_embeddings',\n",
       "               tensor([-0.1714, -0.7549, -0.1850, -0.2769,  0.2418, -0.4486,  0.6229, -0.6479,\n",
       "                        0.6507, -0.1576, -0.2320,  0.2510,  0.7094, -0.5881, -0.1548, -0.1689,\n",
       "                       -0.4031, -0.5152,  0.8924,  0.2729, -0.5003, -0.5212, -0.1039,  0.4079,\n",
       "                        0.3397, -0.3155,  0.0047,  0.1113, -0.0592, -0.3699, -0.6757, -0.2822,\n",
       "                        0.3281, -0.0072, -0.8087, -0.1867, -0.3111,  0.1156,  0.1645, -0.4168,\n",
       "                       -0.3203,  0.1876, -0.0131, -0.1157,  0.0309, -0.5008,  0.4087,  0.2558,\n",
       "                        0.8947,  0.2512, -0.7544, -0.1717,  0.2814, -0.4354, -0.3502, -0.2949,\n",
       "                        0.8499,  0.2330, -0.6120,  0.0607,  0.2288,  0.2005, -0.5318, -0.0992,\n",
       "                        0.6384, -0.1772,  0.5373,  0.4454, -0.1449,  0.1644, -0.7993, -0.7178,\n",
       "                        0.6041,  0.1051, -0.3530,  0.0600,  0.6983,  0.3898, -0.7058, -0.1074,\n",
       "                        0.1072, -0.1729,  0.4731,  0.4126,  0.3192,  0.4341, -0.5519,  0.3149,\n",
       "                       -0.5854,  0.4122, -0.4922, -0.1984, -0.8130, -0.2044,  0.3790,  0.1534,\n",
       "                       -0.5642,  0.1617,  0.3911,  0.3937,  0.3554, -0.0501,  0.3951, -0.2070,\n",
       "                        0.2029, -0.5070,  0.1580,  0.4041,  0.1809,  0.3278, -0.0282,  0.5112,\n",
       "                        0.1873,  0.1470, -0.2050, -0.3450,  0.2176,  0.5238, -0.1398,  0.0748,\n",
       "                        0.0555, -0.3513,  0.4302,  0.3485,  0.2986,  0.0295, -0.7185, -0.2318,\n",
       "                        0.4050,  0.7040, -0.3661,  0.1129, -0.0452, -0.0871, -0.0173, -0.2070,\n",
       "                        0.3742,  0.5105,  0.0638,  0.9088, -0.1654, -0.6288, -0.0518,  0.0624,\n",
       "                        0.1147, -0.1560, -0.5614, -0.0789,  0.2273,  0.6165, -0.0968,  0.3433,\n",
       "                       -0.6945,  0.2836, -0.6074, -0.6174,  0.4441,  0.2822, -0.5132,  0.1960,\n",
       "                        0.8193, -0.5274, -0.0151,  0.3054, -0.3283, -0.0140,  0.4012,  0.7336,\n",
       "                        0.2314, -0.4043,  0.1262, -0.4230,  0.3504,  0.1787,  0.0308,  0.1680,\n",
       "                       -0.2798,  0.5015,  0.0578, -0.3591,  0.0228, -0.4263,  0.3959,  0.5816,\n",
       "                       -0.0945,  0.2807,  0.0498,  0.1066, -0.2157, -0.0175,  0.1531,  0.0533,\n",
       "                        0.4225, -0.1881,  0.3802, -0.1593,  0.5015,  0.5087,  0.1880, -0.5629,\n",
       "                       -0.2870, -0.1561, -0.0446,  0.2089,  0.1347,  0.2792,  0.2232,  0.1332,\n",
       "                       -0.1648,  0.5992, -0.7407,  0.4005,  0.0102, -0.0410,  0.7203,  0.2683,\n",
       "                        0.1482, -0.7277, -0.5486, -0.2976,  0.3399,  0.1558, -0.0957, -0.0792,\n",
       "                       -0.0885,  0.7378,  0.5080, -0.1895, -0.5361, -0.6971,  0.1698,  0.0902,\n",
       "                       -0.2156,  0.2934,  0.1430,  0.2716, -0.0982, -0.0721,  0.0469, -0.0386,\n",
       "                       -0.6856, -0.0863,  0.0919, -0.1980,  0.0782, -0.2220, -0.6258, -0.0991,\n",
       "                        0.3318,  0.2110, -0.2125, -0.5973, -0.0716, -0.5836, -0.2391, -0.2966,\n",
       "                        0.2113, -0.0341, -0.2470, -0.3551, -0.3340, -0.2707,  0.2076, -0.1314,\n",
       "                       -0.0731, -0.2783, -0.0041,  0.4093, -0.4092, -0.7531,  0.3333,  0.3784,\n",
       "                        0.0099, -0.2191, -0.3273,  0.4712, -0.3211,  0.1538, -0.5085,  0.8137,\n",
       "                        0.1070, -0.5799,  0.6254, -0.5964, -0.1787,  0.0940, -0.1908, -0.0230,\n",
       "                        0.4600, -0.1063,  0.0031, -0.0870, -0.5916,  0.1897, -0.3118, -0.4329,\n",
       "                        0.1504,  0.1913,  0.6553,  0.4458, -0.1724,  0.5485, -0.4102, -0.1271,\n",
       "                        0.4049, -0.2904,  0.3723,  0.6262, -0.0402,  0.6938, -0.0082, -0.7513,\n",
       "                        0.3528,  0.4321, -0.0391, -0.0588, -0.5747,  0.5995, -0.7256,  0.2469,\n",
       "                        0.5055, -0.1306, -0.1619,  0.0208,  0.4593,  0.0632, -0.0155, -0.2127,\n",
       "                        0.3016, -0.6980, -0.4049, -0.2195,  0.3361, -0.5575,  0.1388,  0.2099,\n",
       "                       -0.2036, -0.0956, -0.2696,  0.0621, -0.3964, -0.0663,  0.3704, -0.1829,\n",
       "                        0.7257,  0.2545,  0.3367,  0.3362, -0.0986, -0.6574, -0.3436,  0.2972,\n",
       "                        0.0934, -0.0036,  0.6085, -0.5116, -0.3038,  0.5170,  0.1231, -0.6349,\n",
       "                       -0.2929, -0.0292, -0.2813,  0.0824,  0.3512,  0.4401,  0.4912,  0.0241,\n",
       "                        0.0757, -0.0721, -0.3954,  0.3203,  0.2277,  0.1026, -0.5041, -0.3839,\n",
       "                       -0.1426,  0.5282, -0.3563, -0.0855,  0.0923,  0.0531, -0.2887, -0.2325,\n",
       "                        0.5845,  0.0614, -0.5347, -0.1418,  0.1127,  0.3190, -0.1517, -0.4505,\n",
       "                        0.0241, -0.2419, -0.0094,  0.2916,  0.0681, -0.0490, -0.0785, -0.3111,\n",
       "                        0.0480,  0.1428,  0.5263,  0.4840,  0.2209, -0.1726,  0.1793, -0.4403,\n",
       "                       -0.0050, -0.4187, -0.3330, -0.4068, -0.4821,  0.0290, -0.3282, -0.3325,\n",
       "                        0.0574, -0.2971,  0.6940,  0.1049,  0.0292,  0.0098,  0.0725, -0.7187,\n",
       "                        0.3391,  0.7483,  0.0771,  0.2745, -0.0118, -0.3631, -0.1712, -0.1032,\n",
       "                       -0.5591, -0.1944,  0.3224,  0.5489, -0.0612, -0.5795,  0.2006, -0.4858,\n",
       "                        0.1088,  0.0709, -0.0821, -0.3719,  0.1618, -0.6309,  0.3418, -0.5085,\n",
       "                       -0.4552,  0.5260, -0.1152, -0.2662, -0.3608,  0.0325,  0.2410, -0.2030,\n",
       "                        0.5562,  0.0401, -0.5553,  0.5384, -0.6788,  0.5941,  0.5444,  0.5446,\n",
       "                        0.1950, -0.4364, -0.4923, -0.5361, -0.7903, -0.1991, -0.5566, -0.3291,\n",
       "                        0.3522,  0.5642,  0.0098, -0.0536,  0.1080,  0.1306, -0.6130,  0.7258,\n",
       "                       -0.0573,  0.1984,  0.5776, -0.6395, -0.3177, -0.4365,  0.0677, -0.0781,\n",
       "                        0.3397, -0.7239,  0.2117, -0.0828,  0.4052,  0.3707,  0.1220, -0.1146,\n",
       "                        0.3401, -0.1341, -0.6201,  0.2972, -0.4001, -0.2056, -0.5747, -0.4357,\n",
       "                        0.5377,  0.2256, -0.3589,  0.0641,  0.3235,  0.7054, -0.5988,  0.3404],\n",
       "                      device='cuda:0', grad_fn=<CatBackward>)),\n",
       "              ('token_embeddings',\n",
       "               tensor([[-0.6795, -0.2041,  1.0153,  ...,  0.4284, -1.7024, -0.2833],\n",
       "                       [-0.1609, -0.2013,  0.9313,  ...,  0.1025, -0.6252, -0.4562],\n",
       "                       [-0.0846, -0.2399,  0.2524,  ...,  0.1644,  0.0220,  0.1787],\n",
       "                       ...,\n",
       "                       [ 0.2307,  0.0850, -0.3529,  ...,  0.3358,  0.3527,  0.3798],\n",
       "                       [-0.3223,  0.3806, -0.7739,  ..., -0.0810,  0.4421,  0.2971],\n",
       "                       [ 0.2760, -0.0849, -0.0120,  ...,  0.2688,  0.4413,  0.1424]],\n",
       "                      device='cuda:0'))])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
