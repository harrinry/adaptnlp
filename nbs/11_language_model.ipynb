{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp language_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "> Language Models within the AdaptNLP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# coding=utf-8\n",
    "# This file uses code from the language modeling examples in the huggingface Transformer's repo\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import math\n",
    "from typing import Dict, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelWithLMHead,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    # TODO: For XLNet, will be available in Transformers release 3.0.2+\n",
    "    # DataCollatorForPermutationLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    ")\n",
    "\n",
    "from adaptnlp.model_hub import HFModelResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LMFineTuner:\n",
    "    \"\"\"\n",
    "     A Language Model Fine Tuner object you can set language model configurations and then train and evaluate\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> finetuner = adaptnlp.LMFineTuner()\n",
    "    >>> finetuner.train()\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **model_name_or_path** - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path:Union[str, HFModelResult]=\"bert-base-cased\",\n",
    "    ):\n",
    "\n",
    "        logger.info(\n",
    "            \"This is the new updated `LMFineTuner` class object for 0.2.0+. If you're looking for `LMFineTuner` from <=0.1.6, you can instantiate it with LMFineTunerManual\"\n",
    "        )\n",
    "        # Load model and tokenizer\n",
    "        name = getattr(model_name_or_path, 'name', model_name_or_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            name, use_fast=True\n",
    "        )\n",
    "        # TODO: AutoModelWithLMHead deprecated, update to causal, mask, or seq2seq\n",
    "        self.model = AutoModelWithLMHead.from_pretrained(name)\n",
    "        self.trainer = None\n",
    "\n",
    "        # Setup cuda and automatic allocation of model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        training_args: TrainingArguments,\n",
    "        train_file: Union[str, Path],\n",
    "        eval_file: Union[str, Path],\n",
    "        line_by_line: bool = False,\n",
    "        mlm: bool = False,\n",
    "        mlm_probability: float = 0.15,\n",
    "        plm_probability: float = 1 / 6,\n",
    "        max_span_length: int = 5,\n",
    "        block_size: int = -1,\n",
    "        overwrite_cache: bool = False,\n",
    "    ):\n",
    "        \"\"\"Train and fine-tune the loaded language model\n",
    "\n",
    "        * **train_file** - The input training data file (a text file).\n",
    "        * **eval_file** - An optional input evaluation data file to evaluate the perplexity on (a text file).\n",
    "        * **line_by_line** - Whether distinct lines of text in the dataset are to be handled as distinct sequences.\n",
    "        * **mlm** - Train with masked-language modeling loss instead of language modeling.\n",
    "        * **mlm_probability** - Ratio of tokens to mask for masked language modeling loss\n",
    "        * **plm_probability** - Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling.\n",
    "        * **max_span_length** - Maximum length of a span of masked tokens for permutation language modeling.\n",
    "        * **block_size** - Optional input sequence length after tokenization.\n",
    "                            The training dataset will be truncated in block of this size for training.\"\n",
    "                            `-1` will default to the model max input length for single sentence inputs (take into account special tokens).\n",
    "        * **overwrite_cache** - Overwrite the cached training and evaluation sets\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "            datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "            level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "        )\n",
    "        logger.warning(\n",
    "            f\"\"\"Process rank: {training_args.local_rank},\n",
    "                device: {training_args.device},\n",
    "                n_gpu: {training_args.n_gpu},\n",
    "                distributed training: {bool(training_args.local_rank != -1)},\n",
    "                16-bits training: {training_args.fp16}\n",
    "            \"\"\"\n",
    "        )\n",
    "        logger.info(f\"Training/evaluation parameters: {training_args.to_json_string()}\")\n",
    "\n",
    "        # Check if masked language model or not\n",
    "        if (\n",
    "            self.model.config.model_type\n",
    "            in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"]\n",
    "            and not mlm\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"\"\"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run with\n",
    "                mlm set as True(masked language modeling).\"\"\"\n",
    "            )\n",
    "\n",
    "        # Check block size for Dataset\n",
    "        if block_size <= 0:\n",
    "            block_size = self.tokenizer.max_len\n",
    "        else:\n",
    "            block_size = min(block_size, self.tokenizer.max_len)\n",
    "\n",
    "        # Get datasets\n",
    "        train_dataset = self._get_dataset(\n",
    "            file_path=train_file,\n",
    "            line_by_line=line_by_line,\n",
    "            block_size=block_size,\n",
    "            overwrite_cache=overwrite_cache,\n",
    "        )\n",
    "        eval_dataset = self._get_dataset(\n",
    "            file_path=eval_file,\n",
    "            line_by_line=line_by_line,\n",
    "            block_size=block_size,\n",
    "            overwrite_cache=overwrite_cache,\n",
    "        )\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "        # Get Collator\n",
    "        # TODO: DataCollatorForPermutationLanguageModeling not availbe until release 3.0.2+\n",
    "        if self.model.config.model_type == \"xlnet\":\n",
    "            logger.info(\"Cannot currently finetune XLNet model\")\n",
    "            raise ValueError(\n",
    "                \"Use another language model besides XLNet for LM finetuning\"\n",
    "            )\n",
    "            \"\"\"\n",
    "            data_collator = DataCollatorForPermutationLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            plm_probability=plm_probability,\n",
    "            max_span_length=max_span_length,\n",
    "            )\n",
    "            \"\"\"\n",
    "        else:\n",
    "            data_collator = DataCollatorForLanguageModeling(\n",
    "                tokenizer=self.tokenizer, mlm=mlm, mlm_probability=mlm_probability\n",
    "            )\n",
    "\n",
    "        # Initialize Trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "        )\n",
    "\n",
    "        # Train and serialize\n",
    "        self.trainer.train()\n",
    "        self.trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    def evaluate(self) -> Dict[str, float]:\n",
    "\n",
    "        if not self.trainer:\n",
    "            logger.info(\n",
    "                \"No trainer loaded, you should probably run `LMFineTuner.train(...)` first\"\n",
    "            )\n",
    "            return None\n",
    "        results = {}\n",
    "\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        eval_output = self.trainer.evaluate()\n",
    "\n",
    "        perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "        result = {\"perplexity\": perplexity}\n",
    "\n",
    "        output_eval_file = os.path.join(\n",
    "            self.trainer.args.output_dir, \"eval_results_lm.txt\"\n",
    "        )\n",
    "\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        results.update(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _get_dataset(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        line_by_line: bool,\n",
    "        block_size: int,\n",
    "        overwrite_cache: bool,\n",
    "    ) -> Dataset:\n",
    "        if line_by_line:\n",
    "            return LineByLineTextDataset(\n",
    "                tokenizer=self.tokenizer, file_path=file_path, block_size=block_size\n",
    "            )\n",
    "        else:\n",
    "            return TextDataset(\n",
    "                tokenizer=self.tokenizer,\n",
    "                file_path=file_path,\n",
    "                block_size=block_size,\n",
    "                overwrite_cache=overwrite_cache,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
