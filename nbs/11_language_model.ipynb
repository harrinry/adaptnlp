{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp language_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "> Language Models within the AdaptNLP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# coding=utf-8\n",
    "# This file uses code from the language modeling examples in the huggingface Transformer's repo\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import math\n",
    "from typing import Dict, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    # TODO: For XLNet, will be available in Transformers release 3.0.2+\n",
    "    # DataCollatorForPermutationLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    ")\n",
    "\n",
    "from adaptnlp.model_hub import HFModelResult\n",
    "\n",
    "from fastcore.basics import mk_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_types = {'Causal':'causal-lm', 'Masked':'masked-lm', 'Seq2Seq':'seq2seq'}\n",
    "mk_class('LMClass', **_types,\n",
    "        doc=\"All possible Language Model types as attributes to get tab-completion and typo-proofing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LMFineTuner:\n",
    "    \"\"\"\n",
    "     A Language Model Fine Tuner object you can set language model configurations and then train and evaluate\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> finetuner = adaptnlp.LMFineTuner()\n",
    "    >>> finetuner.train()\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **model_name_or_path** - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\n",
    "    * **language_model_class** - The type of language model you are trying to train, such as \"causal\" or \"seq2seq\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def _get_automodel_func(self, tags):\n",
    "        if 'causal-lm' in tags: return AutoModelForCausalLM, 'causal-lm'\n",
    "        elif 'masked-lm' in tags: return AutoModelForMaskedLM, 'masked-lm'\n",
    "        elif 'seq2seq' in tags: return AutoModelForSeq2SeqLM, 'seq2seq'\n",
    "        else: raise ValueError(f'Not a valid Language Model type: {tags[0]}')\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path:Union[str, HFModelResult]=\"bert-base-cased\",\n",
    "        language_model_class:Union[str, LMClass] = 'causal-lm',\n",
    "    ):\n",
    "\n",
    "        logger.info(\n",
    "            \"This is the new updated `LMFineTuner` class object for 0.2.0+. If you're looking for `LMFineTuner` from <=0.1.6, you can instantiate it with LMFineTunerManual\"\n",
    "        )\n",
    "        # Load model and tokenizer\n",
    "        name = getattr(model_name_or_path, 'name', model_name_or_path)\n",
    "        if not isinstance(model_name_or_path, HFModelResult) and language_model_class is None:\n",
    "            raise ValueError(\"\"\"\n",
    "            No `language_model_class` was passed in with a model string. \n",
    "            Please specify the type of language model it is (Either causal, masked, or seq2seq). \n",
    "            \n",
    "            To find the proper type, search your model on the HuggingFaceHub and you will see its tag near the top,\n",
    "              such as \"causal-lm\" or \"seq2seq\"\n",
    "            \"\"\")\n",
    "        \n",
    "        tags = getattr('tags', model_name_or_path, [language_model_class])\n",
    "        model_constructor, self.lm_class = self._get_automodel_func(tags)\n",
    "        self.model = model_constructor.from_pretrained(name)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            name, use_fast=True\n",
    "        )\n",
    "        self.trainer = None\n",
    "\n",
    "        # Setup cuda and automatic allocation of model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        training_args: TrainingArguments,\n",
    "        train_file: Union[str, Path],\n",
    "        eval_file: Union[str, Path],\n",
    "        line_by_line: bool = False,\n",
    "        mlm: bool = False,\n",
    "        mlm_probability: float = 0.15,\n",
    "        plm_probability: float = 1 / 6,\n",
    "        max_span_length: int = 5,\n",
    "        block_size: int = -1,\n",
    "        overwrite_cache: bool = False,\n",
    "    ):\n",
    "        \"\"\"Train and fine-tune the loaded language model\n",
    "\n",
    "        * **train_file** - The input training data file (a text file).\n",
    "        * **eval_file** - An optional input evaluation data file to evaluate the perplexity on (a text file).\n",
    "        * **line_by_line** - Whether distinct lines of text in the dataset are to be handled as distinct sequences.\n",
    "        * **mlm** - Train with masked-language modeling loss instead of language modeling.\n",
    "        * **mlm_probability** - Ratio of tokens to mask for masked language modeling loss\n",
    "        * **plm_probability** - Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling.\n",
    "        * **max_span_length** - Maximum length of a span of masked tokens for permutation language modeling.\n",
    "        * **block_size** - Optional input sequence length after tokenization.\n",
    "                            The training dataset will be truncated in block of this size for training.\"\n",
    "                            `-1` will default to the model max input length for single sentence inputs (take into account special tokens).\n",
    "        * **overwrite_cache** - Overwrite the cached training and evaluation sets\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "            datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "            level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "        )\n",
    "        logger.warning(\n",
    "            f\"\"\"Process rank: {training_args.local_rank},\n",
    "                device: {training_args.device},\n",
    "                n_gpu: {training_args.n_gpu},\n",
    "                distributed training: {bool(training_args.local_rank != -1)},\n",
    "                16-bits training: {training_args.fp16}\n",
    "            \"\"\"\n",
    "        )\n",
    "        logger.info(f\"Training/evaluation parameters: {training_args.to_json_string()}\")\n",
    "\n",
    "        # Check if masked language model or not\n",
    "        if (\n",
    "            self.model.config.model_type\n",
    "            in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"]\n",
    "            and not mlm\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"\"\"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run with\n",
    "                mlm set as True(masked language modeling).\"\"\"\n",
    "            )\n",
    "\n",
    "        # Check block size for Dataset\n",
    "        if block_size <= 0:\n",
    "            block_size = self.tokenizer.model_max_length\n",
    "        else:\n",
    "            block_size = min(block_size, self.tokenizer.model_max_length)\n",
    "\n",
    "        # Get datasets\n",
    "        train_dataset = self._get_dataset(\n",
    "            file_path=train_file,\n",
    "            line_by_line=line_by_line,\n",
    "            block_size=block_size,\n",
    "            overwrite_cache=overwrite_cache,\n",
    "        )\n",
    "        eval_dataset = self._get_dataset(\n",
    "            file_path=eval_file,\n",
    "            line_by_line=line_by_line,\n",
    "            block_size=block_size,\n",
    "            overwrite_cache=overwrite_cache,\n",
    "        )\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "        # Get Collator\n",
    "        # TODO: DataCollatorForPermutationLanguageModeling not availbe until release 3.0.2+\n",
    "        if self.model.config.model_type == \"xlnet\":\n",
    "            logger.info(\"Cannot currently finetune XLNet model\")\n",
    "            raise ValueError(\n",
    "                \"Use another language model besides XLNet for LM finetuning\"\n",
    "            )\n",
    "            \"\"\"\n",
    "            data_collator = DataCollatorForPermutationLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            plm_probability=plm_probability,\n",
    "            max_span_length=max_span_length,\n",
    "            )\n",
    "            \"\"\"\n",
    "        else:\n",
    "            data_collator = DataCollatorForLanguageModeling(\n",
    "                tokenizer=self.tokenizer, mlm=mlm, mlm_probability=mlm_probability\n",
    "            )\n",
    "\n",
    "        # Initialize Trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "        )\n",
    "\n",
    "        # Train and serialize\n",
    "        self.trainer.train()\n",
    "        self.trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    def evaluate(self) -> Dict[str, float]:\n",
    "\n",
    "        if not self.trainer:\n",
    "            logger.info(\n",
    "                \"No trainer loaded, you should probably run `LMFineTuner.train(...)` first\"\n",
    "            )\n",
    "            return None\n",
    "        results = {}\n",
    "\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        eval_output = self.trainer.evaluate()\n",
    "\n",
    "        perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "        result = {\"perplexity\": perplexity}\n",
    "\n",
    "        output_eval_file = os.path.join(\n",
    "            self.trainer.args.output_dir, \"eval_results_lm.txt\"\n",
    "        )\n",
    "\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        results.update(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _get_dataset(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        line_by_line: bool,\n",
    "        block_size: int,\n",
    "        overwrite_cache: bool,\n",
    "    ) -> Dataset:\n",
    "        if line_by_line:\n",
    "            return LineByLineTextDataset(\n",
    "                tokenizer=self.tokenizer, file_path=file_path, block_size=block_size\n",
    "            )\n",
    "        else:\n",
    "            return TextDataset(\n",
    "                tokenizer=self.tokenizer,\n",
    "                file_path=file_path,\n",
    "                block_size=block_size,\n",
    "                overwrite_cache=overwrite_cache,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-18 16:30:35--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.21.45\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.21.45|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4721645 (4.5M) [application/zip]\n",
      "Saving to: ‘wikitext-2-raw-v1.zip’\n",
      "\n",
      "wikitext-2-raw-v1.z 100%[===================>]   4.50M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-06-18 16:30:35 (162 MB/s) - ‘wikitext-2-raw-v1.zip’ saved [4721645/4721645]\n",
      "\n",
      "Archive:  wikitext-2-raw-v1.zip\n",
      "   creating: wikitext-2-raw/\n",
      "  inflating: wikitext-2-raw/wiki.test.raw  \n",
      "  inflating: wikitext-2-raw/wiki.valid.raw  \n",
      "  inflating: wikitext-2-raw/wiki.train.raw  \n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
    "!unzip wikitext-2-raw-v1.zip\n",
    "\n",
    "train_file = \"./wikitext-2-raw/wiki.train.raw\"\n",
    "eval_file = \"./wikitext-2-raw/wiki.test.raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"no\",\n",
    "    logging_dir='../logs',\n",
    "    save_steps=2500,\n",
    "    eval_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "finetuner = LMFineTuner(model_name_or_path='gpt2', language_model_class='causal-lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/18/2021 17:05:36 - WARNING - __main__ -   Process rank: -1,\n",
      "                device: cuda:0,\n",
      "                n_gpu: 1,\n",
      "                distributed training: False,\n",
      "                16-bits training: False\n",
      "            \n",
      "06/18/2021 17:05:36 - INFO - __main__ -   Training/evaluation parameters: {\n",
      "  \"output_dir\": \"../models\",\n",
      "  \"overwrite_output_dir\": false,\n",
      "  \"do_train\": false,\n",
      "  \"do_eval\": false,\n",
      "  \"do_predict\": false,\n",
      "  \"evaluation_strategy\": \"no\",\n",
      "  \"prediction_loss_only\": false,\n",
      "  \"per_device_train_batch_size\": 1,\n",
      "  \"per_device_eval_batch_size\": 1,\n",
      "  \"per_gpu_train_batch_size\": null,\n",
      "  \"per_gpu_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"eval_accumulation_steps\": null,\n",
      "  \"learning_rate\": 5e-05,\n",
      "  \"weight_decay\": 0.01,\n",
      "  \"adam_beta1\": 0.9,\n",
      "  \"adam_beta2\": 0.999,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"max_steps\": -1,\n",
      "  \"lr_scheduler_type\": \"linear\",\n",
      "  \"warmup_ratio\": 0.0,\n",
      "  \"warmup_steps\": 500,\n",
      "  \"logging_dir\": \"../logs\",\n",
      "  \"logging_strategy\": \"steps\",\n",
      "  \"logging_first_step\": false,\n",
      "  \"logging_steps\": 500,\n",
      "  \"save_strategy\": \"steps\",\n",
      "  \"save_steps\": 2500,\n",
      "  \"save_total_limit\": null,\n",
      "  \"no_cuda\": false,\n",
      "  \"seed\": 42,\n",
      "  \"fp16\": false,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"fp16_backend\": \"auto\",\n",
      "  \"fp16_full_eval\": false,\n",
      "  \"local_rank\": -1,\n",
      "  \"tpu_num_cores\": null,\n",
      "  \"tpu_metrics_debug\": false,\n",
      "  \"debug\": [],\n",
      "  \"dataloader_drop_last\": false,\n",
      "  \"eval_steps\": 100,\n",
      "  \"dataloader_num_workers\": 0,\n",
      "  \"past_index\": -1,\n",
      "  \"run_name\": \"../models\",\n",
      "  \"disable_tqdm\": false,\n",
      "  \"remove_unused_columns\": true,\n",
      "  \"label_names\": null,\n",
      "  \"load_best_model_at_end\": false,\n",
      "  \"metric_for_best_model\": null,\n",
      "  \"greater_is_better\": null,\n",
      "  \"ignore_data_skip\": false,\n",
      "  \"sharded_ddp\": [],\n",
      "  \"deepspeed\": null,\n",
      "  \"label_smoothing_factor\": 0.0,\n",
      "  \"adafactor\": false,\n",
      "  \"group_by_length\": false,\n",
      "  \"length_column_name\": \"length\",\n",
      "  \"report_to\": [],\n",
      "  \"ddp_find_unused_parameters\": null,\n",
      "  \"dataloader_pin_memory\": true,\n",
      "  \"skip_memory_metrics\": false,\n",
      "  \"use_legacy_prediction_loop\": false,\n",
      "  \"push_to_hub\": false,\n",
      "  \"resume_from_checkpoint\": null,\n",
      "  \"_n_gpu\": 1,\n",
      "  \"mp_parameters\": \"\"\n",
      "}\n",
      "/opt/venv/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "06/18/2021 17:05:36 - INFO - filelock -   Lock 140612913534192 acquired on ./wikitext-2-raw/cached_lm_GPT2TokenizerFast_1024_wiki.test.raw.lock\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (286221 > 1024). Running this sequence through the model will result in indexing errors\n",
      "06/18/2021 17:05:38 - INFO - filelock -   Lock 140612913534192 released on ./wikitext-2-raw/cached_lm_GPT2TokenizerFast_1024_wiki.test.raw.lock\n",
      "06/18/2021 17:05:38 - INFO - filelock -   Lock 140612913620640 acquired on ./wikitext-2-raw/cached_lm_GPT2TokenizerFast_1024_wiki.test.raw.lock\n",
      "06/18/2021 17:05:38 - INFO - filelock -   Lock 140612913620640 released on ./wikitext-2-raw/cached_lm_GPT2TokenizerFast_1024_wiki.test.raw.lock\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='279' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [279/279 01:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "finetuner.train(\n",
    "    training_args=training_args,\n",
    "    train_file=eval_file,\n",
    "    eval_file=eval_file,\n",
    "    mlm=False,\n",
    "    overwrite_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/18/2021 17:08:00 - INFO - __main__ -   *** Evaluate ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='279' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [279/279 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/18/2021 17:08:35 - INFO - __main__ -   ***** Eval results *****\n",
      "06/18/2021 17:08:35 - INFO - __main__ -     perplexity = 19.988802908297547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': 19.988802908297547}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuner.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from adaptnlp import EasyTextGenerator\n",
    "\n",
    "text = \"China and the U.S. will begin to\"\n",
    "\n",
    "generator = EasyTextGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "06/18/2021 17:09:16 - INFO - adaptnlp.text_generation -   Running text generator on 1 text sequences\n",
      "06/18/2021 17:09:16 - INFO - adaptnlp.text_generation -   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['China and the U.S. will begin to work together to develop a new energy source for the country.\\n\\nThe U.S. is also working with China on a new energy source for the country, the South China Sea, which China has said is a \"strategic and economic priority']\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "generated_text = generator.generate(\n",
    "    text, \n",
    "    model_name_or_path=\"../models\", \n",
    "    num_tokens_to_produce=50\n",
    ")\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "06/18/2021 17:09:37 - INFO - adaptnlp.text_generation -   Running text generator on 1 text sequences\n",
      "06/18/2021 17:09:37 - INFO - adaptnlp.text_generation -   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['China and the U.S. will begin to see the effects of the new sanctions on the Russian economy.\\n\\n\"The U.S. is going to be the first to see the effects of the new sanctions,\" said Michael O\\'Hanlon, a senior fellow at the Center for Strategic']\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "generated_text = generator.generate(\n",
    "    text, \n",
    "    model_name_or_path=\"gpt2\", \n",
    "    num_tokens_to_produce=50\n",
    ")\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_file_utils.ipynb.\n",
      "Converted 01_callback.ipynb.\n",
      "Converted 02_model_hub.ipynb.\n",
      "Converted 03_model.ipynb.\n",
      "Converted 04_embeddings.ipynb.\n",
      "Converted 04a_tutorial.embeddings.ipynb.\n",
      "Converted 05_token_classification.ipynb.\n",
      "Converted 05a_tutorial.token_tagging.ipynb.\n",
      "Converted 06_sequence_classification.ipynb.\n",
      "Converted 06a_tutorial.easy_sequence_classifier.ipynb.\n",
      "Converted 07_summarization.ipynb.\n",
      "Converted 07a_tutorial.summarization.ipynb.\n",
      "Converted 08_translation.ipynb.\n",
      "Converted 08a_tutorial.translation.ipynb.\n",
      "Converted 09_text_generation.ipynb.\n",
      "Converted 09a_tutorial.easy_text_generator.ipynb.\n",
      "Converted 10_question_answering.ipynb.\n",
      "Converted 10a_tutorial.question_answering.ipynb.\n",
      "Converted 11_language_model.ipynb.\n",
      "Converted 12_training.ipynb.\n",
      "Converted 13a_transformers.squad_metrics.ipynb.\n",
      "Converted 13b_transformers.finetuning.ipynb.\n",
      "Converted 13c_transformers.utils_squad_evaluate.ipynb.\n",
      "Converted 20a_tutorial.fine_tuning_lm.ipynb.\n",
      "Converted 20b_tutorial.fine_tuning_manual.ipynb.\n",
      "Converted 20c_tutorial.flair_seq_class_trainer.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
