{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Sequence Classification\n",
    "> Performing Sequence Classification with AdaptNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence Classification (or Text Classification) is the NLP task of predicting a label for a sequence of words.\n",
    "\n",
    "For example, a string of `That movie was terrible because the acting was bad` could be tagged with a label of `negative`. A string of `That movie was great because the acting was good` could be tagged with a label of `positive`.\n",
    "\n",
    "A model that can predict sentiment from text is called a sentiment classifier, which is an example of a sequence classification model.\n",
    "\n",
    " Below, we'll walk through how we can use AdaptNLP's EasySequenceClassification module to easily do the following:\n",
    "1. Load pre-trained models and tag data using mini-batched inference\n",
    "2. Train and fine-tune a pre-trained model on your own dataset\n",
    "3. Evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Models and Tag Data using Mini-Batched Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first get started by importing the EasySequenceClassifier class from AdaptNLP and instantiating the\n",
    "`EasySequenceClassifier` class object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp import EasySequenceClassifier\n",
    "from pprint import pprint\n",
    "\n",
    "classifier = EasySequenceClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this class we can dynamically load models to run on inference. \n",
    "\n",
    "Let's use the `HFModelHub` to search for some pre-trained sequence classification models to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp import HFModelHub\n",
    "hub = HFModelHub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either seach by task or by model name. Below is an example of the associated models HuggingFace has come out with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model Name: distilbert-base-uncased-finetuned-sst-2-english, Tasks: [text-classification],\n",
       " Model Name: roberta-base-openai-detector, Tasks: [text-classification],\n",
       " Model Name: roberta-large-mnli, Tasks: [text-classification],\n",
       " Model Name: roberta-large-openai-detector, Tasks: [text-classification]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub.search_model_by_task('text-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example though we will tag some text with a model that [NLP Town](https://www.nlp.town/) has trained called `nlptown/bert-base-multilingual-uncased-sentiment`. Let's find it in the model hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Name: nlptown/bert-base-multilingual-uncased-sentiment, Tasks: [text-classification]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = hub.search_model_by_name('nlptown/bert-base', user_uploaded=True)[0]; model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a multi-lingual model that predicts how many stars (1-5) a text review has given a product. More information can be found via. the Transformers model card [here](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can perform some inference. First let's write some example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"This didn't work at all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can tell our classifier to tag some text with `tag_text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-31 16:09:42,794 loading file nlptown/bert-base-multilingual-uncased-sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2104: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = classifier.tag_text(\n",
    "    text=example_text,\n",
    "    model_name_or_path=model,\n",
    "    mini_batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at our outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "Sentences: [\"This did n't work at all\"]\n",
      "\n",
      "Classes: ['1 star']\n",
      "\n",
      "Probabilities: \n",
      "\ttensor([[8.4212e-01, 1.3793e-01, 1.8024e-02, 1.2419e-03, 6.8153e-04]])\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Tag Score Outputs:\\n\")\n",
    "print(f'Sentences: {sentences[\"sentences\"]}\\n')\n",
    "print(f'Classes: {sentences[\"classes\"]}\\n')\n",
    "print(f\"Probabilities: \\n\\t{sentences['probs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to pass in multiple sentences at once as well (in an array). Let's try that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_text = [\"This didn't work well at all.\",\n",
    "                 \"I really liked it.\",\n",
    "                 \"It was really useful.\",\n",
    "                 \"It broke after I bought it.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pass it into the `classifier` just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = classifier.tag_text(\n",
    "    text=multiple_text,\n",
    "    model_name_or_path=model,\n",
    "    mini_batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check the outputs again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "Sentences: [\"This did n't work well at all .\", 'I really liked it .', 'It was really useful .', 'It broke after I bought it .']\n",
      "\n",
      "Classes: ['1 star', '4 stars', '5 stars', '1 star']\n",
      "\n",
      "Probabilities: \n",
      "\ttensor([[6.2198e-01, 3.3563e-01, 4.0320e-02, 1.5827e-03, 4.8790e-04],\n",
      "        [3.2305e-03, 4.7872e-03, 5.4017e-02, 4.8129e-01, 4.5668e-01],\n",
      "        [5.9679e-03, 9.2630e-03, 7.0121e-02, 4.1363e-01, 5.0102e-01],\n",
      "        [4.4894e-01, 3.9348e-01, 1.4158e-01, 1.2110e-02, 3.8937e-03]])\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Tag Score Outputs:\\n\")\n",
    "print(f'Sentences: {sentences[\"sentences\"]}\\n')\n",
    "print(f'Classes: {sentences[\"classes\"]}\\n')\n",
    "print(f\"Probabilities: \\n\\t{sentences['probs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The output is going to be a probility distribution of what the text should be tagged. If you're running this on a GPU, you can specify the `mini_batch_size` parameter to run mini-batch inference against your data for faster run time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set `model_name_or_path` to any of Transformer's or Flair's pre-trained sequence classification models.\n",
    "\n",
    "Let's tag some text with another model, specifically Oliver Guhr's German sentiment model called `oliverguhr/german-sentiment-bert`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll write some german text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text = [\"Das hat 端berhaupt nicht gut funktioniert.\",\n",
    "               \"Ich mochte es wirklich.\",\n",
    "               \"Es war wirklich n端tzlich.\",\n",
    "               \"Es ist kaputt gegangen, nachdem ich es gekauft habe.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then tag it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = classifier.tag_text(\n",
    "    german_text,\n",
    "    model_name_or_path=\"oliverguhr/german-sentiment-bert\",\n",
    "    mini_batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: As seen here, you can either search for a model through the various `ModelHub` classes, or you can directly pass in the string to the model you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "Sentences: ['Das hat 端berhaupt nicht gut funktioniert .', 'Ich mochte es wirklich .', 'Es war wirklich n端tzlich .', 'Es ist kaputt gegangen , nachdem ich es gekauft habe .']\n",
      "\n",
      "Classes: ['negative', 'positive', 'positive', 'negative']\n",
      "\n",
      "Probabilities: \n",
      "\ttensor([[8.2706e-04, 9.9915e-01, 2.7373e-05],\n",
      "        [7.0231e-01, 2.0294e-01, 9.4746e-02],\n",
      "        [9.8132e-01, 1.8442e-02, 2.3914e-04],\n",
      "        [4.2462e-03, 9.9566e-01, 9.4817e-05]])\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Tag Score Outputs:\\n\")\n",
    "print(f'Sentences: {sentences[\"sentences\"]}\\n')\n",
    "print(f'Classes: {sentences[\"classes\"]}\\n')\n",
    "print(f\"Probabilities: \\n\\t{sentences['probs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget you can still quickly run inference with the multi-lingual review sentiment model you loaded in earlier (memory permitting)! Just change the `model_name_or_path` param to the model you used before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
