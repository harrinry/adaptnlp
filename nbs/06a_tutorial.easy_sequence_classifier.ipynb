{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Sequence Classification\n",
    "> Performing Sequence Classification with AdaptNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence Classification (or Text Classification) is the NLP task of predicting a label for a sequence of words.\n",
    "\n",
    "For example, a string of `That movie was terrible because the acting was bad` could be tagged with a label of `negative`. A string of `That movie was great because the acting was good` could be tagged with a label of `positive`.\n",
    "\n",
    "A model that can predict sentiment from text is called a sentiment classifier, which is an example of a sequence classification model.\n",
    "\n",
    " Below, we'll walk through how we can use AdaptNLP's EasySequenceClassification module to easily do the following:\n",
    "1. Load pre-trained models and tag data using mini-batched inference\n",
    "2. Train and fine-tune a pre-trained model on your own dataset\n",
    "3. Evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Models and Tag Data using Mini-Batched Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first get started by importing the EasySequenceClassifier class from AdaptNLP and instantiating the\n",
    "`EasySequenceClassifier` class object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp import EasySequenceClassifier\n",
    "from pprint import pprint\n",
    "\n",
    "classifier = EasySequenceClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this class we can dynamically load models to run on inference. \n",
    "\n",
    "Let's use the `HFModelHub` to search for some pre-trained sequence classification models to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp.model_hub import HFModelHub\n",
    "hub = HFModelHub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either seach by task or by model name. Below is an example of the associated models HuggingFace has come out with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model Name: distilbert-base-uncased-finetuned-sst-2-english, Tasks: [text-classification],\n",
       " Model Name: roberta-base-openai-detector, Tasks: [text-classification],\n",
       " Model Name: roberta-large-mnli, Tasks: [text-classification],\n",
       " Model Name: roberta-large-openai-detector, Tasks: [text-classification]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub.search_model_by_task('text-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example though we will tag some text with a model that [NLP Town](https://www.nlp.town/) has trained called `nlptown/bert-base-multilingual-uncased-sentiment`. Let's find it in the model hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Name: nlptown/bert-base-multilingual-uncased-sentiment, Tasks: [text-classification]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = hub.search_model_by_name('nlptown/bert-base', user_uploaded=True)[0]; model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a multi-lingual model that predicts how many stars (1-5) a text review has given a product. More information can be found via. the Transformers model card [here](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can perform some inference. First let's write some example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"This didn't work at all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can tell our classifier to tag some text with `tag_text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-31 16:09:42,794 loading file nlptown/bert-base-multilingual-uncased-sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2104: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = classifier.tag_text(\n",
    "    text=example_text,\n",
    "    model_name_or_path=model,\n",
    "    mini_batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at our outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "Sentences: [\"This did n't work at all\"]\n",
      "\n",
      "Classes: ['1 star']\n",
      "\n",
      "Probabilities: \n",
      "\ttensor([[8.4212e-01, 1.3793e-01, 1.8024e-02, 1.2419e-03, 6.8153e-04]])\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Tag Score Outputs:\\n\")\n",
    "print(f'Sentences: {sentences[\"sentences\"]}\\n')\n",
    "print(f'Classes: {sentences[\"classes\"]}\\n')\n",
    "print(f\"Probabilities: \\n\\t{sentences['probs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to pass in multiple sentences at once as well (in an array). Let's try that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_text = [\"This didn't work well at all.\",\n",
    "                 \"I really liked it.\",\n",
    "                 \"It was really useful.\",\n",
    "                 \"It broke after I bought it.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pass it into the `classifier` just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = classifier.tag_text(\n",
    "    text=multiple_text,\n",
    "    model_name_or_path=model,\n",
    "    mini_batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check the outputs again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "Sentences: [\"This did n't work well at all .\", 'I really liked it .', 'It was really useful .', 'It broke after I bought it .']\n",
      "\n",
      "Classes: ['1 star', '4 stars', '5 stars', '1 star']\n",
      "\n",
      "Probabilities: \n",
      "\ttensor([[6.2198e-01, 3.3563e-01, 4.0320e-02, 1.5827e-03, 4.8790e-04],\n",
      "        [3.2305e-03, 4.7872e-03, 5.4017e-02, 4.8129e-01, 4.5668e-01],\n",
      "        [5.9679e-03, 9.2630e-03, 7.0121e-02, 4.1363e-01, 5.0102e-01],\n",
      "        [4.4894e-01, 3.9348e-01, 1.4158e-01, 1.2110e-02, 3.8937e-03]])\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Tag Score Outputs:\\n\")\n",
    "print(f'Sentences: {sentences[\"sentences\"]}\\n')\n",
    "print(f'Classes: {sentences[\"classes\"]}\\n')\n",
    "print(f\"Probabilities: \\n\\t{sentences['probs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The output is going to be a probility distribution of what the text should be tagged. If you're running this on a GPU, you can specify the `mini_batch_size` parameter to run mini-batch inference against your data for faster run time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set `model_name_or_path` to any of Transformer's or Flair's pre-trained sequence classification models.\n",
    "\n",
    "Let's tag some text with another model, specifically Oliver Guhr's German sentiment model called `oliverguhr/german-sentiment-bert`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll write some german text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text = [\"Das hat überhaupt nicht gut funktioniert.\",\n",
    "               \"Ich mochte es wirklich.\",\n",
    "               \"Es war wirklich nützlich.\",\n",
    "               \"Es ist kaputt gegangen, nachdem ich es gekauft habe.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then tag it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = classifier.tag_text(\n",
    "    german_text,\n",
    "    model_name_or_path=\"oliverguhr/german-sentiment-bert\",\n",
    "    mini_batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: As seen here, you can either search for a model through the various `ModelHub` classes, or you can directly pass in the string to the model you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "Sentences: ['Das hat überhaupt nicht gut funktioniert .', 'Ich mochte es wirklich .', 'Es war wirklich nützlich .', 'Es ist kaputt gegangen , nachdem ich es gekauft habe .']\n",
      "\n",
      "Classes: ['negative', 'positive', 'positive', 'negative']\n",
      "\n",
      "Probabilities: \n",
      "\ttensor([[8.2706e-04, 9.9915e-01, 2.7373e-05],\n",
      "        [7.0231e-01, 2.0294e-01, 9.4746e-02],\n",
      "        [9.8132e-01, 1.8442e-02, 2.3914e-04],\n",
      "        [4.2462e-03, 9.9566e-01, 9.4817e-05]])\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Tag Score Outputs:\\n\")\n",
    "print(f'Sentences: {sentences[\"sentences\"]}\\n')\n",
    "print(f'Classes: {sentences[\"classes\"]}\\n')\n",
    "print(f\"Probabilities: \\n\\t{sentences['probs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget you can still quickly run inference with the multi-lingual review sentiment model you loaded in earlier (memory permitting)! Just change the `model_name_or_path` param to the model you used before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's release the german sentiment model to free up some memory for our next step...training! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.release_model(model_name_or_path=\"oliverguhr/german-sentiment-bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Fine-Tune a Pre-Trained Model on Your Own Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine you have your own dataset with text/label pairs you'd like to create a sequence classification model for.\n",
    "\n",
    "With the easy sequence classifier, you can take advantage of transfer learning by fine-tuning pre-trained models on your own custom datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The `EasySequenceClassifier` is integrated heavily with the `datasets.Dataset` and `transformers.Trainer` class objects, so please check out the [datasets](https://huggingface.co/datasets) and [transformers](https://huggingface.co/transformers) documentation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first need a \"custom\" dataset to start training our model. Our `EasySequenceClassifier.train()` method can run with either `datasets.Dataset` objects or CSV data file paths. Since the datasets library makes it so easy, we'll use the `datasets.load_dataset()` method to load in the IMDB Sentiment dataset. We'll show an example with a CSV later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'builder_name': 'imdb',\n",
      " 'citation': '@InProceedings{maas-EtAl:2011:ACL-HLT2011,\\n'\n",
      "             '  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  '\n",
      "             'Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, '\n",
      "             'Christopher},\\n'\n",
      "             '  title     = {Learning Word Vectors for Sentiment Analysis},\\n'\n",
      "             '  booktitle = {Proceedings of the 49th Annual Meeting of the '\n",
      "             'Association for Computational Linguistics: Human Language '\n",
      "             'Technologies},\\n'\n",
      "             '  month     = {June},\\n'\n",
      "             '  year      = {2011},\\n'\n",
      "             '  address   = {Portland, Oregon, USA},\\n'\n",
      "             '  publisher = {Association for Computational Linguistics},\\n'\n",
      "             '  pages     = {142--150},\\n'\n",
      "             '  url       = {http://www.aclweb.org/anthology/P11-1015}\\n'\n",
      "             '}\\n',\n",
      " 'config_name': 'plain_text',\n",
      " 'dataset_size': 133190346,\n",
      " 'description': 'Large Movie Review Dataset.\\n'\n",
      "                'This is a dataset for binary sentiment classification '\n",
      "                'containing substantially more data than previous benchmark '\n",
      "                'datasets. We provide a set of 25,000 highly polar movie '\n",
      "                'reviews for training, and 25,000 for testing. There is '\n",
      "                'additional unlabeled data for use as well.',\n",
      " 'download_checksums': {'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz': {'checksum': 'c40f74a18d3b61f90feba1e17730e0d38e8b97c05fde7008942e91923d1658fe',\n",
      "                                                                                           'num_bytes': 84125825}},\n",
      " 'download_size': 84125825,\n",
      " 'features': {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None),\n",
      "              'text': Value(dtype='string', id=None)},\n",
      " 'homepage': 'http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      " 'license': '',\n",
      " 'post_processed': None,\n",
      " 'post_processing_size': None,\n",
      " 'size_in_bytes': 217316171,\n",
      " 'splits': {'test': SplitInfo(name='test', num_bytes=32650697, num_examples=25000, dataset_name='imdb'),\n",
      "            'train': SplitInfo(name='train', num_bytes=33432835, num_examples=25000, dataset_name='imdb'),\n",
      "            'unsupervised': SplitInfo(name='unsupervised', num_bytes=67106814, num_examples=50000, dataset_name='imdb')},\n",
      " 'supervised_keys': None,\n",
      " 'version': 1.0.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset, eval_dataset = load_dataset('imdb', split=['train[:1%]', 'test[:1%]'])\n",
    "\n",
    "# Uncomment below if you want to use all the data so you don't spend an hour+ on training and evaluation\n",
    "#train_dataset, eval_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "\n",
    "pprint(vars(train_dataset.info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a brief look at what the IMDB Sentiment dataset looks like. We can see that the label column has two classes of 0 and 1. You can see the name of the classes mapped to the integers with `train_dataset.features[\"names\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1</td>\n",
       "      <td>That hilarious line is typical of what these n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1</td>\n",
       "      <td>Faith and Mortality... viewed through the lens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1</td>\n",
       "      <td>The unlikely duo of Zero Mostel and Harry Bela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1</td>\n",
       "      <td>*some spoilers*&lt;br /&gt;&lt;br /&gt;I was pleasantly su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1</td>\n",
       "      <td>... and I DO mean it. If not literally (after ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0        1  Bromwell High is a cartoon comedy. It ran at t...\n",
       "1        1  Homelessness (or Houselessness as George Carli...\n",
       "2        1  Brilliant over-acting by Lesley Ann Warren. Be...\n",
       "3        1  This is easily the most underrated film inn th...\n",
       "4        1  This is not the typical Mel Brooks film. It wa...\n",
       "..     ...                                                ...\n",
       "245      1  That hilarious line is typical of what these n...\n",
       "246      1  Faith and Mortality... viewed through the lens...\n",
       "247      1  The unlikely duo of Zero Mostel and Harry Bela...\n",
       "248      1  *some spoilers*<br /><br />I was pleasantly su...\n",
       "249      1  ... and I DO mean it. If not literally (after ...\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.set_format(type=\"pandas\", columns=[\"text\", \"label\"])\n",
    "train_dataset[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reformat it back into a more \"pythonic\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(columns=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment below to see training done with CSV files. The cell below will just save the `datasets.Dataset` objects you have in `train_dataset` and `eval_dataset` as CSVs and will train the model with the CSV file paths. Ignore to just continue to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.set_format(type=\"pandas\", columns=[\"text\", \"label\"])\n",
    "#eval_dataset.set_format(type=\"pandas\", columns=[\"text\", \"label\"])\n",
    "\n",
    "#train_dataset[:].to_csv(\"./IMDB train.csv\", index=False)\n",
    "#eval_dataset[:].to_csv(\"./IMDB eval.csv\", index=False)\n",
    "\n",
    "#train_dataset = \"./IMDB train.csv\"\n",
    "#eval_dataset = \"./IMDB eval.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things we'll need to specify before we start training are the training arguments. Training arguments consist mainly of the hyperparameters we want to provide the model. These may include batch size, initial learning rate, number of epochs, etc.\n",
    "\n",
    "We will be using the `transformers.TrainingArguments` data class to store our training args. These are compatible with the `transformers.Trainer` as well as AdaptNLP's train methods. For more documention on the `TrainingArguments` class, please look [here](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments). There are a lot of arguments available, but we will pass in the important args and use default values for the rest.\n",
    "\n",
    "The training arguments below specify the output directory for you model and checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_dir='./logs',\n",
    "    save_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the built-in `train()` method by passing in the training arguments. The training method will also be where you specify your data arguments which include the your train and eval datasets, the pre-trained model ID (this should have been loaded from your earlier cells, but can be loaded dynamically), text column name, label column name, and ordered label names (only required if loading in paths to CSV data file for dataset args).\n",
    "\n",
    "Please checkout AdaptNLP's package reference for more information [here](https://novetta.github.io/adaptnlp/class-api/sequence-classifier-module.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(training_args=training_args,\n",
    "                 train_dataset=train_dataset,\n",
    "                 eval_dataset=eval_dataset,\n",
    "                 model_name_or_path=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "                 text_col_nm=\"text\",\n",
    "                 label_col_nm=\"label\",\n",
    "                 label_names=[\"positive\",\"negative\"]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you can evaluate the model with the eval dataset you passed in for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.017184646800160408,\n",
       " 'eval_accuracy': 1.0,\n",
       " 'eval_f1': array([1.]),\n",
       " 'eval_precision': array([1.]),\n",
       " 'eval_recall': array([1.]),\n",
       " 'eval_runtime': 2.7201,\n",
       " 'eval_samples_per_second': 91.91,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(model_name_or_path=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see it's a little weird that we're still using the `model_name_or_path` of the pre-trained model we fine-tuned and took advantage of via. transfer learning. We can release the model we've fine-tuned, and then load it back in using the directory that we've serialized the fine-tuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.release_model(model_name_or_path=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-20 19:43:56,203 loading file ./models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "{\"This didn't work well at all.\": [neg (0.263), pos (0.737)]}\n",
      "{'I really liked it.': [neg (0.1309), pos (0.8691)]}\n",
      "{'It was really useful.': [neg (0.184), pos (0.816)]}\n",
      "{'It broke after I bought it.': [neg (0.2716), pos (0.7284)]}\n"
     ]
    }
   ],
   "source": [
    "sentences = classifier.tag_text(\n",
    "    multiple_text,\n",
    "    model_name_or_path=\"./models\",\n",
    "    mini_batch_size=1\n",
    ")\n",
    "\n",
    "print(f\"Tag Score Outputs:\\n\")\n",
    "print(f'Sentences: {sentences[\"sentences\"]}\\n')\n",
    "print(f'Classes: {sentences[\"classes\"]}\\n')\n",
    "print(f\"Probabilities: \\n\\t{sentences['probs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
