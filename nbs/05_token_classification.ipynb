{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Tagging and Classification\n",
    "> AdaptiveModels for using Transformers and Flair for token tagging and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    BertForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    ")\n",
    "\n",
    "from adaptnlp.model import AdaptiveModel, DataLoader\n",
    "from adaptnlp.model_hub import HFModelResult, FlairModelResult, FlairModelHub, HFModelHub\n",
    "\n",
    "from fastai_minima.utils import to_detach, apply, to_device\n",
    "\n",
    "from fastcore.basics import Self, risinstance\n",
    "from fastcore.xtras import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTokenTagger(AdaptiveModel):\n",
    "    \"\"\"Adaptive model for Transformer's Token Tagger Model\n",
    "\n",
    "    Usage:\n",
    "    ```python\n",
    "    >>> tagger = TransformersTokenTagger.load(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "    >>> tagger.predict(text=\"Example text\", mini_batch_size=32)\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **tokenizer** - A tokenizer object from Huggingface's transformers (TODO) and tokenizers\n",
    "    * **model** - A transformers token tagger model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n",
    "        # Load up model and tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        super().__init__()\n",
    "\n",
    "        # Sets the internal model\n",
    "        self.set_model(model)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading and constructing this tagger\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Transformer's pre-trained Token Tagger Model or a `HFModelResult`\n",
    "        \n",
    "        Note: To search for valid models, you should use the AdaptNLP `model_hub` API\n",
    "        \"\"\"\n",
    "        if isinstance(model_name_or_path, HFModelResult): model_name_or_path = model_name_or_path.name\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)\n",
    "        tagger = cls(tokenizer, model)\n",
    "        return tagger\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        grouped_entities: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> List[List[Dict]]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained token tagger model.\n",
    "        Returns a list of lists of tagged entities.\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **grouped_entities** - Set True to get whole entity span strings (Default True)\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers tagger\n",
    "        \"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        results: List[Dict] = []\n",
    "\n",
    "        dataset = self._tokenize(text)\n",
    "        dl = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "\n",
    "        logger.info(f'Running prediction on {len(dataset)} text sequences')\n",
    "        logger.info(f'Batch size = {mini_batch_size}')\n",
    "\n",
    "        outputs,_ = super().get_preds(dl=dl)\n",
    "\n",
    "        inputs = apply(to_device, [b for b in dl], device='cpu')\n",
    "        inputs = torch.cat(*inputs)\n",
    "        inputs = apply(Self.numpy(), inputs)\n",
    "\n",
    "        outputs = torch.cat([o['logits'] for o in outputs])\n",
    "        outputs = apply(to_detach, outputs, cpu=True)\n",
    "        outputs = apply(Self.numpy(), outputs)\n",
    "\n",
    "        # Iterate through batch for tagged token predictions\n",
    "        for idx, pred in enumerate(outputs):\n",
    "            entities = pred\n",
    "            input_ids = inputs[idx]\n",
    "            tagged_entities = self._generate_tagged_entities(\n",
    "                entities=entities,\n",
    "                input_ids=input_ids,\n",
    "                grouped_entities=grouped_entities\n",
    "            )\n",
    "            results += tagged_entities\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _tokenize(\n",
    "        self, sentences: Union[List[Sentence], Sentence, List[str], str]\n",
    "    ) -> TensorDataset:\n",
    "        \"\"\" Batch tokenizes text and produces a `TensorDataset` with them \"\"\"\n",
    "\n",
    "        tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "            sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            pad_to_max_length=True,\n",
    "        )\n",
    "\n",
    "        # Bart, XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use token_type_ids\n",
    "        if isinstance(\n",
    "            self.model,\n",
    "            (\n",
    "                BertForSequenceClassification,\n",
    "                XLNetForSequenceClassification,\n",
    "                AlbertForSequenceClassification,\n",
    "            ),\n",
    "        ):\n",
    "            dataset = TensorDataset(\n",
    "                tokenized_text[\"input_ids\"],\n",
    "                tokenized_text[\"attention_mask\"],\n",
    "                tokenized_text[\"token_type_ids\"],\n",
    "            )\n",
    "        else:\n",
    "            dataset = TensorDataset(\n",
    "                tokenized_text[\"input_ids\"], tokenized_text[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    # `_group_entites` and `_generate_tagged_entities` modified from pipeline code snippet from Transformers\n",
    "    def _group_entities(\n",
    "        self, entities: List[dict], idx_start: int, idx_end: int\n",
    "    ) -> Dict:\n",
    "        \"\"\"Returns grouped entities\"\"\"\n",
    "        # Get the last entity in the entity group\n",
    "        entity = entities[-1][\"entity\"]\n",
    "        scores = np.mean([entity[\"score\"] for entity in entities])\n",
    "        tokens = [entity[\"word\"] for entity in entities]\n",
    "\n",
    "        entity_group = {\n",
    "            \"entity_group\": entity,\n",
    "            \"score\": np.mean(scores),\n",
    "            \"word\": self.tokenizer.convert_tokens_to_string(tokens),\n",
    "            \"offsets\": (idx_start, idx_end),\n",
    "        }\n",
    "        return entity_group\n",
    "\n",
    "    def _generate_tagged_entities(\n",
    "        self, entities: np.ndarray, input_ids: np.ndarray, grouped_entities: bool = True\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Generate full list of entities given tagged token predictions and input_ids\"\"\"\n",
    "\n",
    "        score = np.exp(entities) / np.exp(entities).sum(-1, keepdims=True)\n",
    "        labels_idx = score.argmax(axis=-1)\n",
    "\n",
    "        answers = []\n",
    "        entities = []\n",
    "        entity_groups = []\n",
    "        entity_group_disagg = []\n",
    "        # Filter to labels not in `self.ignore_labels`\n",
    "        filtered_labels_idx = [\n",
    "            (idx, label_idx)\n",
    "            for idx, label_idx in enumerate(labels_idx)\n",
    "            if self.model.config.id2label[label_idx] not in [\"O\"]\n",
    "        ]\n",
    "\n",
    "        for idx, label_idx in filtered_labels_idx:\n",
    "            # print(tokenizer.convert_ids_to_tokens(int(input_ids[idx])))\n",
    "            entity = {\n",
    "                \"word\": self.tokenizer.convert_ids_to_tokens(int(input_ids[idx])),\n",
    "                \"score\": score[idx][label_idx].item(),\n",
    "                \"entity\": self.model.config.id2label[label_idx],\n",
    "                \"index\": idx,\n",
    "            }\n",
    "            last_idx, _ = filtered_labels_idx[-1]\n",
    "            if grouped_entities:\n",
    "                if not entity_group_disagg:\n",
    "                    entity_group_disagg += [entity]\n",
    "                    if idx == last_idx:\n",
    "                        entity_groups += [\n",
    "                            self._group_entities(\n",
    "                                entity_group_disagg, idx - len(entity_group_disagg), idx\n",
    "                            )\n",
    "                        ]\n",
    "                    continue\n",
    "\n",
    "                # If the current entity is similar and adjacent to the previous entity, append it to the disaggregated entity group\n",
    "                if (\n",
    "                    entity[\"entity\"] == entity_group_disagg[-1][\"entity\"]\n",
    "                    and entity[\"index\"] == entity_group_disagg[-1][\"index\"] + 1\n",
    "                ):\n",
    "                    entity_group_disagg += [entity]\n",
    "                    # Group the entities at the last entity\n",
    "                    if idx == last_idx:\n",
    "                        entity_groups += [\n",
    "                            self._group_entities(\n",
    "                                entity_group_disagg, idx - len(entity_group_disagg), idx\n",
    "                            )\n",
    "                        ]\n",
    "                # If the current entity is different from the previous entity, aggregate the disaggregated entity group\n",
    "                else:\n",
    "                    entity_groups += [\n",
    "                        self._group_entities(\n",
    "                            entity_group_disagg,\n",
    "                            entity_group_disagg[-1][\"index\"] - len(entity_group_disagg),\n",
    "                            entity_group_disagg[-1][\"index\"],\n",
    "                        )\n",
    "                    ]\n",
    "                    entity_group_disagg = [entity]\n",
    "\n",
    "            entities += [entity]\n",
    "\n",
    "        # Append\n",
    "        if grouped_entities:\n",
    "            answers += [entity_groups]\n",
    "        else:\n",
    "            answers += [entities]\n",
    "\n",
    "        return answers\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "tagger = TransformersTokenTagger.load(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "pred = tagger.predict(text='Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.', mini_batch_size=32)\n",
    "baseline = [[{'entity_group': 'I-ORG',\n",
    "   'score': 0.998292068640391,\n",
    "   'word': 'Novetta Solutions',\n",
    "   'offsets': (0, 3)},\n",
    "  {'entity_group': 'I-PER',\n",
    "   'score': 0.9985582232475281,\n",
    "   'word': 'Albert Einstein',\n",
    "   'offsets': (7, 9)},\n",
    "  {'entity_group': 'I-ORG',\n",
    "   'score': 0.9970489343007406,\n",
    "   'word': 'Novetta Solutions',\n",
    "   'offsets': (14, 17)},\n",
    "  {'entity_group': 'I-PER',\n",
    "   'score': 0.9961656928062439,\n",
    "   'word': 'Wright',\n",
    "   'offsets': (19, 20)},\n",
    "  {'entity_group': 'I-ORG',\n",
    "   'score': 0.9933501183986664,\n",
    "   'word': 'JBF',\n",
    "   'offsets': (25, 27)}]]\n",
    "\n",
    "for base, p in zip(baseline, pred):\n",
    "    test_eq(base['entity_group'], p['entity_group'])\n",
    "    test_close(base['score'], p['score'], 1e-3)\n",
    "    test_eq(base['word'], p['word'])\n",
    "    test_eq(base['offsets'], p['offsets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FlairTokenTagger(AdaptiveModel):\n",
    "    \"\"\"Adaptive Model for Flair's Token Tagger...very basic\n",
    "\n",
    "    Usage:\n",
    "    ```python\n",
    "    >>> tagger = FlairTokenTagger.load(\"flair/chunk-english-fast\")\n",
    "    >>> tagger.predict(text=\"Example text\", mini_batch_size=32)\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **model_name_or_path** - A key string of one of Flair's pre-trained Token tagger Model\n",
    "    \n",
    "    To find a list of available models, see [here](https://huggingface.co/models?filter=flair)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name_or_path: str):\n",
    "        self.tagger = SequenceTagger.load(model_name_or_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading a constructing this tagger\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Flair's pre-trained Token tagger Model\n",
    "        \n",
    "        To find a list of available models, see [here](https://huggingface.co/models?filter=flair)\n",
    "        \"\"\"\n",
    "        tagger = cls(model_name_or_path)\n",
    "        return tagger\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained token tagger model\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Flair tagger\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(text, (Sentence, str)):\n",
    "            text = [text]\n",
    "        if isinstance(text[0], str):\n",
    "            text = [Sentence(s) for s in text]\n",
    "        self.tagger.predict(\n",
    "            sentences=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return text\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "    ):\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-20 18:27:34,432 loading file /root/.flair/models/chunk-english-fast/be3a207f4993dd6d174d5083341a717d371ec16f721358e7a4d72158ebab28a6.a7f897d05c83e618a8235bbb7ddfca5a79d2daefb8a97c776eb73f97dbaea508\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "tagger = FlairTokenTagger.load(\"flair/chunk-english-fast\")\n",
    "preds = tagger.predict(text=\"Example text\", mini_batch_size=32)[0]\n",
    "test_eq(preds.tokens[0].text, 'Example')\n",
    "test_eq(preds.tokens[1].text, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyTokenTagger:\n",
    "    \"\"\"Token level classification models\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> tagger = adaptnlp.EasyTokenTagger()\n",
    "    >>> tagger.tag_text(text=\"text you want to tag\", model_name_or_path=\"ner-ontonotes\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.token_taggers: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "\n",
    "    def tag_text(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        model_name_or_path: Union[str, FlairModelResult, HFModelResult] = \"ner-ontonotes\",\n",
    "        mini_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Tags tokens with labels the token classification models have been trained on\n",
    "\n",
    "        * **text** - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * **model_name_or_path** - The hosted model name key or model path\n",
    "        * **mini_batch_size** - The mini batch size for running inference\n",
    "        * **&ast;&ast;kwargs** - Keyword arguments for Flair's `SequenceTagger.predict()` method\n",
    "        **return** - A list of Flair's `Sentence`'s\n",
    "        \"\"\"\n",
    "        # Load Sequence Tagger Model and Pytorch Module into tagger dict\n",
    "        name = getattr(model_name_or_path, 'name', model_name_or_path)\n",
    "        if not self.token_taggers[name]:\n",
    "            \"\"\"\n",
    "            self.token_taggers[model_name_or_path] = SequenceTagger.load(\n",
    "                model_name_or_path\n",
    "            )\n",
    "            \"\"\"\n",
    "            if risinstance([FlairModelResult, HFModelResult], model_name_or_path):\n",
    "                try:\n",
    "                    self.token_taggers[name] = FlairTokenTagger.load(name)\n",
    "                except:\n",
    "                    self.token_taggers[name] = TransformersTokenTagger.load(name)\n",
    "            elif risinstance([str, Path], model_name_or_path) and (Path(model_name_or_path).exists() and Path(model_name_or_path).is_dir()):\n",
    "                # Load in previously existing model\n",
    "                try:\n",
    "                    self.token_taggers[name] = FlairTokenTagger.load(name)\n",
    "                except:\n",
    "                    self.token_taggers[name] = TransformersTokenTagger.load(name)\n",
    "            else:\n",
    "                _flair_hub = FlairModelHub()\n",
    "                _hf_hub = HFModelHub()\n",
    "                res = _flair_hub.search_model_by_name(name, user_uploaded=True)\n",
    "                if len(res) < 1:\n",
    "                    # No models found\n",
    "                    res = _hf_hub.search_model_by_name(name, user_uploaded=True)\n",
    "                    if len(res) < 1:\n",
    "                        logger.info(\"Not a valid `model_name_or_path` param\")\n",
    "                        return [Sentence('')]\n",
    "                    else:\n",
    "                        res[0].name.replace('flairNLP', 'flair')\n",
    "                        self.token_taggers[res[0].name] = TransformersTokenTagger.load(res[0].name)\n",
    "                        name = res[0].name\n",
    "\n",
    "                else:\n",
    "                    name = res[0].name.replace('flairNLP/', '')\n",
    "                    self.token_taggers[name] = FlairTokenTagger.load(name) # Returning the first should always be the non-fast option\n",
    "                    \n",
    "        tagger = self.token_taggers[name]\n",
    "        return tagger.predict(\n",
    "            text=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def tag_all(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Tags tokens with all labels from all token classification models\n",
    "\n",
    "        * **text** - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * **mini_batch_size** - The mini batch size for running inference\n",
    "        * **&ast;&ast;kwargs** - Keyword arguments for Flair's `SequenceTagger.predict()` method\n",
    "        **return** A list of Flair's `Sentence`'s\n",
    "        \"\"\"\n",
    "        if len(self.token_taggers) == 0:\n",
    "            print(\"No token classification models loaded...\")\n",
    "            return Sentence()\n",
    "        sentences = text\n",
    "        for tagger_name in self.token_taggers.keys():\n",
    "            sentences = self.tag_text(\n",
    "                sentences,\n",
    "                model_name_or_path=tagger_name,\n",
    "                mini_batch_size=mini_batch_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-20 18:30:25,272 --------------------------------------------------------------------------------\n",
      "2021-04-20 18:30:25,273 The model key 'ner-ontonotes' now maps to 'https://huggingface.co/flair/ner-english-ontonotes' on the HuggingFace ModelHub\n",
      "2021-04-20 18:30:25,273  - The most current version of the model is automatically downloaded from there.\n",
      "2021-04-20 18:30:25,274  - (you can alternatively manually download the original model at https://nlp.informatik.hu-berlin.de/resources/models/ner-ontonotes/en-ner-ontonotes-v0.4.pt)\n",
      "2021-04-20 18:30:25,275 --------------------------------------------------------------------------------\n",
      "2021-04-20 18:30:25,295 loading file /root/.flair/models/ner-english-ontonotes/f46dcd14689a594a7dd2a8c9c001a34fd55b02fded2528410913c7e88dbe43d4.1207747bf5ae24291205b6f3e7417c8bedd5c32cacfb5a439f3eff38afda66f7\n",
      "2021-04-20 18:30:31,136 loading file /root/.flair/models/pos-english-fast/36f7923039eed4c66e4275927daaff6cd275997d61d238355fb1fe0338fe10a1.ff87e5b4e47fdb42a0c00237d9506c671db773e0a7932179ace82e584383a1b8\n",
      "2021-04-20 18:30:31,423 loading file /root/.flair/models/pos-english-fast/36f7923039eed4c66e4275927daaff6cd275997d61d238355fb1fe0338fe10a1.ff87e5b4e47fdb42a0c00237d9506c671db773e0a7932179ace82e584383a1b8\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "tagger = EasyTokenTagger()\n",
    "example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. \n",
    "The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.'''\n",
    "sentences = [\"Jack walked through the park on a Sunday.\", \"Sunday was a nice and breezy afternoon.\", \"Jack was going to meet Jill for dinner.\"]\n",
    "tagger.tag_text(text=example_text, model_name_or_path=\"ner-ontonotes\")\n",
    "tagger.tag_text(text=example_text, model_name_or_path=\"pos\")\n",
    "tags = tagger.tag_all(sentences)\n",
    "_types = [[\"PERSON\", \"DATE\"], [\"DATE\"], [\"PERSON\", \"PERSON\"]]\n",
    "\n",
    "for sentence, lbls in zip(tags, _types):\n",
    "    spans = sentence.get_spans()\n",
    "    for span, lbl in zip(spans, lbls):\n",
    "        test_eq(span.tag, lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
