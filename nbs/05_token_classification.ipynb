{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Tagging and Classification\n",
    "> AdaptiveModels for using Transformers and Flair for token tagging and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import test_eq, test_close\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    BertForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    ")\n",
    "\n",
    "from adaptnlp.model import AdaptiveModel, DataLoader\n",
    "from adaptnlp.model_hub import HFModelResult, FlairModelResult, FlairModelHub, HFModelHub\n",
    "\n",
    "from fastai_minima.utils import to_detach, apply, to_device\n",
    "\n",
    "from fastcore.basics import Self, risinstance\n",
    "from fastcore.xtras import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTokenTagger(AdaptiveModel):\n",
    "    \"Adaptive model for Transformer's Token Tagger Model\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizer, # A tokenizer object from Huggingface's transformers (TODO) and tokenizers\n",
    "        model: PreTrainedModel # A transformers token tagger model\n",
    "    ):\n",
    "        # Load up model and tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        super().__init__()\n",
    "\n",
    "        # Sets the internal model\n",
    "        self.set_model(model)\n",
    "\n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls, \n",
    "        model_name_or_path: str # A key string of one of Transformer's pre-trained Token Tagger Model or a `HFModelResult`\n",
    "    ) -> AdaptiveModel:\n",
    "        \"Class method for loading and constructing this tagger\"\n",
    "        if isinstance(model_name_or_path, HFModelResult): model_name_or_path = model_name_or_path.name\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)\n",
    "        tagger = cls(tokenizer, model)\n",
    "        return tagger\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[str], str], # String, list of strings, sentences, or list of sentences to run inference on\n",
    "        mini_batch_size: int = 32, # Mini batch size\n",
    "        grouped_entities: bool = True, # Set True to get whole entity span strings (Default True)\n",
    "        **kwargs, # Optional arguments for the Transformers tagger\n",
    "    ) -> List[List[Dict]]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained token tagger model.\n",
    "        \n",
    "        **Returns**:\n",
    "        A list of lists of tagged entities.\n",
    "        \"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        results: List[Dict] = []\n",
    "\n",
    "        dataset = self._tokenize(text)\n",
    "        dl = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "\n",
    "        logger.info(f'Running prediction on {len(dataset)} text sequences')\n",
    "        logger.info(f'Batch size = {mini_batch_size}')\n",
    "\n",
    "        outputs,_ = super().get_preds(dl=dl)\n",
    "\n",
    "        inputs = apply(to_device, [b for b in dl], device='cpu')\n",
    "        inputs = torch.cat(*inputs)\n",
    "        inputs = apply(Self.numpy(), inputs)\n",
    "\n",
    "        outputs = torch.cat([o['logits'] for o in outputs])\n",
    "        outputs = apply(to_detach, outputs, cpu=True)\n",
    "        outputs = apply(Self.numpy(), outputs)\n",
    "\n",
    "        # Iterate through batch for tagged token predictions\n",
    "        for idx, pred in enumerate(outputs):\n",
    "            entities = pred\n",
    "            input_ids = inputs[idx]\n",
    "            tagged_entities = self._generate_tagged_entities(\n",
    "                entities=entities,\n",
    "                input_ids=input_ids,\n",
    "                grouped_entities=grouped_entities\n",
    "            )\n",
    "            results += tagged_entities\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _tokenize(\n",
    "        self, sentences: Union[List[Sentence], Sentence, List[str], str]\n",
    "    ) -> TensorDataset:\n",
    "        \"\"\" Batch tokenizes text and produces a `TensorDataset` with them \"\"\"\n",
    "\n",
    "        tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "            sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            pad_to_max_length=True,\n",
    "        )\n",
    "\n",
    "        # Bart, XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use token_type_ids\n",
    "        if isinstance(\n",
    "            self.model,\n",
    "            (\n",
    "                BertForSequenceClassification,\n",
    "                XLNetForSequenceClassification,\n",
    "                AlbertForSequenceClassification,\n",
    "            ),\n",
    "        ):\n",
    "            dataset = TensorDataset(\n",
    "                tokenized_text[\"input_ids\"],\n",
    "                tokenized_text[\"attention_mask\"],\n",
    "                tokenized_text[\"token_type_ids\"],\n",
    "            )\n",
    "        else:\n",
    "            dataset = TensorDataset(\n",
    "                tokenized_text[\"input_ids\"], tokenized_text[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    # `_group_entites` and `_generate_tagged_entities` modified from pipeline code snippet from Transformers\n",
    "    def _group_entities(\n",
    "        self, entities: List[dict], idx_start: int, idx_end: int\n",
    "    ) -> Dict:\n",
    "        \"\"\"Returns grouped entities\"\"\"\n",
    "        # Get the last entity in the entity group\n",
    "        entity = entities[-1][\"entity\"]\n",
    "        scores = np.mean([entity[\"score\"] for entity in entities])\n",
    "        tokens = [entity[\"word\"] for entity in entities]\n",
    "\n",
    "        entity_group = {\n",
    "            \"entity_group\": entity,\n",
    "            \"score\": np.mean(scores),\n",
    "            \"word\": self.tokenizer.convert_tokens_to_string(tokens),\n",
    "            \"offsets\": (idx_start, idx_end),\n",
    "        }\n",
    "        return entity_group\n",
    "\n",
    "    def _generate_tagged_entities(\n",
    "        self, entities: np.ndarray, input_ids: np.ndarray, grouped_entities: bool = True\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Generate full list of entities given tagged token predictions and input_ids\"\"\"\n",
    "\n",
    "        score = np.exp(entities) / np.exp(entities).sum(-1, keepdims=True)\n",
    "        labels_idx = score.argmax(axis=-1)\n",
    "\n",
    "        answers = []\n",
    "        entities = []\n",
    "        entity_groups = []\n",
    "        entity_group_disagg = []\n",
    "        # Filter to labels not in `self.ignore_labels`\n",
    "        filtered_labels_idx = [\n",
    "            (idx, label_idx)\n",
    "            for idx, label_idx in enumerate(labels_idx)\n",
    "            if self.model.config.id2label[label_idx] not in [\"O\"]\n",
    "        ]\n",
    "\n",
    "        for idx, label_idx in filtered_labels_idx:\n",
    "            # print(tokenizer.convert_ids_to_tokens(int(input_ids[idx])))\n",
    "            entity = {\n",
    "                \"word\": self.tokenizer.convert_ids_to_tokens(int(input_ids[idx])),\n",
    "                \"score\": score[idx][label_idx].item(),\n",
    "                \"entity\": self.model.config.id2label[label_idx],\n",
    "                \"index\": idx,\n",
    "            }\n",
    "            last_idx, _ = filtered_labels_idx[-1]\n",
    "            if grouped_entities:\n",
    "                if not entity_group_disagg:\n",
    "                    entity_group_disagg += [entity]\n",
    "                    if idx == last_idx:\n",
    "                        entity_groups += [\n",
    "                            self._group_entities(\n",
    "                                entity_group_disagg, idx - len(entity_group_disagg), idx\n",
    "                            )\n",
    "                        ]\n",
    "                    continue\n",
    "\n",
    "                # If the current entity is similar and adjacent to the previous entity, append it to the disaggregated entity group\n",
    "                if (\n",
    "                    entity[\"entity\"] == entity_group_disagg[-1][\"entity\"]\n",
    "                    and entity[\"index\"] == entity_group_disagg[-1][\"index\"] + 1\n",
    "                ):\n",
    "                    entity_group_disagg += [entity]\n",
    "                    # Group the entities at the last entity\n",
    "                    if idx == last_idx:\n",
    "                        entity_groups += [\n",
    "                            self._group_entities(\n",
    "                                entity_group_disagg, idx - len(entity_group_disagg), idx\n",
    "                            )\n",
    "                        ]\n",
    "                # If the current entity is different from the previous entity, aggregate the disaggregated entity group\n",
    "                else:\n",
    "                    entity_groups += [\n",
    "                        self._group_entities(\n",
    "                            entity_group_disagg,\n",
    "                            entity_group_disagg[-1][\"index\"] - len(entity_group_disagg),\n",
    "                            entity_group_disagg[-1][\"index\"],\n",
    "                        )\n",
    "                    ]\n",
    "                    entity_group_disagg = [entity]\n",
    "\n",
    "            entities += [entity]\n",
    "\n",
    "        # Append\n",
    "        if grouped_entities:\n",
    "            answers += [entity_groups]\n",
    "        else:\n",
    "            answers += [entities]\n",
    "\n",
    "        return answers\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee548b6a84f4635865250c39e089470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=998.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e59c565f7f4811bf89d7232237e24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7add96621db14252b1793d9fe3af13c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=60.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eab75a90b1c40af8efe902273e96bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1334448817.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2104: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "tagger = TransformersTokenTagger.load(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "pred = tagger.predict(text='Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.', mini_batch_size=32)\n",
    "baseline = [[{'entity_group': 'I-ORG',\n",
    "   'score': 0.998292068640391,\n",
    "   'word': 'Novetta Solutions',\n",
    "   'offsets': (0, 3)},\n",
    "  {'entity_group': 'I-PER',\n",
    "   'score': 0.9985582232475281,\n",
    "   'word': 'Albert Einstein',\n",
    "   'offsets': (7, 9)},\n",
    "  {'entity_group': 'I-ORG',\n",
    "   'score': 0.9970489343007406,\n",
    "   'word': 'Novetta Solutions',\n",
    "   'offsets': (14, 17)},\n",
    "  {'entity_group': 'I-PER',\n",
    "   'score': 0.9961656928062439,\n",
    "   'word': 'Wright',\n",
    "   'offsets': (19, 20)},\n",
    "  {'entity_group': 'I-ORG',\n",
    "   'score': 0.9933501183986664,\n",
    "   'word': 'JBF',\n",
    "   'offsets': (25, 27)}]]\n",
    "\n",
    "for base, p in zip(baseline, pred):\n",
    "    for base_items, p_items in zip(base, p):\n",
    "        test_eq(base_items['entity_group'], p_items['entity_group'])\n",
    "        test_close(base_items['score'], p_items['score'], 1e-3)\n",
    "        test_eq(base_items['word'], p_items['word'])\n",
    "        test_eq(base_items['offsets'], p_items['offsets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TransformersTokenTagger.load\" class=\"doc_header\"><code>TransformersTokenTagger.load</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TransformersTokenTagger.load</code>(**`model_name_or_path`**:`str`)\n",
       "\n",
       "Class method for loading and constructing this tagger\n",
       "\n",
       "**Function Arguments**:\n",
       "* `model_name_or_path` (`str`): A key string of one of Transformer's pre-trained Token Tagger Model or a `HFModelResult`\n",
       "\n",
       "\n",
       "**Usage Examples:**\n",
       "\n",
       "<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>\n",
       "```python\n",
       "#hide\n",
       "tagger = TransformersTokenTagger.load(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
       "pred = tagger.predict(text='Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.', mini_batch_size=32)\n",
       "baseline = [[{'entity_group': 'I-ORG',\n",
       "   'score': 0.998292068640391,\n",
       "   'word': 'Novetta Solutions',\n",
       "   'offsets': (0, 3)},\n",
       "  {'entity_group': 'I-PER',\n",
       "   'score': 0.9985582232475281,\n",
       "   'word': 'Albert Einstein',\n",
       "   'offsets': (7, 9)},\n",
       "  {'entity_group': 'I-ORG',\n",
       "   'score': 0.9970489343007406,\n",
       "   'word': 'Novetta Solutions',\n",
       "   'offsets': (14, 17)},\n",
       "  {'entity_group': 'I-PER',\n",
       "   'score': 0.9961656928062439,\n",
       "   'word': 'Wright',\n",
       "   'offsets': (19, 20)},\n",
       "  {'entity_group': 'I-ORG',\n",
       "   'score': 0.9933501183986664,\n",
       "   'word': 'JBF',\n",
       "   'offsets': (25, 27)}]]\n",
       "\n",
       "for base, p in zip(baseline, pred):\n",
       "    for base_items, p_items in zip(base, p):\n",
       "        test_eq(base_items['entity_group'], p_items['entity_group'])\n",
       "        test_close(base_items['score'], p_items['score'], 1e-3)\n",
       "        test_eq(base_items['word'], p_items['word'])\n",
       "        test_eq(base_items['offsets'], p_items['offsets'])\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TransformersTokenTagger.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TransformersTokenTagger.predict\" class=\"doc_header\"><code>TransformersTokenTagger.predict</code><a href=\"__main__.py#L28\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TransformersTokenTagger.predict</code>(**`text`**:`Union`\\[`List`\\[`str`\\], `str`\\], **`mini_batch_size`**:`int`=*`32`*, **`grouped_entities`**:`bool`=*`True`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Predict method for running inference using the pre-trained token tagger model.\n",
       "\n",
       "**Returns**:\n",
       "A list of lists of tagged entities.\n",
       "\n",
       "**Function Arguments**:\n",
       "* `text` (`Union[List[str]`, `str]`): String, list of strings, sentences, or list of sentences to run inference on\n",
       "* `mini_batch_size` (`int`): Mini batch size\n",
       "* `grouped_entities` (`bool`): Set True to get whole entity span strings (Default True\n",
       "\n",
       "\n",
       "**Usage Examples:**\n",
       "\n",
       "<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>\n",
       "```python\n",
       "#hide\n",
       "tagger = TransformersTokenTagger.load(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
       "pred = tagger.predict(text='Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.', mini_batch_size=32)\n",
       "baseline = [[{'entity_group': 'I-ORG',\n",
       "   'score': 0.998292068640391,\n",
       "   'word': 'Novetta Solutions',\n",
       "   'offsets': (0, 3)},\n",
       "  {'entity_group': 'I-PER',\n",
       "   'score': 0.9985582232475281,\n",
       "   'word': 'Albert Einstein',\n",
       "   'offsets': (7, 9)},\n",
       "  {'entity_group': 'I-ORG',\n",
       "   'score': 0.9970489343007406,\n",
       "   'word': 'Novetta Solutions',\n",
       "   'offsets': (14, 17)},\n",
       "  {'entity_group': 'I-PER',\n",
       "   'score': 0.9961656928062439,\n",
       "   'word': 'Wright',\n",
       "   'offsets': (19, 20)},\n",
       "  {'entity_group': 'I-ORG',\n",
       "   'score': 0.9933501183986664,\n",
       "   'word': 'JBF',\n",
       "   'offsets': (25, 27)}]]\n",
       "\n",
       "for base, p in zip(baseline, pred):\n",
       "    for base_items, p_items in zip(base, p):\n",
       "        test_eq(base_items['entity_group'], p_items['entity_group'])\n",
       "        test_close(base_items['score'], p_items['score'], 1e-3)\n",
       "        test_eq(base_items['word'], p_items['word'])\n",
       "        test_eq(base_items['offsets'], p_items['offsets'])\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TransformersTokenTagger.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FlairTokenTagger(AdaptiveModel):\n",
    "    \"\"\"Adaptive Model for Flair's Token Tagger\n",
    "    \n",
    "    To find a list of available models, see [here](https://huggingface.co/models?filter=flair)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name_or_path: str # A key string of one of Flair's pre-trained Token tagger Model\n",
    "    ):\n",
    "        self.tagger = SequenceTagger.load(model_name_or_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls, \n",
    "        model_name_or_path: str # A key string of one of Flair's pre-trained Token tagger Model\n",
    "    ) -> AdaptiveModel:\n",
    "        \"Class method for loading a constructing this tagger\"\n",
    "        tagger = cls(model_name_or_path)\n",
    "        return tagger\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str], # String, list of strings, sentences, or list of sentences to run inference on\n",
    "        mini_batch_size: int = 32, # Mini batch size\n",
    "        **kwargs, # Optional arguments for the Flair tagger\n",
    "    ) -> List[Sentence]:\n",
    "        \"Predict method for running inference using the pre-trained token tagger model\"\n",
    "\n",
    "        if isinstance(text, (Sentence, str)):\n",
    "            text = [text]\n",
    "        if isinstance(text[0], str):\n",
    "            text = [Sentence(s) for s in text]\n",
    "        self.tagger.predict(\n",
    "            sentences=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return text\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "    ):\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c6e856f86c47f1b5b5b8000f69c615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=75233247.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-07-15 17:15:47,070 loading file /root/.flair/models/chunk-english-fast/be3a207f4993dd6d174d5083341a717d371ec16f721358e7a4d72158ebab28a6.a7f897d05c83e618a8235bbb7ddfca5a79d2daefb8a97c776eb73f97dbaea508\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "tagger = FlairTokenTagger.load(\"flair/chunk-english-fast\")\n",
    "preds = tagger.predict(text=\"Example text\", mini_batch_size=32)[0]\n",
    "test_eq(preds.tokens[0].text, 'Example')\n",
    "test_eq(preds.tokens[1].text, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FlairTokenTagger.load\" class=\"doc_header\"><code>FlairTokenTagger.load</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FlairTokenTagger.load</code>(**`model_name_or_path`**:`str`)\n",
       "\n",
       "Class method for loading a constructing this tagger\n",
       "\n",
       "**Function Arguments**:\n",
       "* `model_name_or_path` (`str`): A key string of one of Flair's pre-trained Token tagger Model\n",
       "\n",
       "\n",
       "**Usage Examples:**\n",
       "\n",
       "<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>\n",
       "```python\n",
       "#hide\n",
       "tagger = FlairTokenTagger.load(\"flair/chunk-english-fast\")\n",
       "preds = tagger.predict(text=\"Example text\", mini_batch_size=32)[0]\n",
       "test_eq(preds.tokens[0].text, 'Example')\n",
       "test_eq(preds.tokens[1].text, 'text')\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FlairTokenTagger.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FlairTokenTagger.predict\" class=\"doc_header\"><code>FlairTokenTagger.predict</code><a href=\"__main__.py#L23\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FlairTokenTagger.predict</code>(**`text`**:`Union`\\[`List`\\[`Sentence`\\], `Sentence`, `List`\\[`str`\\], `str`\\], **`mini_batch_size`**:`int`=*`32`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Predict method for running inference using the pre-trained token tagger model\n",
       "\n",
       "**Function Arguments**:\n",
       "* `text` (`Union[List[Sentence]`, `Sentence`, `List[str]`, `str]`): String, list of strings, sentences, or list of sentences to run inference on\n",
       "* `mini_batch_size` (`int`): Mini batch size\n",
       "* `**kwargs`: Optional arguments for the Flair tagger\n",
       "\n",
       "\n",
       "**Usage Examples:**\n",
       "\n",
       "<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>\n",
       "```python\n",
       "#hide\n",
       "tagger = FlairTokenTagger.load(\"flair/chunk-english-fast\")\n",
       "preds = tagger.predict(text=\"Example text\", mini_batch_size=32)[0]\n",
       "test_eq(preds.tokens[0].text, 'Example')\n",
       "test_eq(preds.tokens[1].text, 'text')\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FlairTokenTagger.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyTokenTagger:\n",
    "    \"Token level classification models\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.token_taggers: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "\n",
    "    def tag_text(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str], # Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        model_name_or_path: Union[str, FlairModelResult, HFModelResult] = \"ner-ontonotes\", # The hosted model name key or model path\n",
    "        mini_batch_size: int = 32, # The mini batch size for running inference\n",
    "        **kwargs, # Keyword arguments for Flair's `SequenceTagger.predict()` method\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Tags tokens with labels the token classification models have been trained on\n",
    "\n",
    "        **Returns**: A list of Flair's `Sentence`'s\n",
    "        \"\"\"\n",
    "        # Load Sequence Tagger Model and Pytorch Module into tagger dict\n",
    "        name = getattr(model_name_or_path, 'name', model_name_or_path)\n",
    "        if not self.token_taggers[name]:\n",
    "            \"\"\"\n",
    "            self.token_taggers[model_name_or_path] = SequenceTagger.load(\n",
    "                model_name_or_path\n",
    "            )\n",
    "            \"\"\"\n",
    "            if risinstance([FlairModelResult, HFModelResult], model_name_or_path):\n",
    "                try:\n",
    "                    self.token_taggers[name] = FlairTokenTagger.load(name)\n",
    "                except:\n",
    "                    self.token_taggers[name] = TransformersTokenTagger.load(name)\n",
    "            elif risinstance([str, Path], model_name_or_path) and (Path(model_name_or_path).exists() and Path(model_name_or_path).is_dir()):\n",
    "                # Load in previously existing model\n",
    "                try:\n",
    "                    self.token_taggers[name] = FlairTokenTagger.load(name)\n",
    "                except:\n",
    "                    self.token_taggers[name] = TransformersTokenTagger.load(name)\n",
    "            else:\n",
    "                _flair_hub = FlairModelHub()\n",
    "                _hf_hub = HFModelHub()\n",
    "                res = _flair_hub.search_model_by_name(name, user_uploaded=True)\n",
    "                if len(res) < 1:\n",
    "                    # No models found\n",
    "                    res = _hf_hub.search_model_by_name(name, user_uploaded=True)\n",
    "                    if len(res) < 1:\n",
    "                        logger.info(\"Not a valid `model_name_or_path` param\")\n",
    "                        return [Sentence('')]\n",
    "                    else:\n",
    "                        res[0].name.replace('flairNLP', 'flair')\n",
    "                        self.token_taggers[res[0].name] = TransformersTokenTagger.load(res[0].name)\n",
    "                        name = res[0].name\n",
    "\n",
    "                else:\n",
    "                    name = res[0].name.replace('flairNLP/', '')\n",
    "                    self.token_taggers[name] = FlairTokenTagger.load(name) # Returning the first should always be the non-fast option\n",
    "                    \n",
    "        tagger = self.token_taggers[name]\n",
    "        return tagger.predict(\n",
    "            text=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def tag_all(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str], # Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        mini_batch_size: int = 32, # The mini batch size for running inference\n",
    "        **kwargs, # Keyword arguments for Flair's `SequenceTagger.predict()` method\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Tags tokens with all labels from all token classification models\n",
    "\n",
    "        **Returns**: A list of Flair's `Sentence`'s\n",
    "        \"\"\"\n",
    "        if len(self.token_taggers) == 0:\n",
    "            print(\"No token classification models loaded...\")\n",
    "            return Sentence()\n",
    "        sentences = text\n",
    "        for tagger_name in self.token_taggers.keys():\n",
    "            sentences = self.tag_text(\n",
    "                sentences,\n",
    "                model_name_or_path=tagger_name,\n",
    "                mini_batch_size=mini_batch_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-15 17:29:04,020 --------------------------------------------------------------------------------\n",
      "2021-07-15 17:29:04,021 The model key 'ner-ontonotes' now maps to 'https://huggingface.co/flair/ner-english-ontonotes' on the HuggingFace ModelHub\n",
      "2021-07-15 17:29:04,021  - The most current version of the model is automatically downloaded from there.\n",
      "2021-07-15 17:29:04,022  - (you can alternatively manually download the original model at https://nlp.informatik.hu-berlin.de/resources/models/ner-ontonotes/en-ner-ontonotes-v0.4.pt)\n",
      "2021-07-15 17:29:04,022 --------------------------------------------------------------------------------\n",
      "2021-07-15 17:29:04,060 loading file /root/.flair/models/ner-english-ontonotes/f46dcd14689a594a7dd2a8c9c001a34fd55b02fded2528410913c7e88dbe43d4.1207747bf5ae24291205b6f3e7417c8bedd5c32cacfb5a439f3eff38afda66f7\n",
      "2021-07-15 17:29:08,912 loading file /root/.flair/models/pos-english-fast/36f7923039eed4c66e4275927daaff6cd275997d61d238355fb1fe0338fe10a1.ff87e5b4e47fdb42a0c00237d9506c671db773e0a7932179ace82e584383a1b8\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "tagger = EasyTokenTagger()\n",
    "example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. \n",
    "The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.'''\n",
    "sentences = [\"Jack walked through the park on a Sunday.\", \"Sunday was a nice and breezy afternoon.\", \"Jack was going to meet Jill for dinner.\"]\n",
    "text = tagger.tag_text(text=example_text, model_name_or_path=\"ner-ontonotes\")\n",
    "text = tagger.tag_text(text=example_text, model_name_or_path=\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-15 17:29:11,913 loading file /root/.flair/models/pos-english-fast/36f7923039eed4c66e4275927daaff6cd275997d61d238355fb1fe0338fe10a1.ff87e5b4e47fdb42a0c00237d9506c671db773e0a7932179ace82e584383a1b8\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "tags = tagger.tag_all(sentences)\n",
    "_types = [[\"PERSON\", \"DATE\"], [\"DATE\"], [\"PERSON\", \"PERSON\"]]\n",
    "\n",
    "for sentence, lbls in zip(tags, _types):\n",
    "    spans = sentence.get_spans()\n",
    "    for span, lbl in zip(spans, lbls):\n",
    "        test_eq(span.tag, lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EasyTokenTagger.tag_text\" class=\"doc_header\"><code>EasyTokenTagger.tag_text</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EasyTokenTagger.tag_text</code>(**`text`**:`Union`\\[`List`\\[`Sentence`\\], `Sentence`, `List`\\[`str`\\], `str`\\], **`model_name_or_path`**:`Union`\\[`str`, [`FlairModelResult`](/adaptnlpmodel_hub.html#FlairModelResult), [`HFModelResult`](/adaptnlpmodel_hub.html#HFModelResult)\\]=*`'ner-ontonotes'`*, **`mini_batch_size`**:`int`=*`32`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Tags tokens with labels the token classification models have been trained on\n",
       "\n",
       "**Returns**: A list of Flair's `Sentence`'s\n",
       "\n",
       "**Function Arguments**:\n",
       "* `text` (`Union[List[Sentence]`, `Sentence`, `List[str]`, `str]`): Text input, it can be a string or any of Flair's `Sentence` input formats\n",
       "* `model_name_or_path` (`Union[str`, `FlairModelResult`, `HFModelResult]`): The hosted model name key or model path\n",
       "* `mini_batch_size` (`int`): The mini batch size for running inference\n",
       "* `**kwargs`: Keyword arguments for Flair's `SequenceTagger.predict(\n",
       "\n",
       "\n",
       "**Usage Examples:**\n",
       "\n",
       "<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05a_tutorial.token_tagging.ipynb>Source</a></b>\n",
       "```python\n",
       "tagger = EasyTokenTagger()\n",
       "_ = tagger.tag_text(text=example_text, model_name_or_path=\"ner-ontonotes\")\n",
       "_ = tagger.tag_text(text=example_text, model_name_or_path=\"pos\")\n",
       "```\n",
       "<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>\n",
       "```python\n",
       "#hide\n",
       "tagger = EasyTokenTagger()\n",
       "example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. \n",
       "The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.'''\n",
       "sentences = [\"Jack walked through the park on a Sunday.\", \"Sunday was a nice and breezy afternoon.\", \"Jack was going to meet Jill for dinner.\"]\n",
       "text = tagger.tag_text(text=example_text, model_name_or_path=\"ner-ontonotes\")\n",
       "text tagger.tag_text(text=example_text, model_name_or_path=\"pos\")\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EasyTokenTagger.tag_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EasyTokenTagger.tag_all\" class=\"doc_header\"><code>EasyTokenTagger.tag_all</code><a href=\"__main__.py#L64\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EasyTokenTagger.tag_all</code>(**`text`**:`Union`\\[`List`\\[`Sentence`\\], `Sentence`, `List`\\[`str`\\], `str`\\], **`mini_batch_size`**:`int`=*`32`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Tags tokens with all labels from all token classification models\n",
       "\n",
       "**Returns**: A list of Flair's `Sentence`'s\n",
       "\n",
       "**Function Arguments**:\n",
       "* `text` (`Union[List[Sentence]`, `Sentence`, `List[str]`, `str]`): Text input, it can be a string or any of Flair's `Sentence` input formats\n",
       "* `mini_batch_size` (`int`): The mini batch size for running inference\n",
       "* `**kwargs`: Keyword arguments for Flair's `SequenceTagger.predict(\n",
       "\n",
       "\n",
       "**Usage Examples:**\n",
       "\n",
       "<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>\n",
       "```python\n",
       "#hide\n",
       "tagger = EasyTokenTagger()\n",
       "example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. \n",
       "The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.'''\n",
       "sentences = [\"Jack walked through the park on a Sunday.\", \"Sunday was a nice and breezy afternoon.\", \"Jack was going to meet Jill for dinner.\"]\n",
       "text = tagger.tag_text(text=example_text, model_name_or_path=\"ner-ontonotes\")\n",
       "text = tagger.tag_text(text=example_text, model_name_or_path=\"pos\")\n",
       "#hide\n",
       "tags = tagger.tag_all(sentences)\n",
       "_types = [[\"PERSON\", \"DATE\"], [\"DATE\"], [\"PERSON\", \"PERSON\"]]\n",
       "\n",
       "for sentence, lbls in zip(tags, _types):\n",
       "    spans = sentence.get_spans()\n",
       "    for span, lbl in zip(spans, lbls):\n",
       "        test_eq(span.tag, lbl)\n",
       "```\n",
       "<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05a_tutorial.token_tagging.ipynb>Source</a></b>\n",
       "```python\n",
       "tagger = EasyTokenTagger()\n",
       "_ = tagger.tag_text(text=example_text, model_name_or_path=\"ner-ontonotes\")\n",
       "_ = tagger.tag_text(text=example_text, model_name_or_path=\"pos\")\n",
       "sentences = tagger.tag_all(text=example_text)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EasyTokenTagger.tag_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
