{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp inference.token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Tagging and Classification\n",
    "> AdaptiveModels for using Transformers and Flair for token tagging and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import test_eq, test_close\n",
    "from nbverbose.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    BertForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    ")\n",
    "\n",
    "from adaptnlp.result import DetailLevel\n",
    "\n",
    "from adaptnlp.model import AdaptiveModel, DataLoader\n",
    "from adaptnlp.model_hub import HFModelResult, FlairModelResult, FlairModelHub, HFModelHub\n",
    "\n",
    "from fastai.torch_core import to_detach, apply, to_device\n",
    "\n",
    "from fastcore.basics import Self, risinstance, filter_ex, listify\n",
    "from fastcore.xtras import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenClassificationResult:\n",
    "    \"A result class for Token Tagging tasks\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        inputs:str, # Original text inputs\n",
    "        tokenized_inputs:torch.tensor, # Tokenized inputs\n",
    "        tagged_entities:dict\n",
    "    ):\n",
    "        self.inputs = inputs\n",
    "        self.tokenized_inputs = tokenized_inputs\n",
    "        self.tagged_entities = tagged_entities\n",
    "    \n",
    "    def to_dict(\n",
    "        self, \n",
    "        detail_level:DetailLevel=DetailLevel.Low # A level of detail to return\n",
    "    ) -> dict:\n",
    "        \"Convert `self` to a dictionary\"\n",
    "        o = OrderedDict()\n",
    "        o.update({\n",
    "            'tags': self.tagged_entities\n",
    "        })\n",
    "        if detail_level == 'medium' or detail_level == 'high':\n",
    "            o.update({\n",
    "                'inputs':self.inputs,\n",
    "                'tokenized_inputs':self.tokenized_inputs\n",
    "            })\n",
    "        if detail_level != 'high':\n",
    "            try:\n",
    "                for tag in o['tags'][0]:\n",
    "                    tag.pop('offsets')\n",
    "            except:\n",
    "                return o\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TokenClassificationResult.to_dict\" class=\"doc_header\"><code>TokenClassificationResult.to_dict</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TokenClassificationResult.to_dict</code>(**`detail_level`**:`DetailLevel`=*`'low'`*)\n",
       "\n",
       "Convert `self` to a dictionary\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`detail_level`** : *`<class 'fastcore.basics.DetailLevel'>`*, *optional*\t<p>A level of detail to return</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`<class 'dict'>`*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TokenClassificationResult.to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTokenTagger(AdaptiveModel):\n",
    "    \"Adaptive model for Transformer's Token Tagger Model\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizer, # A tokenizer object from Huggingface's transformers (TODO) and tokenizers \n",
    "        model: PreTrainedModel # A transformers token tagger model\n",
    "    ):\n",
    "        # Load up model and tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        super().__init__()\n",
    "\n",
    "        # Sets the internal model\n",
    "        self.set_model(model)\n",
    "\n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls, \n",
    "        model_name_or_path: str # A key string of one of Transformer's pre-trained Token Tagger Model or a `HFModelResult`\n",
    "    ) -> AdaptiveModel:\n",
    "        \"Class method for loading and constructing this tagger\"\n",
    "        if isinstance(model_name_or_path, HFModelResult): model_name_or_path = model_name_or_path.name\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)\n",
    "        tagger = cls(tokenizer, model)\n",
    "        return tagger\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[str], str], # Sentences to run inference on\n",
    "        mini_batch_size: int = 32, # Mini batch size\n",
    "        grouped_entities: bool = True, # Return whole entity span strings\n",
    "        detail_level:DetailLevel = DetailLevel.Low, # A level of detail to return\n",
    "        **kwargs, # Optional arguments for the Transformers tagger\n",
    "    ) -> List[List[Dict]]: # Returns a list of lists of tagged entities\n",
    "        \"Predict method for running inference using the pre-trained token tagger model\"\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        results: List[Dict] = []\n",
    "\n",
    "        dataset = self._tokenize(text)\n",
    "        dl = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "\n",
    "        logger.info(f'Running prediction on {len(dataset)} text sequences')\n",
    "        logger.info(f'Batch size = {mini_batch_size}')\n",
    "\n",
    "        outputs,_ = super().get_preds(dl=dl)\n",
    "\n",
    "        inputs = apply(to_device, [b for b in dl], device='cpu')\n",
    "        inputs = torch.cat(*inputs)\n",
    "        inputs = apply(Self.numpy(), inputs)\n",
    "\n",
    "        outputs = torch.cat([o['logits'] for o in outputs])\n",
    "        outputs = apply(to_detach, outputs, cpu=True)\n",
    "        outputs = apply(Self.numpy(), outputs)\n",
    "\n",
    "        # Iterate through batch for tagged token predictions\n",
    "        for idx, pred in enumerate(outputs):\n",
    "            entities = pred\n",
    "            input_ids = inputs[idx]\n",
    "            tagged_entities = self._generate_tagged_entities(\n",
    "                entities=entities,\n",
    "                input_ids=input_ids,\n",
    "                grouped_entities=grouped_entities\n",
    "            )\n",
    "            results += tagged_entities\n",
    "            \n",
    "        results = TokenClassificationResult(text, inputs, results)\n",
    "\n",
    "        return results.to_dict(detail_level) if detail_level is not None else detail_level\n",
    "\n",
    "    def _tokenize(\n",
    "        self, sentences: Union[List[Sentence], Sentence, List[str], str]\n",
    "    ) -> TensorDataset:\n",
    "        \"Batch tokenizes text and produces a `TensorDataset` with them\"\n",
    "\n",
    "        tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "            sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=None,\n",
    "        )\n",
    "\n",
    "        # Bart, XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use token_type_ids\n",
    "        if isinstance(\n",
    "            self.model,\n",
    "            (\n",
    "                BertForSequenceClassification,\n",
    "                XLNetForSequenceClassification,\n",
    "                AlbertForSequenceClassification,\n",
    "            ),\n",
    "        ):\n",
    "            dataset = TensorDataset(\n",
    "                tokenized_text[\"input_ids\"],\n",
    "                tokenized_text[\"attention_mask\"],\n",
    "                tokenized_text[\"token_type_ids\"],\n",
    "            )\n",
    "        else:\n",
    "            dataset = TensorDataset(\n",
    "                tokenized_text[\"input_ids\"], tokenized_text[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    # `_group_entites` and `_generate_tagged_entities` modified from pipeline code snippet from Transformers\n",
    "    def _group_entities(\n",
    "        self, entities: List[dict], idx_start: int, idx_end: int\n",
    "    ) -> Dict:\n",
    "        \"\"\"Returns grouped entities\"\"\"\n",
    "        # Get the last entity in the entity group\n",
    "        entity = entities[-1][\"entity\"]\n",
    "        scores = np.mean([entity[\"score\"] for entity in entities])\n",
    "        tokens = [entity[\"word\"] for entity in entities]\n",
    "\n",
    "        entity_group = {\n",
    "            \"entity\": entity,\n",
    "            \"score\": np.mean(scores),\n",
    "            \"word\": self.tokenizer.convert_tokens_to_string(tokens),\n",
    "            \"offsets\": (idx_start, idx_end),\n",
    "        }\n",
    "        return entity_group\n",
    "\n",
    "    def _generate_tagged_entities(\n",
    "        self, entities: np.ndarray, input_ids: np.ndarray, grouped_entities: bool = True\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Generate full list of entities given tagged token predictions and input_ids\"\"\"\n",
    "\n",
    "        score = np.exp(entities) / np.exp(entities).sum(-1, keepdims=True)\n",
    "        labels_idx = score.argmax(axis=-1)\n",
    "\n",
    "        answers = []\n",
    "        entities = []\n",
    "        entity_groups = []\n",
    "        entity_group_disagg = []\n",
    "        # Filter to labels not in `self.ignore_labels`\n",
    "        filtered_labels_idx = [\n",
    "            (idx, label_idx)\n",
    "            for idx, label_idx in enumerate(labels_idx)\n",
    "            if self.model.config.id2label[label_idx] not in [\"O\"]\n",
    "        ]\n",
    "\n",
    "        for idx, label_idx in filtered_labels_idx:\n",
    "            # print(tokenizer.convert_ids_to_tokens(int(input_ids[idx])))\n",
    "            entity = {\n",
    "                \"word\": self.tokenizer.convert_ids_to_tokens(int(input_ids[idx])),\n",
    "                \"score\": score[idx][label_idx].item(),\n",
    "                \"entity\": self.model.config.id2label[label_idx],\n",
    "                \"index\": idx,\n",
    "            }\n",
    "            last_idx, _ = filtered_labels_idx[-1]\n",
    "            if grouped_entities:\n",
    "                if not entity_group_disagg:\n",
    "                    entity_group_disagg += [entity]\n",
    "                    if idx == last_idx:\n",
    "                        entity_groups += [\n",
    "                            self._group_entities(\n",
    "                                entity_group_disagg, idx - len(entity_group_disagg), idx\n",
    "                            )\n",
    "                        ]\n",
    "                    continue\n",
    "\n",
    "                # If the current entity is similar and adjacent to the previous entity, append it to the disaggregated entity group\n",
    "                if (\n",
    "                    entity[\"entity\"] == entity_group_disagg[-1][\"entity\"]\n",
    "                    and entity[\"index\"] == entity_group_disagg[-1][\"index\"] + 1\n",
    "                ):\n",
    "                    entity_group_disagg += [entity]\n",
    "                    # Group the entities at the last entity\n",
    "                    if idx == last_idx:\n",
    "                        entity_groups += [\n",
    "                            self._group_entities(\n",
    "                                entity_group_disagg, idx - len(entity_group_disagg), idx\n",
    "                            )\n",
    "                        ]\n",
    "                # If the current entity is different from the previous entity, aggregate the disaggregated entity group\n",
    "                else:\n",
    "                    entity_groups += [\n",
    "                        self._group_entities(\n",
    "                            entity_group_disagg,\n",
    "                            entity_group_disagg[-1][\"index\"] - len(entity_group_disagg),\n",
    "                            entity_group_disagg[-1][\"index\"],\n",
    "                        )\n",
    "                    ]\n",
    "                    entity_group_disagg = [entity]\n",
    "\n",
    "            entities += [entity]\n",
    "\n",
    "        # Append\n",
    "        if grouped_entities:\n",
    "            answers += [entity_groups]\n",
    "        else:\n",
    "            answers += [entities]\n",
    "\n",
    "        return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "tagger = TransformersTokenTagger.load(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "pred = tagger.predict(text='Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.', mini_batch_size=32)\n",
    "baseline = [[{'entity': 'I-ORG',\n",
    "   'score': 0.998292068640391,\n",
    "   'word': 'Novetta Solutions'},\n",
    "  {'entity': 'I-PER',\n",
    "   'score': 0.9985582232475281,\n",
    "   'word': 'Albert Einstein'},\n",
    "  {'entity': 'I-ORG',\n",
    "   'score': 0.9970489343007406,\n",
    "   'word': 'Novetta Solutions'},\n",
    "  {'entity': 'I-PER',\n",
    "   'score': 0.9961656928062439,\n",
    "   'word': 'Wright'},\n",
    "  {'entity': 'I-ORG',\n",
    "   'score': 0.9933501183986664,\n",
    "   'word': 'JBF'}]]\n",
    "\n",
    "for base, p in zip(baseline, pred['tags']):\n",
    "    for base_items, p_items in zip(base, p):\n",
    "        test_eq(base_items['entity'], p_items['entity'])\n",
    "        test_close(base_items['score'], p_items['score'], 1e-3)\n",
    "        test_eq(base_items['word'], p_items['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TransformersTokenTagger.load\" class=\"doc_header\"><code>TransformersTokenTagger.load</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TransformersTokenTagger.load</code>(**`model_name_or_path`**:`str`)\n",
       "\n",
       "Class method for loading and constructing this tagger\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`model_name_or_path`** : *`<class 'str'>`*\t<p>A key string of one of Transformer's pre-trained Token Tagger Model or a `HFModelResult`</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`<class 'adaptnlp.model.AdaptiveModel'>`*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TransformersTokenTagger.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TransformersTokenTagger.predict\" class=\"doc_header\"><code>TransformersTokenTagger.predict</code><a href=\"__main__.py#L28\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TransformersTokenTagger.predict</code>(**`text`**:`Union`\\[`List`\\[`str`\\], `str`\\], **`mini_batch_size`**:`int`=*`32`*, **`grouped_entities`**:`bool`=*`True`*, **`detail_level`**:`DetailLevel`=*`'low'`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Predict method for running inference using the pre-trained token tagger model\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`text`** : *`typing.Union[typing.List[str], str]`*\t<p>Sentences to run inference on</p>\n",
       "\n",
       "\n",
       " - **`mini_batch_size`** : *`<class 'int'>`*, *optional*\t<p>Mini batch size</p>\n",
       "\n",
       "\n",
       " - **`grouped_entities`** : *`<class 'bool'>`*, *optional*\t<p>Return whole entity span strings</p>\n",
       "\n",
       "\n",
       " - **`detail_level`** : *`<class 'fastcore.basics.DetailLevel'>`*, *optional*\t<p>A level of detail to return</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`typing.List[typing.List[typing.Dict]]`*\t<p>Returns a list of lists of tagged entities</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TransformersTokenTagger.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FlairTokenTagger(AdaptiveModel):\n",
    "    \"Adaptive Model for Flair's Token Tagger\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name_or_path: str # A key string of one of Flair's pre-trained Token tagger Model, [link](https://huggingface.co/models?filter=flair)\n",
    "    ):\n",
    "        self.tagger = SequenceTagger.load(model_name_or_path)\n",
    "        self.splitter = SegtokSentenceSplitter()\n",
    "\n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls, \n",
    "        model_name_or_path: str # A key string of one of Flair's pre-trained Token tagger Model, [link](https://huggingface.co/models?filter=flair)\n",
    "    ) -> AdaptiveModel:\n",
    "        \"Class method for loading a constructing this tagger\"\n",
    "        tagger = cls(model_name_or_path)\n",
    "        return tagger\n",
    "    \n",
    "    def _split_articles(self, tagged_articles):\n",
    "        \"Takes merged NER results and splits them back out into their original articles\"\n",
    "        all_payloads = []\n",
    "        for article in tagged_articles:\n",
    "            payload = [sentence.to_dict(tag_type=self.tagger.tag_type) for sentence in article]\n",
    "            curr_pos = 0\n",
    "            new_payload = {}\n",
    "            new_payload['entities'] = []\n",
    "            new_payload['labels'] = []\n",
    "            text = ''\n",
    "            for i, pay in enumerate(payload):\n",
    "                entities = pay['entities']\n",
    "                for e in entities:\n",
    "                    labels = e['labels']\n",
    "                    d = labels[0].to_dict()\n",
    "                    e['value'] = d['value']\n",
    "                    e['confidence'] = d['confidence']\n",
    "                \n",
    "                if i > 0:\n",
    "                    for e in entities:\n",
    "                        # Get back the original positions\n",
    "                        e['start_pos'] += curr_length\n",
    "                        e['end_pos'] += curr_length\n",
    "                \n",
    "                text += pay['text'] + ' '\n",
    "                curr_length = len(text)\n",
    "                new_payload['entities'] += entities\n",
    "                if len(entities) > 0:\n",
    "                    new_payload['labels'].append(entities)\n",
    "            new_payload['text'] = text\n",
    "            all_payloads.append(new_payload)\n",
    "        return all_payloads\n",
    "    \n",
    "    def decode_articles(\n",
    "        self,\n",
    "        original_text: Union[List[str], str], # Original text inference was run on\n",
    "        sentences: List[Sentence] # Sentences inference was run on\n",
    "    ) -> List[dict]:\n",
    "        \"Decodes `text` back into articles and extracts entities\"\n",
    "        if not isinstance(original_text, list):\n",
    "            original_text = [original_text]\n",
    "        for i,sentence in enumerate(original_text):\n",
    "            if isinstance(sentence, Sentence):\n",
    "                text[i] = sentence.to_original_text()\n",
    "        tagged_articles = [[] for _ in range(len(original_text))]\n",
    "        for sentence in sentences:\n",
    "            for i, article in enumerate(original_text):\n",
    "                if sentence.to_original_text() in article:\n",
    "                    tagged_articles[i].append(sentence)\n",
    "        return self._split_articles(tagged_articles)\n",
    "            \n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str], # Sentences to run inference on\n",
    "        mini_batch_size: int = 32, # Mini batch size\n",
    "        raw:bool = False, # Whether to return a list of raw Sentences\n",
    "        **kwargs, # Optional arguments for the Flair tagger\n",
    "    ) -> List[Sentence]: # A list of predicted sentences\n",
    "        \"Predict method for running inference using the pre-trained token tagger model\"\n",
    "        if not isinstance(text, list):\n",
    "            text = [text]\n",
    "        for i,sentence in enumerate(text):\n",
    "            if isinstance(sentence, Sentence):\n",
    "                text[i] = sentence.to_original_text()\n",
    "        tagged_articles = [[] for _ in range(len(text))]\n",
    "        t = '\\n'.join(text)\n",
    "        sentences = self.splitter.split(t)\n",
    "        self.tagger.predict(sentences, mini_batch_size=mini_batch_size, **kwargs)\n",
    "        \n",
    "        if not raw:\n",
    "            return self.decode_articles(text, sentences)\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-05 17:07:44,424 loading file /root/.flair/models/chunk-english-fast/be3a207f4993dd6d174d5083341a717d371ec16f721358e7a4d72158ebab28a6.a7f897d05c83e618a8235bbb7ddfca5a79d2daefb8a97c776eb73f97dbaea508\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "tagger = FlairTokenTagger.load(\"flair/chunk-english-fast\")\n",
    "text = ['Novetta Solutions is the best.', 'Albert Einstein used to be employed at Novetta Solutions.', 'The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.']\n",
    "preds = tagger.predict(text=text, mini_batch_size=32)\n",
    "preds[0]['entities'][0]['text'] = 'Novetta Solutions'\n",
    "preds[0]['entities'][0]['value'] = 'NP'\n",
    "\n",
    "# Make sure we calculated the positioning right\n",
    "random_entity = preds[1]['entities'][3]\n",
    "entity_name = random_entity['text']\n",
    "start_pos, end_pos = random_entity['start_pos'], random_entity['end_pos']\n",
    "test_eq(entity_name, text[1][start_pos:end_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FlairTokenTagger.load\" class=\"doc_header\"><code>FlairTokenTagger.load</code><a href=\"__main__.py#L12\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FlairTokenTagger.load</code>(**`model_name_or_path`**:`str`)\n",
       "\n",
       "Class method for loading a constructing this tagger\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`model_name_or_path`** : *`<class 'str'>`*\t<p>A key string of one of Flair's pre-trained Token tagger Model, [link](https://huggingface.co/models?filter=flair)</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`<class 'adaptnlp.model.AdaptiveModel'>`*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FlairTokenTagger.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FlairTokenTagger.predict\" class=\"doc_header\"><code>FlairTokenTagger.predict</code><a href=\"__main__.py#L72\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FlairTokenTagger.predict</code>(**`text`**:`Union`\\[`List`\\[`Sentence`\\], `Sentence`, `List`\\[`str`\\], `str`\\], **`mini_batch_size`**:`int`=*`32`*, **`raw`**:`bool`=*`False`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Predict method for running inference using the pre-trained token tagger model\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`text`** : *`typing.Union[typing.List[flair.data.Sentence], flair.data.Sentence, typing.List[str], str]`*\t<p>Sentences to run inference on</p>\n",
       "\n",
       "\n",
       " - **`mini_batch_size`** : *`<class 'int'>`*, *optional*\t<p>Mini batch size</p>\n",
       "\n",
       "\n",
       " - **`raw`** : *`<class 'bool'>`*, *optional*\t<p>Whether to return a list of raw Sentences</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`typing.List[flair.data.Sentence]`*\t<p>A list of predicted sentences</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FlairTokenTagger.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyTokenTagger:\n",
    "    \"Token level classification models\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.token_taggers: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "\n",
    "    def tag_text(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str], # Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        model_name_or_path: Union[str, FlairModelResult, HFModelResult] = \"ner-ontonotes\", # The hosted model name key or model path\n",
    "        mini_batch_size: int = 32, # The mini batch size for running inference\n",
    "        detail_level:DetailLevel = DetailLevel.Low, # The level fo detail to return on a TransformerTagger\n",
    "        **kwargs, # Keyword arguments for Flair's `SequenceTagger.predict()` method\n",
    "    ) -> List[Sentence]: # A list of Flair's `Sentence`'s\n",
    "        \"Tags tokens with labels the token classification models have been trained on\"\n",
    "        # Load Sequence Tagger Model and Pytorch Module into tagger dict\n",
    "        name = getattr(model_name_or_path, 'name', model_name_or_path)\n",
    "        if not self.token_taggers[name]:\n",
    "            \"\"\"\n",
    "            self.token_taggers[model_name_or_path] = SequenceTagger.load(\n",
    "                model_name_or_path\n",
    "            )\n",
    "            \"\"\"\n",
    "            if risinstance([FlairModelResult, HFModelResult], model_name_or_path):\n",
    "                try:\n",
    "                    self.token_taggers[name] = FlairTokenTagger.load(name)\n",
    "                except:\n",
    "                    self.token_taggers[name] = TransformersTokenTagger.load(name)\n",
    "            elif risinstance([str, Path], model_name_or_path) and (Path(model_name_or_path).exists() and Path(model_name_or_path).is_dir()):\n",
    "                # Load in previously existing model\n",
    "                try:\n",
    "                    self.token_taggers[name] = FlairTokenTagger.load(name)\n",
    "                except:\n",
    "                    self.token_taggers[name] = TransformersTokenTagger.load(name)\n",
    "            else:\n",
    "                _flair_hub = FlairModelHub()\n",
    "                _hf_hub = HFModelHub()\n",
    "                res = _flair_hub.search_model_by_name(name, user_uploaded=True)\n",
    "                if len(res) < 1:\n",
    "                    # No models found\n",
    "                    res = _hf_hub.search_model_by_name(name, user_uploaded=True)\n",
    "                    if len(res) < 1:\n",
    "                        logger.info(\"Not a valid `model_name_or_path` param\")\n",
    "                        return [Sentence('')]\n",
    "                    else:\n",
    "                        res[0].name.replace('flairNLP', 'flair')\n",
    "                        self.token_taggers[res[0].name] = TransformersTokenTagger.load(res[0].name)\n",
    "                        name = res[0].name\n",
    "\n",
    "                else:\n",
    "                    name = res[0].name.replace('flairNLP/', '')\n",
    "                    self.token_taggers[name] = FlairTokenTagger.load(name) # Returning the first should always be the non-fast option\n",
    "                    \n",
    "        tagger = self.token_taggers[name]\n",
    "        if isinstance(tagger, TransformersTokenTagger):\n",
    "            return tagger.predict(\n",
    "                text=text,\n",
    "                mini_batch_size=mini_batch_size,\n",
    "                **kwargs\n",
    "            )\n",
    "        else:\n",
    "            return tagger.predict(\n",
    "                text=text,\n",
    "                mini_batch_size=mini_batch_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "    def tag_all(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str], # Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        mini_batch_size: int = 32, # The mini batch size for running inference\n",
    "        detail_level:DetailLevel = DetailLevel.Low, # The level of detail for a TransformersTagger to return\n",
    "        **kwargs, # Keyword arguments for Flair's `SequenceTagger.predict()` method\n",
    "    ) -> List[dict]: # A dictionary of Token Tagging results\n",
    "        \"Tags tokens with all labels from all token classification models\"\n",
    "        if len(self.token_taggers) == 0:\n",
    "            print(\"No token classification models loaded...\")\n",
    "            return Sentence()\n",
    "        if not isinstance(text, list):\n",
    "            text = [text]\n",
    "        results = []\n",
    "        all_models = [o for o in list(self.token_taggers.keys()) if self.token_taggers[o]]\n",
    "        for tagger_name in all_models:\n",
    "            results.append(\n",
    "                self.tag_text(\n",
    "                    text,\n",
    "                    model_name_or_path=tagger_name,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            )\n",
    "        tag_results = [{'entities':[], 'labels':[], 'text':None} for _ in range(len(results[0]))]\n",
    "        for tag in results:\n",
    "            for i, inp in enumerate(tag):\n",
    "                if not tag_results[i]['text']:\n",
    "                    tag_results[i]['text'] = inp['text']\n",
    "                if 'entities' in inp:\n",
    "                    if len(inp['entities']) > 0:\n",
    "                        tag_results[i]['entities'] += inp['entities']\n",
    "                if 'labels' in inp:\n",
    "                    if len(inp['labels']) > 0:\n",
    "                        tag_results[i]['labels'] += inp['labels']\n",
    "                    \n",
    "        all_results = []\n",
    "        for result in tag_results:\n",
    "            res = []\n",
    "            for entity in result['entities']:\n",
    "                unique_entities = []\n",
    "                new_entity = {\n",
    "                    'text':entity['text'], \n",
    "                    'start_pos':entity['start_pos'], \n",
    "                    'end_pos':entity['end_pos']}\n",
    "\n",
    "                if new_entity not in unique_entities:\n",
    "                    unique_entities.append(new_entity)\n",
    "                if len(unique_entities) > 0:\n",
    "                    merged = []\n",
    "                    for entity in unique_entities:\n",
    "                        instances = filter_ex(\n",
    "                                result['entities'],\n",
    "                                lambda x: (\n",
    "                                    x['text'] == entity['text'] and \\\n",
    "                                    x['start_pos'] == entity['start_pos'] and \\\n",
    "                                    x['end_pos'] == entity['end_pos']\n",
    "                                )\n",
    "                            )\n",
    "                        attrs = ('labels','value','confidence')\n",
    "                        for attr in attrs: \n",
    "                            entity[attr] = []\n",
    "                        for instance in instances:\n",
    "                            for attr in attrs:\n",
    "                                if attr in instance.keys():\n",
    "                                    entity[attr] += listify(instance[attr])\n",
    "                    merged = sorted(unique_entities, key=lambda x: x['start_pos'])\n",
    "                    res += merged\n",
    "            all_results.append(res)\n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-05 17:37:10,219 loading file /root/.flair/models/ner-english-ontonotes-fast/0d55dd3b912da9cf26e003035a0c269a0e9ab222f0be1e48a3bbba3a58c0fed0.c9907cd5fde3ce84b71a4172e7ca03841cd81ab71d13eb68aa08b259f57c00b6\n",
      "2021-10-05 17:37:14,860 loading file /root/.flair/models/pos-english-fast/36f7923039eed4c66e4275927daaff6cd275997d61d238355fb1fe0338fe10a1.ff87e5b4e47fdb42a0c00237d9506c671db773e0a7932179ace82e584383a1b8\n"
     ]
    }
   ],
   "source": [
    "tagger = EasyTokenTagger()\n",
    "sentences = [\"Jack walked through the park on a Sunday.\", \"Sunday was a nice and breezy afternoon.\", \"Jack was going to meet Jill for dinner.\"]\n",
    "tagger.tag_text(text=sentences, model_name_or_path='flair/ner-english-ontonotes')\n",
    "tagger.tag_text(text=sentences, model_name_or_path=\"flair/pos\")\n",
    "results = tagger.tag_all(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(len(results), 3)\n",
    "test_eq(results[0][0]['text'], 'Jack')\n",
    "test_eq(results[1][0]['text'], 'Sunday')\n",
    "test_eq(len(results[1][0]['labels']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "t = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. \n",
    "The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.'''\n",
    "results = tagger.tag_all(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EasyTokenTagger.tag_text\" class=\"doc_header\"><code>EasyTokenTagger.tag_text</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EasyTokenTagger.tag_text</code>(**`text`**:`Union`\\[`List`\\[`Sentence`\\], `Sentence`, `List`\\[`str`\\], `str`\\], **`model_name_or_path`**:`Union`\\[`str`, [`FlairModelResult`](/adaptnlp/model_hub.html#FlairModelResult), [`HFModelResult`](/adaptnlp/model_hub.html#HFModelResult)\\]=*`'ner-ontonotes'`*, **`mini_batch_size`**:`int`=*`32`*, **`detail_level`**:`DetailLevel`=*`'low'`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Tags tokens with labels the token classification models have been trained on\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`text`** : *`typing.Union[typing.List[flair.data.Sentence], flair.data.Sentence, typing.List[str], str]`*\t<p>Text input, it can be a string or any of Flair's `Sentence` input formats</p>\n",
       "\n",
       "\n",
       " - **`model_name_or_path`** : *`typing.Union[str, adaptnlp.model_hub.FlairModelResult, adaptnlp.model_hub.HFModelResult]`*, *optional*\t<p>The hosted model name key or model path</p>\n",
       "\n",
       "\n",
       " - **`mini_batch_size`** : *`<class 'int'>`*, *optional*\t<p>The mini batch size for running inference</p>\n",
       "\n",
       "\n",
       " - **`detail_level`** : *`<class 'fastcore.basics.DetailLevel'>`*, *optional*\t<p>The level fo detail to return on a TransformerTagger</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`typing.List[flair.data.Sentence]`*\t<p>A list of Flair's `Sentence`'s</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EasyTokenTagger.tag_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EasyTokenTagger.tag_all\" class=\"doc_header\"><code>EasyTokenTagger.tag_all</code><a href=\"__main__.py#L69\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EasyTokenTagger.tag_all</code>(**`text`**:`Union`\\[`List`\\[`Sentence`\\], `Sentence`, `List`\\[`str`\\], `str`\\], **`mini_batch_size`**:`int`=*`32`*, **`detail_level`**:`DetailLevel`=*`'low'`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Tags tokens with all labels from all token classification models\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`text`** : *`typing.Union[typing.List[flair.data.Sentence], flair.data.Sentence, typing.List[str], str]`*\t<p>Text input, it can be a string or any of Flair's `Sentence` input formats</p>\n",
       "\n",
       "\n",
       " - **`mini_batch_size`** : *`<class 'int'>`*, *optional*\t<p>The mini batch size for running inference</p>\n",
       "\n",
       "\n",
       " - **`detail_level`** : *`<class 'fastcore.basics.DetailLevel'>`*, *optional*\t<p>The level of detail for a TransformersTagger to return</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`typing.List[dict]`*\t<p>A dictionary of Token Tagging results</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EasyTokenTagger.tag_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
