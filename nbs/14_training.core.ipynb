{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1240551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training.core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59802bb2",
   "metadata": {},
   "source": [
    "# Training Foundations\n",
    "> Basic classes and helpers for modularized training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c35ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8da528",
   "metadata": {},
   "source": [
    "## Label Functions\n",
    "\n",
    "Functions that help extract a label from an item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2e38318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.xtras import Path, range_of # pathlib `Path` with extra bits\n",
    "from fastcore.foundation import mask2idxs, L\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "from fastai.learner import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.metrics import *\n",
    "from fastai.losses import *\n",
    "from fastai.optimizer import *\n",
    "## Note: Eventually get more specific about this\n",
    "\n",
    "from fastai.data.core import DataLoaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import default_data_collator, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2590af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ParentLabeller:\n",
    "    \"\"\"\n",
    "    Extracts class based on filename's parent at `level`\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        level=1 # The level up from `fname` to find the label\n",
    "    ):\n",
    "        self.level = level\n",
    "        \n",
    "    def __call__(self, o:Path): return self._do_level(o, self.level)\n",
    "    \n",
    "    def _do_level(self, o:Path, level:int):\n",
    "        \"Goes down one level on parent\"\n",
    "        def _inner(a): return a.parent\n",
    "        if level == 1: return o.parent.name\n",
    "        else: return self._do_level(_inner(o), level - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path = Path('a/b/c/d/text.txt')\n",
    "get_p = ParentLabeller()\n",
    "test_eq('d', get_p(path))\n",
    "get_p.level = 2\n",
    "test_eq('c', get_p(path))\n",
    "get_p.level = 3\n",
    "test_eq('b', get_p(path))\n",
    "get_p.level = 4\n",
    "test_eq('a', get_p(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb32ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ColReader:\n",
    "    \"\"\"\n",
    "    Reads `cols` in `row` with potential `pref` and `suff`\n",
    "    Based on the fastai class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cols, # Some column names to use\n",
    "        pref:str='', # A prefix\n",
    "        suff:str='', # A suffix\n",
    "        label_delim:str=None, # A label delimiter\n",
    "    ):\n",
    "        self.pref = str(pref) + os.path.sep if isinstance(pref, Path) else pref\n",
    "        self.suff, self.label_delim = suff, label_delim\n",
    "        self.cols = L(cols)\n",
    "    \n",
    "    def _do_one(self, r, c):\n",
    "        o = r[c] if isinstance(c,int) else r[c] if c=='name' or c=='cat' else getattr(r,c)\n",
    "        if len(self.pref)==0 and len(self.suff)==0 and self.label_delim is None: return o\n",
    "        if self.label_delim is None: return f'{self.pref}{o}{self.suff}'\n",
    "        else: return o.split(self.label_delim) if len(o)>0 else []\n",
    "    \n",
    "    def __call__(self, o):\n",
    "        if len(self.cols) == 1: return self._do_one(o, self.cols[0])\n",
    "        return L(self._do_one(o,c) for c in self.cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([[0, 'a'], [1, 'b'], [2, 'c']], columns=['number', 'letter'])\n",
    "num_reader = ColReader('number')\n",
    "let_reader = ColReader('letter')\n",
    "\n",
    "test_eq(list(num_reader(df)), [0,1,2])\n",
    "test_eq(list(let_reader(df)), ['a', 'b', 'c'])\n",
    "\n",
    "# Test we will return two lists\n",
    "reader = ColReader(['number', 'letter'])\n",
    "n,l = reader(df)\n",
    "test_eq(list(n), [0,1,2])\n",
    "test_eq(list(l), ['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Categorize:\n",
    "    \"\"\"\n",
    "    Collection of categories with reverse mapping in `o2i`\n",
    "    Based on the fastai class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        names, # An interable collection of items to create a vocab from\n",
    "        sort=True # Whether to make the items sorted\n",
    "    ):\n",
    "        names = L(names)\n",
    "        self.classes = L(o for o in names.unique() if o == o)\n",
    "        if sort: self.classes = self.classes.sorted()\n",
    "        self.o2i = dict(self.classes.val2idx())\n",
    "        \n",
    "    def map_objs(\n",
    "        self, \n",
    "        objs # Some iterable collection\n",
    "    ):\n",
    "        \"Map `objs` to IDs\"\n",
    "        return L(self.o2i[o] for o in objs)\n",
    "    \n",
    "    def map_ids(\n",
    "        self, \n",
    "        ids # Some ids correlating to `self.classes`\n",
    "    ):\n",
    "        \"Map `ids` to objects in vocab\"\n",
    "        return L(self.classes[o] for o in ids)\n",
    "    \n",
    "    def __call__(self, o): \n",
    "        \"Label encode a single `o`\"\n",
    "        return int(self.o2i[o])\n",
    "    \n",
    "    def decode(self, o): return self.classes[o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "cat = Categorize(['a','b','c'])\n",
    "\n",
    "test_eq(cat('a'), 0)\n",
    "\n",
    "test_eq(cat.map_objs(['a','b','c']), L(0,1,2))\n",
    "test_eq(cat.map_ids([0,1,2]), L('a','b','c'))\n",
    "test_eq(cat.decode(0), 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75526c",
   "metadata": {},
   "source": [
    "## Splitters\n",
    "\n",
    "Functions designed for splitting your data\n",
    "\n",
    "To write your own you should make a function that returns two `L`'s of indicies (or `lists` work as well)\n",
    "\n",
    "For example, if I have a dataset of 5 items, we start with `[0,1,2,3,4]`. If I wanted to write a split function to split the first three and last two items into train and validation, I can write it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379bf135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_func(idxs): return L(idxs[:3]), L(idxs[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32f654",
   "metadata": {},
   "source": [
    "And we can see it work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc5be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_func([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def RandomSplitter(valid_pct=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Creates a function that splits some items between train and validation with `valid_pct` randomly\n",
    "    Based on the fastai class\n",
    "    \"\"\"\n",
    "    def _inner(o):\n",
    "        if seed is not None: torch.manual_seed(seed)\n",
    "        rand_idx = L(list(torch.randperm(len(o)).numpy()))\n",
    "        cut = int(valid_pct * len(o))\n",
    "        return rand_idx[cut:], rand_idx[:cut]\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1122031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "splitter = RandomSplitter(valid_pct=0.2)\n",
    "items = [0,1,2,3,4,5,6,7,8,9]\n",
    "res = splitter(items)\n",
    "test_eq(len(res[0]), 8)\n",
    "test_eq(len(res[1]), 2)\n",
    "test_eq(len(res), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e1a56",
   "metadata": {},
   "source": [
    "## The Basic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e869af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TaskDatasets:\n",
    "    \"\"\"\n",
    "    A set of datasets for a particular task, with a simple API.\n",
    "    \n",
    "    Note: This is the base API, `items` should be a set of regular text and model-ready labels,\n",
    "          including label or one-hot encoding being applied.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dset, # A train `Dataset` object\n",
    "        valid_dset, # A validation `Dataset` object\n",
    "        tokenizer_name:str = None, # The string name of a `HuggingFace` tokenizer or model. If `None`, will not tokenize the dataset.\n",
    "        tokenize:bool = True, # Whether to tokenize the dataset immediatly\n",
    "        tokenize_kwargs:dict = {}, # Some kwargs for when we call the tokenizer\n",
    "        auto_kwargs:dict = {}, # Some kwargs when calling `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        self.train = train_dset\n",
    "        self.valid = valid_dset\n",
    "        self.tokenizer = None\n",
    "        if tokenizer_name is not None: self.set_tokenizer(tokenizer_name, **auto_kwargs)\n",
    "        if self.tokenizer:\n",
    "            if 'max_length' in tokenize_kwargs.keys() and self.tokenizer.model_max_length >= tokenize_kwargs['max_length']: pass\n",
    "            elif 'max_length' in tokenize_kwargs.keys() and self.tokenizer.model_max_length < tokenize_kwargs['max_length']:\n",
    "                print(\"Warning: `max_length` is larger than the pretrained model\")\n",
    "            elif 'max_length' not in tokenize_kwargs.keys():\n",
    "                print(\"No value for `max_length` set, automatically adjusting to the size of the model and including truncation\")\n",
    "                tokenize_kwargs['max_length'] = self.tokenizer.model_max_length\n",
    "                tokenize_kwargs['truncation'] = True\n",
    "                print(f\"Sequence length set to: {tokenize_kwargs['max_length']}\")\n",
    "        if tokenize and self.tokenizer is not None: self._tokenize(**tokenize_kwargs)\n",
    "        elif tokenize and self.tokenizer is None:\n",
    "            print(\"Tried to tokenize a dataset without a tokenizer. Please set a tokenizer with `set_tokenizer` and call `_tokenize()`\")\n",
    "        \n",
    "            \n",
    "    def __getitem__(self, idx): return self.train[idx]\n",
    "    \n",
    "    def _tokenize(self, **kwargs):\n",
    "        \"Tokenize dataset in `self.items` with `kwargs` for `tokenize()`\"\n",
    "        if not self.tokenizer: raise ValueError(\"Tried to tokenize a dataset without a tokenizer. Please add a tokenizer with `set_tokenizer(tokenizer_name` and try again\")\n",
    "        def _inner(item):return self.tokenizer(item['text'], **kwargs)\n",
    "        self.train = self.train.map(_inner,batched=True,remove_columns = ['text'])\n",
    "        self.valid = self.valid.map(_inner,batched=True,remove_columns = ['text'])\n",
    "    \n",
    "    @delegates(AutoTokenizer.from_pretrained)\n",
    "    def set_tokenizer(\n",
    "        self,\n",
    "        tokenizer_name:str, # A string name of a `HuggingFace` tokenizer or model\n",
    "        override_existing:bool = False, # Whether to override an existing tokenizer\n",
    "        **kwargs # kwargs to go to `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        \"Sets a new `AutoTokenizer` to `self.tokenizer`\"\n",
    "        if self.tokenizer and not override_existing:\n",
    "            print(f'Warning! You are trying to override an existing tokenizer: {self.tokenizer.name_or_path}. Pass `override_existing=True` to use a new tokenizer')\n",
    "            return\n",
    "        elif self.tokenizer and override_existing:\n",
    "            print(f'Setting new tokenizer to {tokenizer_name}')\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, **kwargs)\n",
    "        except:\n",
    "            raise ValueError(f'{tokenizer_name} is not a valid pretrained model on the HuggingFace Hub or a local model')\n",
    "    \n",
    "    @delegates(DataLoaders)\n",
    "    def dataloaders(\n",
    "        self, \n",
    "        batch_size=8, # A batch size\n",
    "        shuffle_train=True, # Whether to shuffle the training dataset\n",
    "        collate_fn = None, # A custom collation function\n",
    "        **kwargs): # Torch DataLoader kwargs\n",
    "        \"Creates `DataLoaders` from the dataset\"\n",
    "        if collate_fn is None: collate_fn = default_data_collator\n",
    "        train_dl = DataLoader(self.train, shuffle=shuffle_train, collate_fn=collate_fn, batch_size=batch_size, **kwargs)\n",
    "        valid_dl = DataLoader(self.valid, shuffle=False, collate_fn=collate_fn, batch_size=batch_size, **kwargs)\n",
    "        return DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f0549",
   "metadata": {},
   "source": [
    "## The Basic `Tuner` and Integration with [fastai](https://docs.fast.ai)\n",
    "\n",
    "Since `fastai` is a _very_ lightweight framework that is easily approachable and incorporates state-of-the-art ideas, `AdaptNLP` bridges the gap between HuggingFace and fastai, allowing you to train with their framework through the `*Tuner` classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a7dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _AdaptiveLearner(Learner):\n",
    "    \"\"\"\n",
    "    A base fastai `Learner` that overrides `_split` and `_do_one_batch` to\n",
    "    have it work with HuggingFace datasets and models\n",
    "    \"\"\"\n",
    "    def _split(self, b):\n",
    "        \"Assign `self.xb` to model input and labels\"\n",
    "        self.xb = b\n",
    "        if 'labels' in b.keys(): self.yb = b['labels'].unsqueeze(0)\n",
    "    \n",
    "    def _do_one_batch(self):\n",
    "        \"Move a batch of data to a device, get predictions, calculate the loss, and perform backward pass\"\n",
    "        self.xb = {k:v.to(self.device) for k,v in self.xb.items()} # See if `to_device` fixes this\n",
    "        self.yb = self.yb.to(self.device)\n",
    "        out = self.model(**self.xb)\n",
    "        self.pred = out['logits'].to(self.device)\n",
    "        self('after_pred')\n",
    "        self.loss_grad = out['loss'].to(self.device)\n",
    "        self.loss = self.loss_grad.clone()\n",
    "        self('after_loss')\n",
    "        if not self.training or not len(self.yb): return\n",
    "        self('before_backward')\n",
    "        self.loss_grad.backward()\n",
    "        self._with_events(self.opt.step, 'step', CancelStepException)\n",
    "        self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59817daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mk_class('Strategy', **{'OneCycle':'fit_one_cycle', 'CosineAnnealing':'fit_flat_cos', 'SGDR':'fit_sgdr'}, doc_string='Class for fitting strategies with typo-proofing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7eb3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveTuner:\n",
    "    \"\"\"\n",
    "    A base `Tuner` that interfaces with `AdaptiveLearner` with specific exposed functions\n",
    "    \"\"\"\n",
    "    @delegates(_AdaptiveLearner.__init__)\n",
    "    def __init__(self, expose_fastai:bool=False, **kwargs):\n",
    "        self._tuner = _AdaptiveLearner(**kwargs)\n",
    "\n",
    "        exposed_attrs = ['dls', 'model', 'loss_func', 'metrics']\n",
    "        for attr in exposed_attrs:\n",
    "            setattr(self, attr, getattr(self._tuner, attr))\n",
    "        if expose_fastai:\n",
    "            cls = self.__class__\n",
    "            self.__class__ = cls.__class__(\"AdaptiveTuner\", (cls, _AdaptiveLearner), kwargs)\n",
    "            \n",
    "    def tune(\n",
    "        self,\n",
    "        epochs:int, # Number of epochs to train for\n",
    "        lr:float = None, # If None, finds a new LR and uses suggestion_method\n",
    "        strategy:Strategy = Strategy.OneCycle,\n",
    "        callbacks = [], # Extra fastai Callbacks\n",
    "        **kwargs ## kwargs for the fit function\n",
    "        \n",
    "    ):\n",
    "        \"Fine tune `self.model` for `epochs` with an `lr` and `strategy`\"\n",
    "        func = getattr(self, strategy, getattr(self._tuner, strategy, None))\n",
    "        for attr in 'epochs,lr,cbs'.split(): \n",
    "            if attr in kwargs.keys(): kwargs.pop(attr)\n",
    "        func(epochs, lr, cbs=callbacks, **kwargs)\n",
    "        \n",
    "    @delegates(Learner.lr_find)\n",
    "    def lr_find(self, **kwargs): return self._tuner.lr_find(**kwargs)\n",
    "    \n",
    "    def save(self, file:Union[Path,str], with_opt=True, pickle_protocol=2): \n",
    "        file = join_path_file(kwargs['file'], self.path/self.model_dir, ext='.pth')\n",
    "        if rank_distrib(): return # Don't save if child proc\n",
    "        opt = getattr(self, 'opt', None)\n",
    "        if opt is None: with_opt = False\n",
    "        state = get_model(self.model).state_dict()\n",
    "        state = {'model':state}\n",
    "        if with_opt: state['opt'] = opt.state_dict()\n",
    "        state['model_name_or_path'] = self.model.name_or_path\n",
    "        torch.save(state, file, pickle_protocol=pickle_protocol)\n",
    "        return file\n",
    "    \n",
    "    def load(self, file:Union[Path,str], device=None, with_opt=True, strict=True):\n",
    "        if device is None and hasattr(self.dls, 'device'): device = self.dls.device\n",
    "        if self.opt is None: self.create_opt()\n",
    "        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "        distrib_barrier()\n",
    "        if isinstance(device, int): device = torch.device('cuda', device)\n",
    "        elif device is None: device='cpu'\n",
    "        state = torch.load(file, map_location=device)\n",
    "        hasopt = 'opt' in state.keys()\n",
    "        model_state = state['model']\n",
    "        get_model(self.model).load_state_dict(model_state, strict=strict)\n",
    "        if hasopt and with_opt:\n",
    "            try: self.opt.load_state_dict(state['opt'])\n",
    "            except:\n",
    "                if with_opt: warn(\"Could not load the optimizer state.\")\n",
    "        elif with_opt: warn(\"Saved file doesn't contain an optimizer state\")\n",
    "        return self\n",
    "    \n",
    "for attr in ['lr_find', 'save', 'load']: \n",
    "    setattr(getattr(AdaptiveTuner, attr), '__doc__', getattr(_AdaptiveLearner, attr).__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b06d5",
   "metadata": {},
   "source": [
    "The constructor of the `AdaptiveTuner` class has an optional `expose_fastai_api` parameter. When set to `True`, the `Tuner` inherits fastai's `Learner`, so every attribute of the `Learner` is available to you. This is only recommended for those very familiar with the fastai API.\n",
    "\n",
    "Otherwise, you have access to six* functions in each class:\n",
    "  - `tune`\n",
    "  - `lr_find`\n",
    "  - `predict` (Coming soon)\n",
    "  - `save`\n",
    "  - `load`\n",
    "  - `export` (Coming soon)\n",
    "  \n",
    "All task fine-tuners should inherit the `AdaptiveTuner`, write good defaults, and override any specific needs as dictated by the task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac421c",
   "metadata": {},
   "source": [
    "## Export - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b890f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
