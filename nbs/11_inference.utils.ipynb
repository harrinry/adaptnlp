{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp inference.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Utils\n",
    "> Squad Metrics from the Transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\"\"\"Very heavily inspired by the official evaluation script for SQuAD version 2.0 which was\n",
    "modified by XLNet authors to update `find_best_threshold` scripts for SQuAD V2.0\n",
    "\n",
    "In addition to basic functionality, we also compute additional statistics and\n",
    "plot precision-recall curves if an additional na_prob.json file is provided.\n",
    "This file is expected to map question ID's to the model's predicted probability\n",
    "that a question is unanswerable.\n",
    "\n",
    "Additional modifications for adaptnlp include making prediction writes optional\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "\n",
    "from transformers.models.bert import BasicTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize_answer(\n",
    "    s:str # Some text\n",
    ") -> str:\n",
    "    \"Lowercase text and remove punctuation, articles and extra whitespace.\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_tokens(\n",
    "    s:str, # Some text\n",
    ") -> list: # Either a normalized answer or an empty array\n",
    "    \"Get normalized tokens if s is not none\"\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_exact(\n",
    "    a_gold:str, \n",
    "    a_pred:str\n",
    ") -> int:\n",
    "    \"Whether `a_gold` and `a_pred` match after normalizing\"\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_f1(\n",
    "    a_gold:str, \n",
    "    a_pred:str\n",
    ") -> float:\n",
    "    \"Calculate the F1 score between `a_gold` and `a_pred`\"\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_raw_scores(\n",
    "    examples:list, # Ground truth examples\n",
    "    preds:list # Model predictions\n",
    ") -> tuple([float, float]): # The exact score and f1 score\n",
    "    \"Computes the exact and F1 scores\"\n",
    "    exact_scores = {}\n",
    "    f1_scores = {}\n",
    "\n",
    "    for example in examples:\n",
    "        qas_id = example.qas_id\n",
    "        gold_answers = [\n",
    "            answer[\"text\"]\n",
    "            for answer in example.answers\n",
    "            if normalize_answer(answer[\"text\"])\n",
    "        ]\n",
    "\n",
    "        if not gold_answers:\n",
    "            # For unanswerable questions, only correct answer is empty string\n",
    "            gold_answers = [\"\"]\n",
    "\n",
    "        if qas_id not in preds:\n",
    "            print(\"Missing prediction for %s\" % qas_id)\n",
    "            continue\n",
    "\n",
    "        prediction = preds[qas_id]\n",
    "        exact_scores[qas_id] = max(compute_exact(a, prediction) for a in gold_answers)\n",
    "        f1_scores[qas_id] = max(compute_f1(a, prediction) for a in gold_answers)\n",
    "\n",
    "    return exact_scores, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_no_ans_threshold(\n",
    "    scores:list, \n",
    "    na_probs:list, \n",
    "    qid_to_has_ans:list, \n",
    "    na_prob_thresh:float\n",
    "):\n",
    "    \"Applies a threshold to non-answers\"\n",
    "    new_scores = {}\n",
    "    for qid, s in scores.items():\n",
    "        pred_na = na_probs[qid] > na_prob_thresh\n",
    "        if pred_na:\n",
    "            new_scores[qid] = float(not qid_to_has_ans[qid])\n",
    "        else:\n",
    "            new_scores[qid] = s\n",
    "    return new_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_eval_dict(\n",
    "    exact_scores:list, # Exact scores\n",
    "    f1_scores:list, # F1 Scores \n",
    "    qid_list=None # QID's\n",
    "):\n",
    "    \"Generates an eval dictionary with exact and F1 scores\"\n",
    "    if not qid_list:\n",
    "        total = len(exact_scores)\n",
    "        return collections.OrderedDict(\n",
    "            [\n",
    "                (\"exact\", 100.0 * sum(exact_scores.values()) / total),\n",
    "                (\"f1\", 100.0 * sum(f1_scores.values()) / total),\n",
    "                (\"total\", total),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        total = len(qid_list)\n",
    "        return collections.OrderedDict(\n",
    "            [\n",
    "                (\"exact\", 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n",
    "                (\"f1\", 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n",
    "                (\"total\", total),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def merge_eval(main_eval, new_eval, prefix):\n",
    "    \"Merges eval dictionaries inplace based on prefix\"\n",
    "    for k in new_eval:\n",
    "        main_eval[\"%s_%s\" % (prefix, k)] = new_eval[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_best_thresh_v2(\n",
    "    preds, # Model predictions\n",
    "    scores, # Real scores\n",
    "    na_probs, # Probabilities for NA\n",
    "    qid_to_has_ans\n",
    "):\n",
    "    \"Finds the best score threshold\"\n",
    "    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
    "    cur_score = num_no_ans\n",
    "    best_score = cur_score\n",
    "    best_thresh = 0.0\n",
    "    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
    "    for i, qid in enumerate(qid_list):\n",
    "        if qid not in scores:\n",
    "            continue\n",
    "        if qid_to_has_ans[qid]:\n",
    "            diff = scores[qid]\n",
    "        else:\n",
    "            if preds[qid]:\n",
    "                diff = -1\n",
    "            else:\n",
    "                diff = 0\n",
    "        cur_score += diff\n",
    "        if cur_score > best_score:\n",
    "            best_score = cur_score\n",
    "            best_thresh = na_probs[qid]\n",
    "\n",
    "    has_ans_score, has_ans_cnt = 0, 0\n",
    "    for qid in qid_list:\n",
    "        if not qid_to_has_ans[qid]:\n",
    "            continue\n",
    "        has_ans_cnt += 1\n",
    "\n",
    "        if qid not in scores:\n",
    "            continue\n",
    "        has_ans_score += scores[qid]\n",
    "\n",
    "    return (\n",
    "        100.0 * best_score / len(scores),\n",
    "        best_thresh,\n",
    "        1.0 * has_ans_score / has_ans_cnt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_all_best_thresh_v2(\n",
    "    main_eval, # Main evaluation dictionary\n",
    "    preds, # A list of predictions\n",
    "    exact_raw, # A list of exact scores\n",
    "    f1_raw, # A list of F1 scores\n",
    "    na_probs, # A list of NA probabilities\n",
    "    qid_to_has_ans\n",
    "):\n",
    "    \"Finds the best threshold for all inputs\"\n",
    "    best_exact, exact_thresh, has_ans_exact = find_best_thresh_v2(\n",
    "        preds, exact_raw, na_probs, qid_to_has_ans\n",
    "    )\n",
    "    best_f1, f1_thresh, has_ans_f1 = find_best_thresh_v2(\n",
    "        preds, f1_raw, na_probs, qid_to_has_ans\n",
    "    )\n",
    "    main_eval[\"best_exact\"] = best_exact\n",
    "    main_eval[\"best_exact_thresh\"] = exact_thresh\n",
    "    main_eval[\"best_f1\"] = best_f1\n",
    "    main_eval[\"best_f1_thresh\"] = f1_thresh\n",
    "    main_eval[\"has_ans_exact\"] = has_ans_exact\n",
    "    main_eval[\"has_ans_f1\"] = has_ans_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_best_thresh(\n",
    "    preds, # Model predictions\n",
    "    scores, # Real scores\n",
    "    na_probs, # Probabilities for NA\n",
    "    qid_to_has_ans\n",
    "):\n",
    "    \"Finds best answer threshold\"\n",
    "    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
    "    cur_score = num_no_ans\n",
    "    best_score = cur_score\n",
    "    best_thresh = 0.0\n",
    "    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
    "    for _, qid in enumerate(qid_list):\n",
    "        if qid not in scores:\n",
    "            continue\n",
    "        if qid_to_has_ans[qid]:\n",
    "            diff = scores[qid]\n",
    "        else:\n",
    "            if preds[qid]:\n",
    "                diff = -1\n",
    "            else:\n",
    "                diff = 0\n",
    "        cur_score += diff\n",
    "        if cur_score > best_score:\n",
    "            best_score = cur_score\n",
    "            best_thresh = na_probs[qid]\n",
    "    return 100.0 * best_score / len(scores), best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_all_best_thresh(\n",
    "    main_eval, # Main evaluation dictionary\n",
    "    preds, # A list of predictions\n",
    "    exact_raw, # A list of exact scores\n",
    "    f1_raw, # A list of F1 scores\n",
    "    na_probs, # A list of NA probabilities\n",
    "    qid_to_has_ans\n",
    "):\n",
    "    \"Finds the best threshold for all inputs\"\n",
    "    best_exact, exact_thresh = find_best_thresh(\n",
    "        preds, exact_raw, na_probs, qid_to_has_ans\n",
    "    )\n",
    "    best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n",
    "\n",
    "    main_eval[\"best_exact\"] = best_exact\n",
    "    main_eval[\"best_exact_thresh\"] = exact_thresh\n",
    "    main_eval[\"best_f1\"] = best_f1\n",
    "    main_eval[\"best_f1_thresh\"] = f1_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def squad_evaluate(\n",
    "    examples, # Ground truth examples\n",
    "    preds, # Model predictions\n",
    "    no_answer_probs=None, \n",
    "    no_answer_probability_threshold=1.0\n",
    "):\n",
    "    \"Evalues SQuAD scores on inputs\"\n",
    "    qas_id_to_has_answer = {\n",
    "        example.qas_id: bool(example.answers) for example in examples\n",
    "    }\n",
    "    has_answer_qids = [\n",
    "        qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if has_answer\n",
    "    ]\n",
    "    no_answer_qids = [\n",
    "        qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if not has_answer\n",
    "    ]\n",
    "\n",
    "    if no_answer_probs is None:\n",
    "        no_answer_probs = {k: 0.0 for k in preds}\n",
    "\n",
    "    exact, f1 = get_raw_scores(examples, preds)\n",
    "\n",
    "    exact_threshold = apply_no_ans_threshold(\n",
    "        exact, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold\n",
    "    )\n",
    "    f1_threshold = apply_no_ans_threshold(\n",
    "        f1, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold\n",
    "    )\n",
    "\n",
    "    evaluation = make_eval_dict(exact_threshold, f1_threshold)\n",
    "\n",
    "    if has_answer_qids:\n",
    "        has_ans_eval = make_eval_dict(\n",
    "            exact_threshold, f1_threshold, qid_list=has_answer_qids\n",
    "        )\n",
    "        merge_eval(evaluation, has_ans_eval, \"HasAns\")\n",
    "\n",
    "    if no_answer_qids:\n",
    "        no_ans_eval = make_eval_dict(\n",
    "            exact_threshold, f1_threshold, qid_list=no_answer_qids\n",
    "        )\n",
    "        merge_eval(evaluation, no_ans_eval, \"NoAns\")\n",
    "\n",
    "    if no_answer_probs:\n",
    "        find_all_best_thresh(\n",
    "            evaluation, preds, exact, f1, no_answer_probs, qas_id_to_has_answer\n",
    "        )\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n",
    "    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
    "\n",
    "    # When we created the data, we kept track of the alignment between original\n",
    "    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
    "    # now `orig_text` contains the span of our original text corresponding to the\n",
    "    # span that we predicted.\n",
    "    #\n",
    "    # However, `orig_text` may contain extra characters that we don't want in\n",
    "    # our prediction.\n",
    "    #\n",
    "    # For example, let's say:\n",
    "    #   pred_text = steve smith\n",
    "    #   orig_text = Steve Smith's\n",
    "    #\n",
    "    # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
    "    #\n",
    "    # We don't want to return `pred_text` because it's already been normalized\n",
    "    # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
    "    # our tokenizer does additional normalization like stripping accent\n",
    "    # characters).\n",
    "    #\n",
    "    # What we really want to return is \"Steve Smith\".\n",
    "    #\n",
    "    # Therefore, we have to apply a semi-complicated alignment heuristic between\n",
    "    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n",
    "    # can fail in certain cases in which case we just return `orig_text`.\n",
    "\n",
    "    def _strip_spaces(text):\n",
    "        ns_chars = []\n",
    "        ns_to_s_map = collections.OrderedDict()\n",
    "        for (i, c) in enumerate(text):\n",
    "            if c == \" \":\n",
    "                continue\n",
    "            ns_to_s_map[len(ns_chars)] = i\n",
    "            ns_chars.append(c)\n",
    "        ns_text = \"\".join(ns_chars)\n",
    "        return (ns_text, ns_to_s_map)\n",
    "\n",
    "    # We first tokenize `orig_text`, strip whitespace from the result\n",
    "    # and `pred_text`, and check if they are the same length. If they are\n",
    "    # NOT the same length, the heuristic has failed. If they are the same\n",
    "    # length, we assume the characters are one-to-one aligned.\n",
    "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "\n",
    "    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
    "\n",
    "    start_position = tok_text.find(pred_text)\n",
    "    if start_position == -1:\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
    "        return orig_text\n",
    "    end_position = start_position + len(pred_text) - 1\n",
    "\n",
    "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
    "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
    "\n",
    "    if len(orig_ns_text) != len(tok_ns_text):\n",
    "        if verbose_logging:\n",
    "            logger.info(\n",
    "                \"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
    "                orig_ns_text,\n",
    "                tok_ns_text,\n",
    "            )\n",
    "        return orig_text\n",
    "\n",
    "    # We then project the characters in `pred_text` back to `orig_text` using\n",
    "    # the character-to-character alignment.\n",
    "    tok_s_to_ns_map = {}\n",
    "    for (i, tok_index) in tok_ns_to_s_map.items():\n",
    "        tok_s_to_ns_map[tok_index] = i\n",
    "\n",
    "    orig_start_position = None\n",
    "    if start_position in tok_s_to_ns_map:\n",
    "        ns_start_position = tok_s_to_ns_map[start_position]\n",
    "        if ns_start_position in orig_ns_to_s_map:\n",
    "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
    "\n",
    "    if orig_start_position is None:\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Couldn't map start position\")\n",
    "        return orig_text\n",
    "\n",
    "    orig_end_position = None\n",
    "    if end_position in tok_s_to_ns_map:\n",
    "        ns_end_position = tok_s_to_ns_map[end_position]\n",
    "        if ns_end_position in orig_ns_to_s_map:\n",
    "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
    "\n",
    "    if orig_end_position is None:\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Couldn't map end position\")\n",
    "        return orig_text\n",
    "\n",
    "    output_text = orig_text[orig_start_position : (orig_end_position + 1)]\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _get_best_indexes(logits, n_best_size):\n",
    "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
    "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_indexes = []\n",
    "    for i in range(len(index_and_score)):\n",
    "        if i >= n_best_size:\n",
    "            break\n",
    "        best_indexes.append(index_and_score[i][0])\n",
    "    return best_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _compute_softmax(scores):\n",
    "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
    "    if not scores:\n",
    "        return []\n",
    "\n",
    "    max_score = None\n",
    "    for score in scores:\n",
    "        if max_score is None or score > max_score:\n",
    "            max_score = score\n",
    "\n",
    "    exp_scores = []\n",
    "    total_sum = 0.0\n",
    "    for score in scores:\n",
    "        x = math.exp(score - max_score)\n",
    "        exp_scores.append(x)\n",
    "        total_sum += x\n",
    "\n",
    "    probs = []\n",
    "    for score in exp_scores:\n",
    "        probs.append(score / total_sum)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_predictions_logits(\n",
    "    all_examples,\n",
    "    all_features,\n",
    "    all_results,\n",
    "    n_best_size,\n",
    "    max_answer_length,\n",
    "    do_lower_case,\n",
    "    verbose_logging,\n",
    "    version_2_with_negative,\n",
    "    null_score_diff_threshold,\n",
    "    tokenizer,\n",
    "    output_prediction_file=None,\n",
    "    output_nbest_file=None,\n",
    "    output_null_log_odds_file=None,\n",
    "):\n",
    "\n",
    "    example_index_to_features = collections.defaultdict(list)\n",
    "    for feature in all_features:\n",
    "        example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "    unique_id_to_result = {}\n",
    "    for result in all_results:\n",
    "        unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\",\n",
    "        [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"],\n",
    "    )\n",
    "\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "    for (example_index, example) in enumerate(all_examples):\n",
    "        features = example_index_to_features[example_index]\n",
    "\n",
    "        prelim_predictions = []\n",
    "        # keep track of the minimum score of null start+end of position 0\n",
    "        score_null = 1000000  # large and positive\n",
    "        min_null_feature_index = 0  # the paragraph slice with min null score\n",
    "        null_start_logit = 0  # the start logit at the slice with min null score\n",
    "        null_end_logit = 0  # the end logit at the slice with min null score\n",
    "        for (feature_index, feature) in enumerate(features):\n",
    "            result = unique_id_to_result[feature.unique_id]\n",
    "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
    "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
    "            # if we could have irrelevant answers, get the min score of irrelevant\n",
    "            if version_2_with_negative:\n",
    "                feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
    "                if feature_null_score < score_null:\n",
    "                    score_null = feature_null_score\n",
    "                    min_null_feature_index = feature_index\n",
    "                    null_start_logit = result.start_logits[0]\n",
    "                    null_end_logit = result.end_logits[0]\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # We could hypothetically create invalid predictions, e.g., predict\n",
    "                    # that the start of the span is in the question. We throw out all\n",
    "                    # invalid predictions.\n",
    "                    if start_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if end_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if start_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if end_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if not feature.token_is_max_context.get(start_index, False):\n",
    "                        continue\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "                    length = end_index - start_index + 1\n",
    "                    if length > max_answer_length:\n",
    "                        continue\n",
    "                    prelim_predictions.append(\n",
    "                        _PrelimPrediction(\n",
    "                            feature_index=feature_index,\n",
    "                            start_index=start_index,\n",
    "                            end_index=end_index,\n",
    "                            start_logit=result.start_logits[start_index],\n",
    "                            end_logit=result.end_logits[end_index],\n",
    "                        )\n",
    "                    )\n",
    "        if version_2_with_negative:\n",
    "            prelim_predictions.append(\n",
    "                _PrelimPrediction(\n",
    "                    feature_index=min_null_feature_index,\n",
    "                    start_index=0,\n",
    "                    end_index=0,\n",
    "                    start_logit=null_start_logit,\n",
    "                    end_logit=null_end_logit,\n",
    "                )\n",
    "            )\n",
    "        prelim_predictions = sorted(\n",
    "            prelim_predictions,\n",
    "            key=lambda x: (x.start_logit + x.end_logit),\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"NbestPrediction\",\n",
    "            [\"text\", \"start_logit\", \"end_logit\", \"start_index\", \"end_index\"],\n",
    "        )\n",
    "\n",
    "        seen_predictions = {}\n",
    "        nbest = []\n",
    "        for pred in prelim_predictions:\n",
    "            if len(nbest) >= n_best_size:\n",
    "                break\n",
    "            feature = features[pred.feature_index]\n",
    "            if pred.start_index > 0:  # this is a non-null prediction\n",
    "                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n",
    "                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n",
    "\n",
    "                tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n",
    "\n",
    "                # tok_text = \" \".join(tok_tokens)\n",
    "                #\n",
    "                # # De-tokenize WordPieces that have been split off.\n",
    "                # tok_text = tok_text.replace(\" ##\", \"\")\n",
    "                # tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "                # Clean whitespace\n",
    "                tok_text = tok_text.strip()\n",
    "                tok_text = \" \".join(tok_text.split())\n",
    "                orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "                final_text = get_final_text(\n",
    "                    tok_text, orig_text, do_lower_case, verbose_logging\n",
    "                )\n",
    "                if final_text in seen_predictions:\n",
    "                    continue\n",
    "\n",
    "                seen_predictions[final_text] = True\n",
    "            else:\n",
    "                final_text = \"\"\n",
    "                seen_predictions[final_text] = True\n",
    "\n",
    "            nbest.append(\n",
    "                _NbestPrediction(\n",
    "                    text=final_text,\n",
    "                    start_logit=pred.start_logit,\n",
    "                    end_logit=pred.end_logit,\n",
    "                    start_index=orig_doc_start,\n",
    "                    end_index=orig_doc_end,\n",
    "                )\n",
    "            )\n",
    "        # if we didn't include the empty option in the n-best, include it\n",
    "        if version_2_with_negative:\n",
    "            if \"\" not in seen_predictions:\n",
    "                nbest.append(\n",
    "                    _NbestPrediction(\n",
    "                        text=\"\",\n",
    "                        start_logit=null_start_logit,\n",
    "                        end_logit=null_end_logit,\n",
    "                        start_index=0,\n",
    "                        end_index=0,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # In very rare edge cases we could only have single null prediction.\n",
    "            # So we just create a nonce prediction in this case to avoid failure.\n",
    "            if len(nbest) == 1:\n",
    "                nbest.insert(\n",
    "                    0,\n",
    "                    _NbestPrediction(\n",
    "                        text=\"empty\",\n",
    "                        start_logit=0.0,\n",
    "                        end_logit=0.0,\n",
    "                        start_index=0,\n",
    "                        end_index=0,\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "        # In very rare edge cases we could have no valid predictions. So we\n",
    "        # just create a nonce prediction in this case to avoid failure.\n",
    "        if not nbest:\n",
    "            nbest.append(\n",
    "                _NbestPrediction(\n",
    "                    text=\"empty\",\n",
    "                    start_logit=0.0,\n",
    "                    end_logit=0.0,\n",
    "                    start_index=0,\n",
    "                    end_index=0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        assert len(nbest) >= 1\n",
    "\n",
    "        total_scores = []\n",
    "        best_non_null_entry = None\n",
    "        for entry in nbest:\n",
    "            total_scores.append(entry.start_logit + entry.end_logit)\n",
    "            if not best_non_null_entry:\n",
    "                if entry.text:\n",
    "                    best_non_null_entry = entry\n",
    "\n",
    "        probs = _compute_softmax(total_scores)\n",
    "\n",
    "        nbest_json = []\n",
    "        for (i, entry) in enumerate(nbest):\n",
    "            output = collections.OrderedDict()\n",
    "            output[\"text\"] = entry.text\n",
    "            output[\"probability\"] = probs[i]\n",
    "            output[\"start_logit\"] = entry.start_logit\n",
    "            output[\"end_logit\"] = entry.end_logit\n",
    "            output[\"start_index\"] = entry.start_index\n",
    "            output[\"end_index\"] = entry.end_index\n",
    "            nbest_json.append(output)\n",
    "\n",
    "        assert len(nbest_json) >= 1\n",
    "\n",
    "        if not version_2_with_negative:\n",
    "            all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
    "        else:\n",
    "            # predict \"\" iff the null score - the score of best non-null > threshold\n",
    "            score_diff = (\n",
    "                score_null\n",
    "                - best_non_null_entry.start_logit\n",
    "                - (best_non_null_entry.end_logit)\n",
    "            )\n",
    "            scores_diff_json[example.qas_id] = score_diff\n",
    "            if score_diff > null_score_diff_threshold:\n",
    "                all_predictions[example.qas_id] = \"\"\n",
    "            else:\n",
    "                all_predictions[example.qas_id] = best_non_null_entry.text\n",
    "        all_nbest_json[example.qas_id] = nbest_json\n",
    "\n",
    "    if output_prediction_file:\n",
    "        with open(output_prediction_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "\n",
    "    if output_nbest_file:\n",
    "        with open(output_nbest_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "    if output_null_log_odds_file and version_2_with_negative:\n",
    "        with open(output_null_log_odds_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
    "\n",
    "    return all_predictions, all_nbest_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_predictions_log_probs(\n",
    "    all_examples,\n",
    "    all_features,\n",
    "    all_results,\n",
    "    n_best_size,\n",
    "    max_answer_length,\n",
    "    start_n_top,\n",
    "    end_n_top,\n",
    "    version_2_with_negative,\n",
    "    tokenizer,\n",
    "    verbose_logging,\n",
    "    output_prediction_file=None,\n",
    "    output_nbest_file=None,\n",
    "    output_null_log_odds_file=None,\n",
    "):\n",
    "    \"\"\"XLNet write prediction logic (more complex than Bert's).\n",
    "    Write final predictions to the json file and log-odds of null if needed.\n",
    "    Requires utils_squad_evaluate.py\n",
    "    \"\"\"\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\",\n",
    "        [\"feature_index\", \"start_index\", \"end_index\", \"start_log_prob\", \"end_log_prob\"],\n",
    "    )\n",
    "\n",
    "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"NbestPrediction\", [\"text\", \"start_log_prob\", \"end_log_prob\"]\n",
    "    )\n",
    "\n",
    "    example_index_to_features = collections.defaultdict(list)\n",
    "    for feature in all_features:\n",
    "        example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "    unique_id_to_result = {}\n",
    "    for result in all_results:\n",
    "        unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "    for (example_index, example) in enumerate(all_examples):\n",
    "        features = example_index_to_features[example_index]\n",
    "\n",
    "        prelim_predictions = []\n",
    "        # keep track of the minimum score of null start+end of position 0\n",
    "        score_null = 1000000  # large and positive\n",
    "\n",
    "        for (feature_index, feature) in enumerate(features):\n",
    "            result = unique_id_to_result[feature.unique_id]\n",
    "\n",
    "            cur_null_score = result.cls_logits\n",
    "\n",
    "            # if we could have irrelevant answers, get the min score of irrelevant\n",
    "            score_null = min(score_null, cur_null_score)\n",
    "\n",
    "            for i in range(start_n_top):\n",
    "                for j in range(end_n_top):\n",
    "                    start_log_prob = result.start_logits[i]\n",
    "                    start_index = result.start_top_index[i]\n",
    "\n",
    "                    j_index = i * end_n_top + j\n",
    "\n",
    "                    end_log_prob = result.end_logits[j_index]\n",
    "                    end_index = result.end_top_index[j_index]\n",
    "\n",
    "                    # We could hypothetically create invalid predictions, e.g., predict\n",
    "                    # that the start of the span is in the question. We throw out all\n",
    "                    # invalid predictions.\n",
    "                    if start_index >= feature.paragraph_len - 1:\n",
    "                        continue\n",
    "                    if end_index >= feature.paragraph_len - 1:\n",
    "                        continue\n",
    "\n",
    "                    if not feature.token_is_max_context.get(start_index, False):\n",
    "                        continue\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "                    length = end_index - start_index + 1\n",
    "                    if length > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    prelim_predictions.append(\n",
    "                        _PrelimPrediction(\n",
    "                            feature_index=feature_index,\n",
    "                            start_index=start_index,\n",
    "                            end_index=end_index,\n",
    "                            start_log_prob=start_log_prob,\n",
    "                            end_log_prob=end_log_prob,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        prelim_predictions = sorted(\n",
    "            prelim_predictions,\n",
    "            key=lambda x: (x.start_log_prob + x.end_log_prob),\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        seen_predictions = {}\n",
    "        nbest = []\n",
    "        for pred in prelim_predictions:\n",
    "            if len(nbest) >= n_best_size:\n",
    "                break\n",
    "            feature = features[pred.feature_index]\n",
    "\n",
    "            # XLNet un-tokenizer\n",
    "            # Let's keep it simple for now and see if we need all this later.\n",
    "            #\n",
    "            # tok_start_to_orig_index = feature.tok_start_to_orig_index\n",
    "            # tok_end_to_orig_index = feature.tok_end_to_orig_index\n",
    "            # start_orig_pos = tok_start_to_orig_index[pred.start_index]\n",
    "            # end_orig_pos = tok_end_to_orig_index[pred.end_index]\n",
    "            # paragraph_text = example.paragraph_text\n",
    "            # final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()\n",
    "\n",
    "            # Previously used Bert untokenizer\n",
    "            tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n",
    "            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "            orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n",
    "            tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n",
    "\n",
    "            # Clean whitespace\n",
    "            tok_text = tok_text.strip()\n",
    "            tok_text = \" \".join(tok_text.split())\n",
    "            orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "            if hasattr(tokenizer, \"do_lower_case\"):\n",
    "                do_lower_case = tokenizer.do_lower_case\n",
    "            else:\n",
    "                do_lower_case = tokenizer.do_lowercase_and_remove_accent\n",
    "\n",
    "            final_text = get_final_text(\n",
    "                tok_text, orig_text, do_lower_case, verbose_logging\n",
    "            )\n",
    "\n",
    "            if final_text in seen_predictions:\n",
    "                continue\n",
    "\n",
    "            seen_predictions[final_text] = True\n",
    "\n",
    "            nbest.append(\n",
    "                _NbestPrediction(\n",
    "                    text=final_text,\n",
    "                    start_log_prob=pred.start_log_prob,\n",
    "                    end_log_prob=pred.end_log_prob,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # In very rare edge cases we could have no valid predictions. So we\n",
    "        # just create a nonce prediction in this case to avoid failure.\n",
    "        if not nbest:\n",
    "            nbest.append(\n",
    "                _NbestPrediction(text=\"\", start_log_prob=-1e6, end_log_prob=-1e6)\n",
    "            )\n",
    "\n",
    "        total_scores = []\n",
    "        best_non_null_entry = None\n",
    "        for entry in nbest:\n",
    "            total_scores.append(entry.start_log_prob + entry.end_log_prob)\n",
    "            if not best_non_null_entry:\n",
    "                best_non_null_entry = entry\n",
    "\n",
    "        probs = _compute_softmax(total_scores)\n",
    "\n",
    "        nbest_json = []\n",
    "        for (i, entry) in enumerate(nbest):\n",
    "            output = collections.OrderedDict()\n",
    "            output[\"text\"] = entry.text\n",
    "            output[\"probability\"] = probs[i]\n",
    "            output[\"start_log_prob\"] = entry.start_log_prob\n",
    "            output[\"end_log_prob\"] = entry.end_log_prob\n",
    "            nbest_json.append(output)\n",
    "\n",
    "        assert len(nbest_json) >= 1\n",
    "        assert best_non_null_entry is not None\n",
    "\n",
    "        score_diff = score_null\n",
    "        scores_diff_json[example.qas_id] = score_diff\n",
    "        # note(zhiliny): always predict best_non_null_entry\n",
    "        # and the evaluation script will search for the best threshold\n",
    "        all_predictions[example.qas_id] = best_non_null_entry.text\n",
    "\n",
    "        all_nbest_json[example.qas_id] = nbest_json\n",
    "\n",
    "    if output_prediction_file:\n",
    "        with open(output_prediction_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "\n",
    "    if output_nbest_file:\n",
    "        with open(output_nbest_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "    if version_2_with_negative:\n",
    "        with open(output_null_log_odds_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
    "\n",
    "    return all_predictions, all_nbest_json"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
