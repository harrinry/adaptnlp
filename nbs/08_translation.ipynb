{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp inference.translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation\n",
    "> Translation API for AdaptNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "\n",
    "from adaptnlp.model import AdaptiveModel\n",
    "from adaptnlp.callback import GeneratorCallback\n",
    "\n",
    "from fastai_minima.utils import apply, to_device\n",
    "\n",
    "from fastcore.basics import Self\n",
    "from adaptnlp.model_hub import HFModelResult, FlairModelResult, HFModelHub, FlairModelHub\n",
    "from adaptnlp.result import DetailLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TranslationResult:\n",
    "    \"\"\"\n",
    "    A basic result class for Translation problems\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputs:List[str],\n",
    "        input_lang:str,\n",
    "        output_lang:str,\n",
    "        translations:List[str]\n",
    "    ):\n",
    "        self.inputs = inputs\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.translations = translations\n",
    "    \n",
    "    def to_dict(self, detail_level:DetailLevel=DetailLevel.Low):\n",
    "        o = OrderedDict()\n",
    "        o.update(\n",
    "            {'translations':self.translations,}\n",
    "        )\n",
    "        if detail_level == 'medium' or detail_level == 'high':\n",
    "            o.update({\n",
    "                'inputs': self.inputs,\n",
    "                'input_language': self.input_lang,\n",
    "                'output_language': self.output_lang\n",
    "            })\n",
    "        if detail_level == 'high':\n",
    "            print(\"Warning: There is no difference between `medium` and `high` for Translation tasks\")\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTranslator(AdaptiveModel):\n",
    "    \"\"\"Adaptive model for Transformer's Conditional Generation or Language Models (Transformer's T5 and Bart\n",
    "    conditional generation models have a language modeling head)\n",
    "\n",
    "    Usage:\n",
    "    ```python\n",
    "    >>> translator = TransformersTranslator.load('transformers-translator-model')\n",
    "    >>> translator.predict(text='Example text', mini_batch_size=32)\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **tokenizer** - A tokenizer object from Huggingface's transformers (TODO)and tokenizers\n",
    "    * **model** - A transformers Conditional Generation (Bart or T5) or Language model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n",
    "        # Load up model and tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sets internal model\n",
    "        self.set_model(model)\n",
    "        \n",
    "        # Set inputs to come in as `dict`\n",
    "        super().set_as_dict(True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading and constructing this classifier\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Transformer's pre-trained translator Model\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "        translator = cls(tokenizer, model)\n",
    "        return translator\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[str], str],\n",
    "        t5_prefix: str = 'translate English to German',\n",
    "        mini_batch_size: int = 32,\n",
    "        num_beams: int = 1,\n",
    "        min_length: int = 0,\n",
    "        max_length: int = 128,\n",
    "        early_stopping: bool = True,\n",
    "        detail_level:DetailLevel = DetailLevel.Low,\n",
    "        **kwargs,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model.  Keyword arguments\n",
    "        for parameters of the method `Transformers.PreTrainedModel.generate()` can be used as well.\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **t5_prefix**(Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models.\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **num_beams** - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search.  Default to 1.\n",
    "        * **min_length** -  The min length of the sequence to be generated. Default to 0\n",
    "        * **max_length** - The max length of the sequence to be generated. Between min_length and infinity. Default to 128\n",
    "        * **early_stopping** - if set to True beam search is stopped when at least num_beams sentences finished per batch.\n",
    "        * **detail_level** - The level of detail to return\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make all inputs lists\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        \n",
    "        # T5 requires 'translate: ' precursor text for pre-trained translator\n",
    "        if isinstance(self.model, T5ForConditionalGeneration):\n",
    "            text = [f'{t5_prefix}: {t}' for t in text]\n",
    "            \n",
    "        dataset = self._tokenize(text)\n",
    "        dl = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "        translations = []\n",
    "        \n",
    "        logger.info(f'Running translator on {len(dataset)} text sequences')\n",
    "        logger.info(f'Batch size = {mini_batch_size}')\n",
    "        \n",
    "        cb = GeneratorCallback(num_beams, min_length, max_length, early_stopping, **kwargs)\n",
    "        \n",
    "        preds,_ = super().get_preds(dl=dl, cbs=[cb])\n",
    "        \n",
    "        preds = apply(Self.squeeze(0), preds)\n",
    "\n",
    "        for o in preds:\n",
    "            translations.append(\n",
    "                [\n",
    "                    self.tokenizer.decode(\n",
    "                        o,\n",
    "                        skip_special_tokens=True,\n",
    "                        clean_up_tokenization_spaces=False,\n",
    "                    )\n",
    "                ].pop()\n",
    "            )\n",
    "        \n",
    "        languages = t5_prefix.strip('translate ').split(' to ')\n",
    "        \n",
    "        res = TranslationResult(text, *languages, translations)\n",
    "\n",
    "        return res if detail_level is None else res.to_dict(detail_level)\n",
    "\n",
    "    def _tokenize(self, text: Union[List[str], str]) -> TensorDataset:\n",
    "        \"\"\" Batch tokenizes text and produces a `TensorDataset` with text \"\"\"\n",
    "\n",
    "        tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        # Bart doesn't use `token_type_ids`\n",
    "        dataset = TensorDataset(\n",
    "            tokenized_text['input_ids'],\n",
    "            tokenized_text['attention_mask'],\n",
    "        )\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyTranslator:\n",
    "    \"\"\"Translation Module\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> translator = EasyTranslator()\n",
    "    >>> translator.translate(text=\"translate this text\", model_name_or_path=\"t5-small\")\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.translators: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        text: Union[List[str], str],\n",
    "        model_name_or_path: str = \"t5-small\",\n",
    "        t5_prefix: str = \"translate English to German\",\n",
    "        detail_level=DetailLevel.Low,\n",
    "        mini_batch_size: int = 32,\n",
    "        num_beams: int = 1,\n",
    "        min_length: int = 0,\n",
    "        max_length: int = 128,\n",
    "        early_stopping: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments\n",
    "        for parameters of the method `Transformers.PreTrainedModel.generate()` can be used as well.\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **model_name_or_path** - A String model id or path to a pre-trained model repository or custom trained model directory\n",
    "        * **t5_prefix**(Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models.\n",
    "        * **detail_level** - The level of detail to return\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **num_beams** - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search.  Default to 1.\n",
    "        * **min_length** -  The min length of the sequence to be generated. Default to 0\n",
    "        * **max_length** - The max length of the sequence to be generated. Between min_length and infinity. Default to 128\n",
    "        * **early_stopping** - if set to True beam search is stopped when at least num_beams sentences finished per batch.\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n",
    "        \"\"\"\n",
    "        name = getattr(model_name_or_path, 'name', model_name_or_path)\n",
    "        if not self.translators[name]:\n",
    "            self.translators[name] = TransformersTranslator.load(\n",
    "                name\n",
    "            )\n",
    "\n",
    "        translator = self.translators[name]\n",
    "        return translator.predict(\n",
    "            text=text,\n",
    "            t5_prefix=t5_prefix,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            num_beams=num_beams,\n",
    "            min_length=min_length,\n",
    "            max_length=max_length,\n",
    "            early_stopping=early_stopping,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "text = [\"Machine learning will take over the world very soon.\",\n",
    "        \"Machines can speak in many languages.\",]\n",
    "\n",
    "translator = EasyTranslator()\n",
    "translations = translator.translate(text = text, t5_prefix=\"translate English to German\", model_name_or_path=\"t5-small\", mini_batch_size=1, min_length=0, max_length=100, early_stopping=True)\n",
    "test_eq(translations['translations'], ['Das Maschinenlernen wird die Welt in Kürze übernehmen.',\n",
    " 'Maschinen können in vielen Sprachen sprechen.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "hub = HFModelHub()\n",
    "model = hub.search_model_by_task(\"translation\")[-1]\n",
    "translations = translator.translate(text = text, t5_prefix=\"translate English to German\", model_name_or_path=model, mini_batch_size=1, min_length=0, max_length=100, early_stopping=True)\n",
    "test_eq(translations['translations'], ['Das Maschinenlernen wird die Welt in Kürze übernehmen.',\n",
    " 'Maschinen können in vielen Sprachen sprechen.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
