{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64deeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training.sequence_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2fcad",
   "metadata": {},
   "source": [
    "# Sequence Classification Tuning\n",
    "> Data and Tuning API for Sequence Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23625fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from fastcore.foundation import L\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.xtras import Path, range_of\n",
    "\n",
    "from fastai.basics import * # TODO: Replace with absolutes in fastai_minima\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, default_data_collator, AutoTokenizer\n",
    "\n",
    "from adaptnlp.training.core import * # Core has everything we need so you should always import * with it\n",
    "\n",
    "from adaptnlp.inference.sequence_classification import TransformersSequenceClassifier, SequenceResult, DetailLevel\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659d49f",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1465bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceClassificationDatasets(TaskDatasets):\n",
    "    \"\"\"\n",
    "    A set of datasets designed for sequence classification\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        items, # Some items we can pull x's and y's from\n",
    "        get_x = ColReader('text'), # A function taking in one item and extracting the text\n",
    "        get_y = ColReader('label'), # A function taking in one item and extracting the label(s)\n",
    "        splits = None, # Indexs to split the data from\n",
    "        tokenizer_name:str = None, # The string name of a `HuggingFace` tokenizer or model. If `None`, will not tokenize the dataset.\n",
    "        tokenize_func:callable = None, # Optional custom tokenize function for a single item, such as `def _inner(item): return self.tokenizer(item['text'])`\n",
    "        tokenize:bool = True, # Whether to tokenize the dataset immediatly\n",
    "        tokenize_kwargs:dict = {'padding':True}, # Some kwargs for when we call the tokenizer\n",
    "        auto_kwargs:dict = {}, # Some kwargs when calling `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        xs = L(L(items).map(get_x)[0].values, use_list=True)\n",
    "        ys = L(L(items).map(get_y)[0].values, use_list=True)\n",
    "        self.categorize = Categorize(ys)\n",
    "        ys = L([self.categorize(y) for y in ys], use_list=True)\n",
    "        train_xs, train_ys = xs[splits[0]], ys[splits[0]]\n",
    "        valid_xs, valid_ys = xs[splits[1]], ys[splits[1]]\n",
    "        \n",
    "        train_dset = Dataset.from_dict({\n",
    "            'text':train_xs,\n",
    "            'labels':train_ys\n",
    "        })\n",
    "        \n",
    "        valid_dset = Dataset.from_dict({\n",
    "            'text':valid_xs,\n",
    "            'labels':valid_ys\n",
    "        })\n",
    "        \n",
    "        super().__init__(train_dset, valid_dset, tokenizer_name, tokenize, tokenize_func, tokenize_kwargs, auto_kwargs)\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def from_df(\n",
    "        cls,\n",
    "        df:pd.DataFrame, # A Pandas Dataframe or Path to a DataFrame\n",
    "        text_col:str = 'text', # Name of the column the text is stored\n",
    "        label_col:str = 'labels', # Name of the column the label(s) are stored\n",
    "        splits = None, # Indexes to split the data with\n",
    "        tokenizer_name:str = None, # The string name of a `HuggingFace` tokenizer or model. If `None`, will not tokenize the dataset.\n",
    "        tokenize:bool = True, # Whether to tokenize the dataset immediatly\n",
    "        tokenize_func:callable = None, # Optional custom tokenize function for a single item, such as `def _inner(item): return self.tokenizer(item['text'])`\n",
    "        tokenize_kwargs:dict = {'padding':True}, # Some kwargs for when we call the tokenizer\n",
    "        auto_kwargs:dict = {}, # Some kwargs when calling `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        \"Builds `SequenceClassificationDatasets` from a `DataFrame` or file path\"\n",
    "        get_x = ColReader(text_col)\n",
    "        get_y = ColReader(label_col)\n",
    "        if splits is None: splits = RandomSplitter(0.2)(range_of(df))\n",
    "        return cls(df, get_x, get_y, splits, tokenizer_name, tokenize_func, tokenize, tokenize_kwargs, auto_kwargs)\n",
    "    \n",
    "    @delegates(DataLoaders)\n",
    "    def dataloaders(\n",
    "        self, \n",
    "        batch_size=8, # A batch size\n",
    "        shuffle_train=True, # Whether to shuffle the training dataset\n",
    "        collate_fn = None, # A custom collation function\n",
    "        **kwargs): # Torch DataLoader kwargs\n",
    "        dls = super().dataloaders(batch_size, shuffle_train, collate_fn, **kwargs)\n",
    "        dls[0].categorize = self.categorize\n",
    "        return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ec21f",
   "metadata": {},
   "source": [
    "Currently `SequenceClassificationDatasets` only supports reading in data from a `DataFrame` or similar item.\n",
    "\n",
    "When passing in kwargs if anything should go to the `tokenize` function they should go to `tokenize_kwargs`, and if it should go to the `Auto` class constructor, they should go to `auto_kwargs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b587259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fb77ea8c28411a90da774822bdbe26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7a74fdfd5afa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muntar_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURLs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMDB_SAMPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'texts.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m dset = SequenceClassificationDatasets.from_df(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6818f65ada43>\u001b[0m in \u001b[0;36mfrom_df\u001b[0;34m(cls, df, text_col, label_col, splits, tokenizer_name, tokenize, tokenize_func, tokenize_kwargs, auto_kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mget_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdelegates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataLoaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6818f65ada43>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, items, get_x, get_y, splits, tokenizer_name, tokenize_func, tokenize, tokenize_kwargs, auto_kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         })\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/adaptnlp/adaptnlp/training/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_dset, valid_dset, tokenizer_name, tokenize, tokenize_func, tokenize_kwargs, auto_kwargs, remove_cols)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sequence length set to: {tokenize_kwargs['max_length']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtokenize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtokenize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to tokenize a dataset without a tokenizer. Please set a tokenizer with `set_tokenizer` and call `_tokenize()`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/adaptnlp/adaptnlp/training/core.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to tokenize a dataset without a tokenizer. Please add a tokenizer with `set_tokenizer(tokenizer_name` and try again\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mremove_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mremove_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m             return self._map_single(\n\u001b[0m\u001b[1;32m   1620\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         }\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, desc)\u001b[0m\n\u001b[1;32m   1957\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   1958\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   1960\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                 \u001b[0meffective_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m             processed_inputs = (\n\u001b[0;32m-> 1855\u001b[0;31m                 \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwith_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m             )\n\u001b[1;32m   1857\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/adaptnlp/adaptnlp/training/core.py\u001b[0m in \u001b[0;36m_tokenize_inner\u001b[0;34m(item, tokenizer, tokenize_kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_tokenize_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenize_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2262\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2263\u001b[0m                 \u001b[0;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2264\u001b[0m                 \u001b[0;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "from fastai.data.external import URLs, untar_data\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "dset = SequenceClassificationDatasets.from_df(\n",
    "    df,\n",
    "    'text',\n",
    "    'label',\n",
    "    tokenizer_name = \"bert-base-uncased\",\n",
    "    tokenize=True,\n",
    "    tokenize_kwargs = {'max_length':512, 'truncation':True}\n",
    ")\n",
    "\n",
    "test_eq(len(dset.train), 800)\n",
    "test_eq(len(dset.valid), 200)\n",
    "test_eq(len(dset.train[0]), 4) \n",
    "test_eq(dset.train[0].keys(), ['attention_mask', 'input_ids', 'labels', 'token_type_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db08ec",
   "metadata": {},
   "source": [
    "## Sequence Classification Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f068bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceClassificationTuner(AdaptiveTuner):\n",
    "    \"\"\"\n",
    "    An `AdaptiveTuner` with good defaults for Sequence Classification tasks\n",
    "    \n",
    "    **Valid kwargs and defaults:**\n",
    "      - `lr`:float = 0.001\n",
    "      - `splitter`:function = `trainable_params`\n",
    "      - `cbs`:list = None\n",
    "      - `path`:Path = None\n",
    "      - `model_dir`:Path = 'models'\n",
    "      - `wd`:float = None\n",
    "      - `wd_bn_bias`:bool = False\n",
    "      - `train_bn`:bool = True\n",
    "      - `moms`: tuple(float) = (0.95, 0.85, 0.95)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dls:DataLoaders, # A set of DataLoaders\n",
    "        model_name:str, # A HuggingFace model\n",
    "        tokenizer = None, # A HuggingFace tokenizer\n",
    "        loss_func = CrossEntropyLossFlat(), # A loss function\n",
    "        metrics = [accuracy, F1Score()], # Metrics to monitor the training with\n",
    "        opt_func = Adam, # A fastai or torch Optimizer\n",
    "        additional_cbs = None, # Additional Callbacks to have always tied to the Tuner,\n",
    "        expose_fastai_api = False, # Whether to expose the fastai API\n",
    "        num_classes:int=None, # The number of classes\n",
    "        **kwargs, # kwargs for `Learner.__init__`\n",
    "    ):\n",
    "        additional_cbs = listify(additional_cbs)\n",
    "        for arg in 'dls,model,loss_func,metrics,opt_func,cbs,expose_fastai'.split(','): \n",
    "            if arg in kwargs.keys(): kwargs.pop(arg) # Pop all existing kwargs\n",
    "        if hasattr(dls[0], 'categorize'): num_classes = getattr(dls[0].categorize, 'classes', None)\n",
    "        if num_classes is None: raise ValueError(\"Could not extrapolate number of classes, please pass it in as a param\")\n",
    "        if not isinstance(num_classes, int): num_classes = len(num_classes)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "        if tokenizer is None: tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        super().__init__(\n",
    "            expose_fastai_api,\n",
    "            dls = dls, \n",
    "            model = model, \n",
    "            tokenizer = tokenizer,\n",
    "            loss_func = loss_func, \n",
    "            metrics = metrics, \n",
    "            opt_func = opt_func, \n",
    "            cbs=additional_cbs, \n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    @delegates(Learner.__init__)\n",
    "    @classmethod\n",
    "    def from_df(\n",
    "        cls,\n",
    "        df:pd.DataFrame, # A Pandas Dataframe or Path to a DataFrame\n",
    "        text_col:str = 'text', # Name of the column the text is stored\n",
    "        label_col:str = 'labels', # Name of the column the label(s) are stored\n",
    "        model_name:str = None, # The string name of a huggingFace model\n",
    "        split_func:callable = RandomSplitter(), # A function which splits the data\n",
    "        loss_func = CrossEntropyLossFlat(), # A loss function\n",
    "        metrics = [accuracy, F1Score()], # Metrics to monitor the training with\n",
    "        batch_size=8, # A batch size\n",
    "        collate_fn=default_data_collator, # An optional custom collate function\n",
    "        opt_func = Adam, # A fastai or torch Optimizer\n",
    "        additional_cbs = None, # Additional Callbacks to have always tied to the Tuner,\n",
    "        expose_fastai_api = False, # Whether to expose the fastai API\n",
    "        tokenize_func:callable = None, # Optional custom tokenize function for a single item, such as `def _inner(item): return self.tokenizer(item['text'])`\n",
    "        tokenize_kwargs:dict = {'padding':True}, # Some kwargs for when we call the tokenizer\n",
    "        auto_kwargs:dict = {}, # Some kwargs when calling `AutoTokenizer.from_pretrained`\n",
    "        **kwargs # Learner kwargs\n",
    "    ):\n",
    "        \"Convience method to build a `SequenceClassificationTuner` from a Pandas Dataframe\"\n",
    "        try:\n",
    "            splits = split_func(df)\n",
    "        except:\n",
    "            splits = split_func(range_of(df))\n",
    "        dset = SequenceClassificationDatasets.from_df(\n",
    "            df,\n",
    "            text_col,\n",
    "            label_col,\n",
    "            splits,\n",
    "            tokenizer_name=model_name,\n",
    "            tokenize_kwargs=tokenize_kwargs,\n",
    "            auto_kwargs=auto_kwargs,\n",
    "            tokenize_func=tokenize_func\n",
    "        )\n",
    "        \n",
    "        tokenizer = dset.tokenizer\n",
    "        \n",
    "        dls = dset.dataloaders(batch_size, collate_fn)\n",
    "        \n",
    "        return cls(dls, model_name, tokenizer, loss_func, metrics, opt_func, additional_cbs, expose_fastai_api)\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        text:Union[List[str], str], # Some text or list of texts to do inference with\n",
    "        bs:int=64, # A batch size to use for multiple texts\n",
    "        detail_level:DetailLevel = DetailLevel.Low, # A detail level to return on the predictions\n",
    "    ):\n",
    "        \"Predict some `text` for sequence classification with the currently loaded model\"\n",
    "        if getattr(self, '_inferencer', None) is None: self._inferencer = TransformersSequenceClassifier(self.tokenizer, self.model)\n",
    "        preds = self._inferencer.predict(text,bs)\n",
    "        cat = getattr(self.dls, 'categorize', None)\n",
    "        vocab = cat.classes if cat is not None else None\n",
    "        return SequenceResult(preds, vocab).to_dict(detail_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9910daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SequenceClassificationTuner.from_df\" class=\"doc_header\"><code>SequenceClassificationTuner.from_df</code><a href=\"__main__.py#L48\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SequenceClassificationTuner.from_df</code>(**`df`**:`DataFrame`, **`text_col`**:`str`=*`'text'`*, **`label_col`**:`str`=*`'labels'`*, **`model_name`**:`str`=*`None`*, **`split_func`**:`callable`=*`_inner`*, **`loss_func`**=*`CrossEntropyLoss()`*, **`metrics`**=*`[<function accuracy at 0x7fd44c491550>, <fastai.metrics.AccumMetric object at 0x7fd43c9a0070>]`*, **`batch_size`**=*`8`*, **`collate_fn`**=*`default_data_collator`*, **`opt_func`**=*`Adam`*, **`additional_cbs`**=*`None`*, **`expose_fastai_api`**=*`False`*, **`tokenize_kwargs`**:`dict`=*`{'padding': True}`*, **`auto_kwargs`**:`dict`=*`{}`*, **`lr`**=*`0.001`*, **`splitter`**=*`trainable_params`*, **`cbs`**=*`None`*, **`path`**=*`None`*, **`model_dir`**=*`'models'`*, **`wd`**=*`None`*, **`wd_bn_bias`**=*`False`*, **`train_bn`**=*`True`*, **`moms`**=*`(0.95, 0.85, 0.95)`*)\n",
       "\n",
       "Convience method to build a [`SequenceClassificationTuner`](/adaptnlptraining.sequence_classification.html#SequenceClassificationTuner) from a Pandas Dataframe\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SequenceClassificationTuner.from_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9659e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SequenceClassificationTuner.predict\" class=\"doc_header\"><code>SequenceClassificationTuner.predict</code><a href=\"__main__.py#L85\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SequenceClassificationTuner.predict</code>(**`text`**:`Union`\\[`List`\\[`str`\\], `str`\\], **`bs`**:`int`=*`64`*)\n",
       "\n",
       "Predict some `text` for sequence classification with the currently loaded model\n",
       "\n",
       "**Function Arguments**:\n",
       "* `text` (`Union[List[str], str]`): Some text or list of texts to do inference with\n",
       "* `bs` (`int`): A batch size to use for multiple texts\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SequenceClassificationTuner.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a0a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
