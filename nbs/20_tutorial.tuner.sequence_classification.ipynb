{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf91e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f98c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107d7b0",
   "metadata": {},
   "source": [
    "# Tutorial: Sequence Classification Tuning\n",
    "> A tutorial following the fine-tuning and data API for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6957532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp.training.core import Strategy, ColReader, RandomSplitter\n",
    "from adaptnlp.training.sequence_classification import SequenceClassificationTuner, SequenceClassificationDatasets\n",
    "\n",
    "from fastai.data.transforms import ColSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fbb260",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this tutorial we will cover building a dataset with the AdaptNLP data API for Sequence Classification, and fine-tuning a model towards that dataset. \n",
    "\n",
    "Throughout the tutorial you will notice we call from `fastai`, this is because the tuning API is built upon their library \n",
    "\n",
    "> Note: Eventually this will change to `fastai_minima`\n",
    "\n",
    "For our task, we will use the `IMDB_SAMPLE` dataset to help us tune a BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f919953",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "The first thing we need to do is get some data. We will use fastai's handy `untar_data` function and `URLs` class to download it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.external import untar_data, URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534fcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [Path('/root/.fastai/data/imdb_sample/texts.csv')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE); path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff93b2",
   "metadata": {},
   "source": [
    "As you can see, we get a `Path` object showing where the data is stored. Let's open it in `pandas`, as currently the API only supports reading in through `DataFrame`'s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6caba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b6a9b",
   "metadata": {},
   "source": [
    "So we have a `label`, `text`, and an `is_valid` column. Let's see how we can frame this into AdaptNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3daa061",
   "metadata": {},
   "source": [
    "## High Level API\n",
    "\n",
    "Each `*Tuner` class comes with a factory method to prepare your dataset and generate the `Tuner` class at once. If you want a more exposed API, the next section will introduce you to the mid-level API. Let's see the doc for our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33599e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SequenceClassificationTuner.from_df\" class=\"doc_header\"><code>SequenceClassificationTuner.from_df</code><a href=\"https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/sequence_classification.py#L139\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SequenceClassificationTuner.from_df</code>(**`df`**:`DataFrame`, **`text_col`**:`str`=*`'text'`*, **`label_col`**:`str`=*`'labels'`*, **`model_name`**:`str`=*`None`*, **`split_func`**:`callable`=*`_inner`*, **`loss_func`**=*`CrossEntropyLoss()`*, **`metrics`**=*`[<function accuracy at 0x7ff24d17e820>, <fastai.metrics.AccumMetric object at 0x7ff239ec28b0>]`*, **`batch_size`**=*`8`*, **`collate_fn`**=*`default_data_collator`*, **`opt_func`**=*`Adam`*, **`additional_cbs`**=*`None`*, **`expose_fastai_api`**=*`False`*, **`tokenize_func`**:`callable`=*`None`*, **`tokenize_kwargs`**:`dict`=*`{'padding': True}`*, **`auto_kwargs`**:`dict`=*`{}`*, **`lr`**=*`0.001`*, **`splitter`**=*`trainable_params`*, **`cbs`**=*`None`*, **`path`**=*`None`*, **`model_dir`**=*`'models'`*, **`wd`**=*`None`*, **`wd_bn_bias`**=*`False`*, **`train_bn`**=*`True`*, **`moms`**=*`(0.95, 0.85, 0.95)`*)\n",
       "\n",
       "Convience method to build a [`SequenceClassificationTuner`](/adaptnlptraining.sequence_classification.html#SequenceClassificationTuner) from a Pandas Dataframe\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SequenceClassificationTuner.from_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d869589",
   "metadata": {},
   "source": [
    "We can see that it wants a `DataFrame`, the name of the text column, the name of the label column, the name of a transformers model, potentially a way to split the data, and some other configurable bits for training (such as loss function, optimizers, etc).\n",
    "\n",
    "We can also pass in specific kwargs to both the `tokenize()` function (such as `padding`, `truncation`, etc) and the `AutoTokenizer.from_pretrained` constructor (`auto_kwargs`)\n",
    "\n",
    "The defaults for the high-level API will work out of the box for their task. In this case our loss function is CrossEntropy, we have `accuracy` and `F1Score` as our metrics, and our optimizer is `Adam`. Let's use these defaults and build a `Tuner`:\n",
    "\n",
    "> Note: `ColSplitter` checks if the `is_valid` column is `True`\n",
    "\n",
    "We also need to write a tokenization function, which tokenizes our text for us. It should take in a single item for us to grab the text from, a `tokenizer`, and some `tokenize_kwargs`. When in use, the class will pull `tokenizer` and `tokenize_kwargs` from `self.tokenizer` and `self.tokenize_kwargs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(item, tokenizer, tokenize_kwargs): return tokenizer(item['text'], **tokenize_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee57eeb9",
   "metadata": {},
   "source": [
    "In our case, we want to tokenize only the column `text`, and then pass in any `tokenize_kwargs` that were passed in `SequenceClassificationTuner.from_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a9acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No value for `max_length` set, automatically adjusting to the size of the model and including truncation\n",
      "Sequence length set to: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9af6d6f83c94c3ebc2c34336f1550b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e612869a1d84cc898167d9a5e2deaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tuner = SequenceClassificationTuner.from_df(\n",
    "    df,\n",
    "    text_col='text',\n",
    "    label_col='label',\n",
    "    model_name='bert-base-uncased',\n",
    "    split_func=ColSplitter('is_valid'),\n",
    "    tokenize_func=tok_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a918d29b",
   "metadata": {},
   "source": [
    "What this did for us is first tokenize our datasets into a `transformers.Dataset`, then it built our `DataLoaders`, before finally setting up a sequence classification model for us to tune with. \n",
    "\n",
    "All that's left now is to train! \n",
    "\n",
    "We can use the `lr_find` method to find a good learning rate to use, and the use `tune` to train for a few epochs:\n",
    "\n",
    "> Note: `lr_find` is fastai's Learning Rate Finder, you can read the documentation on it [here](https://docs.fast.ai/callback.schedule#Learner.lr_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5062cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tuner.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c35da8d",
   "metadata": {},
   "source": [
    "We can then see the recommended learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d980c",
   "metadata": {},
   "source": [
    "And now let's use it and tune:\n",
    "\n",
    "> Note: By default it will use the `OneCyclePolicy`, but `CosineAnnealing` and `SGDR` are also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.tune(\n",
    "    epochs=3,\n",
    "    lr=float(lr.valley),\n",
    "    strategy=Strategy.OneCycle\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377fbc9",
   "metadata": {},
   "source": [
    "Since we now have a trained model, let's try and predict with it! We have access to a `.predict` method, which will hand over inference to the adaptnlp inference API:\n",
    "\n",
    "> Note: this is mostly aimed at seeing how your model performed, for inference you should still call the direct inference API shown [here](https://novetta.github.io/adaptnlp/sequence_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"This movie is garbage! \n",
    "It even had Matt Damon and Michael Keaton in the lead roles, but it was still terrible. \n",
    "They couldn't even save it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SequenceClassificationTuner.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.predict(review, detail_level='low')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545501c",
   "metadata": {},
   "source": [
    "And now you're done, with only three lines of code! Let's try and convert that over to the mid-level API, and remove some of that (heavy) abstraction next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92760cd0",
   "metadata": {},
   "source": [
    "## Mid-Level API\n",
    "\n",
    "Let's see now how to recreate that high level API, with the `Datasets` and `Tuner` APIs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be3799",
   "metadata": {},
   "source": [
    "We'll use the `SequenceClassificationDatasets.from_df` helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75623dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(SequenceClassificationDatasets.from_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08bb97",
   "metadata": {},
   "source": [
    "As we can see, it expects a `DataFrame`, a text column name our text is stored at, a label column name, splits, and a tokenizer model.\n",
    "\n",
    "First let's look at how to write our splits. \n",
    "\n",
    "These should be in the format of a list of indicies, dictating what goes to the training, and what goes to the validation sets such as: `[[0,1,2],[3,4,5]]`.\n",
    "\n",
    "Since we have an `is_valid` column, we will bring in `fastai`'s `ColSplitter`, which can read in that `is_valid` column and generate some indicies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6df8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.transforms import ColSplitter\n",
    "\n",
    "splitter = ColSplitter('is_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ce2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = splitter(df); splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2eafa",
   "metadata": {},
   "source": [
    "We can see that 800 indicies went to `[0]`, and 200 indicies went to `[1]`. Now that we have some splits, let's build and tokenize our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a008c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = SequenceClassificationDatasets.from_df(\n",
    "    df = df,\n",
    "    text_col = 'text',\n",
    "    label_col = 'label',\n",
    "    splits = splits,\n",
    "    tokenizer_name = 'bert-base-uncased',\n",
    "    tokenize = True,\n",
    "    tokenize_func = tok_func,\n",
    "    tokenize_kwargs = {'padding':True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8535d366",
   "metadata": {},
   "source": [
    "Once we have our tokenized `Dataset`, all we need to do is build our `DataLoaders`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dset.dataloaders(batch_size=8, shuffle_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af1f29",
   "metadata": {},
   "source": [
    "Let's make sure a batch looks okay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dls[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac36e44",
   "metadata": {},
   "source": [
    "Looks great! Next let's move onto our Tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512543b",
   "metadata": {},
   "source": [
    "## Tuning a Model\n",
    "\n",
    "Next we need to tune a model to our dataset! For this we will use the `SequenceClassificationTuner`, which has some good defaults for our task such as metrics and a loss function to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41969d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = SequenceClassificationTuner(dls, model_name='bert-base-uncased', tokenizer=dset.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb679b34",
   "metadata": {},
   "source": [
    "And now we can `tune`! Each `Tuner` class has 6 functions you can call:\n",
    "* `lr_find`\n",
    "* `tune`\n",
    "* `save`\n",
    "* `load`\n",
    "* `predict`\n",
    "\n",
    "> Note: if you want full access to the fastai `Learner`, add `expose_fastai_api=True` to the constructor, and the tuner will expose everything a `Learner` does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ea007",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tuner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95032393",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.valley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02894c",
   "metadata": {},
   "source": [
    "And now we can fit with the `OneCycle` policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.tune(\n",
    "    epochs=3, \n",
    "    lr=float(lr.valley),\n",
    "    strategy=Strategy.OneCycle\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501ec08",
   "metadata": {},
   "source": [
    "Finally we can `save` or `load` our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.save('tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.load('tuned_model');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37621ff2",
   "metadata": {},
   "source": [
    "## Using the AdaptNLP Inference API On Our Trained Model\n",
    "\n",
    "Another option is to use the in-house inference API on that model we just trained. Let's learn how to use it!\n",
    "\n",
    "First, let's get rid of our `tuner` and clear some memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef32bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tuner\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763dd48",
   "metadata": {},
   "source": [
    "Next we'll import `AdaptNLP`'s `EasySequenceClassifier` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp.sequence_classification import EasySequenceClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140cc4e0",
   "metadata": {},
   "source": [
    "Build one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ade32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = EasySequenceClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6191d1",
   "metadata": {},
   "source": [
    "And call `tag_text`, passing in our saved model to point at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(classifier.tag_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0568c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.tag_text(\n",
    "    review,\n",
    "    model_name_or_path='tuned_model'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
