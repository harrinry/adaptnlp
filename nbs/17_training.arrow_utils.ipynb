{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training.arrow_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08307357",
   "metadata": {},
   "source": [
    "# Arrow Utils\n",
    "> Various pyarrow utility functions, such as custom dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c6901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Optional\n",
    "\n",
    "from datasets import Features, NamedSplit\n",
    "from datasets.packaged_modules.text.text import Text\n",
    "from datasets.utils.typing import NestedDataStructureLike, PathLike\n",
    "from datasets.io.abc import AbstractDatasetReader\n",
    "import datasets\n",
    "from datasets.packaged_modules.text.text import TextConfig\n",
    "\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TextNoNewLine(datasets.ArrowBasedBuilder):\n",
    "    BUILDER_CONFIG_CLASS = TextConfig\n",
    "\n",
    "    def _info(self):\n",
    "        return datasets.DatasetInfo(features=self.config.features)\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        \"\"\"The `data_files` kwarg in load_dataset() can be a str, List[str], Dict[str,str], or Dict[str,List[str]].\n",
    "        If str or List[str], then the dataset returns only the 'train' split.\n",
    "        If dict, then keys should be from the `datasets.Split` enum.\n",
    "        \"\"\"\n",
    "        if not self.config.data_files:\n",
    "            raise ValueError(f\"At least one data file must be specified, but got data_files={self.config.data_files}\")\n",
    "        data_files = dl_manager.download_and_extract(self.config.data_files)\n",
    "        if isinstance(data_files, (str, list, tuple)):\n",
    "            files = data_files\n",
    "            if isinstance(files, str):\n",
    "                files = [files]\n",
    "            return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"files\": files})]\n",
    "        splits = []\n",
    "        for split_name, files in data_files.items():\n",
    "            if isinstance(files, str):\n",
    "                files = [files]\n",
    "            splits.append(datasets.SplitGenerator(name=split_name, gen_kwargs={\"files\": files}))\n",
    "        return splits\n",
    "\n",
    "    def _generate_tables(self, files):\n",
    "        schema = pa.schema(self.config.features.type if self.config.features is not None else {\"text\": pa.string()})\n",
    "        for file_idx, file in enumerate(files):\n",
    "            batch_idx = 0\n",
    "            with open(file, \"r\", encoding=self.config.encoding) as f:\n",
    "                while True:\n",
    "                    batch = f.read(self.config.chunksize)\n",
    "                    if not batch:\n",
    "                        break\n",
    "                    batch = [batch]\n",
    "                    pa_table = pa.Table.from_arrays([pa.array(batch)], schema=schema)\n",
    "                    # Uncomment for debugging (will print the Arrow table size and elements)\n",
    "                    # logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\n",
    "                    # logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\n",
    "                    yield (file_idx, batch_idx), pa_table\n",
    "                    batch_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TextNoNewLineDatasetReader(AbstractDatasetReader):\n",
    "    \"A `DatasetReader` class that mimics `TextDatasetReader`, but uses `TextNoNewLine`\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_or_paths: NestedDataStructureLike[PathLike],\n",
    "        split: Optional[NamedSplit] = None,\n",
    "        features: Optional[Features] = None,\n",
    "        cache_dir: str = None,\n",
    "        keep_in_memory: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            path_or_paths, split=split, features=features, cache_dir=cache_dir, keep_in_memory=keep_in_memory, **kwargs\n",
    "        )\n",
    "        path_or_paths = path_or_paths if isinstance(path_or_paths, dict) else {self.split: path_or_paths}\n",
    "        self.builder = TextNoNewLine(\n",
    "            cache_dir=cache_dir,\n",
    "            data_files=path_or_paths,\n",
    "            features=features,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def read(self):\n",
    "        download_config = None\n",
    "        download_mode = None\n",
    "        ignore_verifications = False\n",
    "        use_auth_token = None\n",
    "        base_path = None\n",
    "\n",
    "        self.builder.download_and_prepare(\n",
    "            download_config=download_config,\n",
    "            download_mode=download_mode,\n",
    "            ignore_verifications=ignore_verifications,\n",
    "            # try_from_hf_gcs=try_from_hf_gcs,\n",
    "            base_path=base_path,\n",
    "            use_auth_token=use_auth_token,\n",
    "        )\n",
    "\n",
    "        # Build dataset for splits\n",
    "        dataset = self.builder.as_dataset(\n",
    "            split=self.split, ignore_verifications=ignore_verifications, in_memory=self.keep_in_memory\n",
    "        )\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423a376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
