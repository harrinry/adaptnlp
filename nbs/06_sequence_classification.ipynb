{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp inference.sequence_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification\n",
    "> Sequence Classification API for Transformers and Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from typing import List, Dict, Union, Tuple, Callable\n",
    "from collections import defaultdict, OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import datasets\n",
    "from datasets import ClassLabel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from flair.data import Sentence, DataPoint\n",
    "from flair.models import TextClassifier\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    BertPreTrainedModel,\n",
    "    DistilBertPreTrainedModel,\n",
    "    XLMPreTrainedModel,\n",
    "    XLNetPreTrainedModel,\n",
    "    ElectraPreTrainedModel,\n",
    "    BertForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from adaptnlp.model import AdaptiveModel\n",
    "from adaptnlp.model_hub import HFModelResult, FlairModelResult\n",
    "\n",
    "from fastcore.basics import risinstance\n",
    "from fastcore.xtras import Path\n",
    "from adaptnlp.result import DetailLevel, SentenceResult\n",
    "\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceResult(SentenceResult):\n",
    "    \"\"\"\n",
    "    A result class designed for Sequence Classification models\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences:List[Sentence], class_names:list = None):\n",
    "        super().__init__(sentences)\n",
    "        self.classes = sentences[0].get_label_names()\n",
    "        self.class_names = class_names\n",
    "\n",
    "    @property\n",
    "    def probabilities(self) -> List[List[tensor]]:\n",
    "        \"\"\"\n",
    "        The probabilities returned for each classification\n",
    "        \"\"\"\n",
    "        return torch.stack([tensor(list(map(lambda x: x.score, i.get_labels()))) for i in self._sentences], dim=0)\n",
    "\n",
    "    @property\n",
    "    def predictions(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        A list of the best classification for each input\n",
    "        \"\"\"\n",
    "        if self.class_names is not None:\n",
    "            return [self.class_names[p.argmax()] for p in self.probabilities]\n",
    "        return [max(s.labels, key=lambda x: x.score).value for s in self._sentences]\n",
    "\n",
    "    def to_dict(self, detail_level:DetailLevel=DetailLevel.Low):\n",
    "        \"\"\"\n",
    "        Returns details about `self` at various detail levels\n",
    "        \"\"\"\n",
    "        o = {\n",
    "            'sentences':self.inputs,\n",
    "            'predictions':self.predictions,\n",
    "            'probs':self.probabilities\n",
    "        }\n",
    "        if detail_level == 'medium' or detail_level == 'high':\n",
    "            # Add a dictionary of sentence and probabilities, and return the vocab\n",
    "            o['pairings'] = OrderedDict({\n",
    "                s:probs for (s,probs) in zip(self.inputs, self.probabilities)\n",
    "            })\n",
    "            o['classes'] = self.classes\n",
    "\n",
    "        if detail_level == 'high':\n",
    "            # Add original `Sentences`\n",
    "            o['sentences'] = self._sentences\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersSequenceClassifier(AdaptiveModel):\n",
    "    \"\"\"Adaptive model for Transformer's Sequence Classification Model\n",
    "\n",
    "    Usage:\n",
    "    ```python\n",
    "    >>> classifier = TransformersSequenceClassifier.load('transformers-sc-model')\n",
    "    >>> classifier.predict(text='Example text', mini_batch_size=32)\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **tokenizer** - A tokenizer object from Huggingface's transformers (TODO)and tokenizers\n",
    "    * **model** - A transformers Sequence Classsifciation model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n",
    "        # Load up model and tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        super().__init__()\n",
    "        self.set_model(model)\n",
    "\n",
    "        # Load empty trainer\n",
    "        self.trainer = None\n",
    "\n",
    "        # Setup cuda and automatic allocation of model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: Union[HFModelResult, str]) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading and constructing this classifier\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Transformer's pre-trained Sequence Classifier Model or a `HFModelResult`\n",
    "        \"\"\"\n",
    "        if isinstance(model_name_or_path, HFModelResult): model_name_or_path = model_name_or_path.name\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
    "        classifier = cls(tokenizer, model)\n",
    "        return classifier\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers classifier\n",
    "        \"\"\"\n",
    "        id2label = self.model.config.id2label\n",
    "        sentences = text\n",
    "        results: List[Sentence] = []\n",
    "\n",
    "        if not sentences: return sentences\n",
    "\n",
    "        if risinstance([DataPoint, str], sentences):\n",
    "            sentences = [sentences]\n",
    "\n",
    "        # filter empty sentences\n",
    "        if isinstance(sentences[0], Sentence):\n",
    "            sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "        if len(sentences) == 0:\n",
    "            return sentences\n",
    "\n",
    "        # reverse sort all sequences by their length\n",
    "        rev_order_len_index = sorted(\n",
    "            range(len(sentences)), key=lambda k: len(sentences[k]), reverse=True\n",
    "        )\n",
    "        original_order_index = sorted(\n",
    "            range(len(rev_order_len_index)), key=lambda k: rev_order_len_index[k]\n",
    "        )\n",
    "\n",
    "        reordered_sentences: List[Union[DataPoint, str]] = [\n",
    "            sentences[index] for index in rev_order_len_index\n",
    "        ]\n",
    "\n",
    "        # Turn all Sentence objects into strings\n",
    "        if isinstance(reordered_sentences[0], Sentence):\n",
    "            str_reordered_sentences = [\n",
    "                sentence.to_original_text() for sentence in sentences\n",
    "            ]\n",
    "        else:\n",
    "            str_reordered_sentences = reordered_sentences\n",
    "\n",
    "        dataset = self._tokenize(str_reordered_sentences)\n",
    "        dl = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "        predictions: List[Tuple[str, float]] = []\n",
    "\n",
    "        outputs, _ = super().get_preds(dl=dl)\n",
    "        logits = torch.cat([o['logits'] for o in outputs])\n",
    "        predictions = torch.softmax(logits, dim=1).tolist()\n",
    "\n",
    "        for text, pred in zip(str_reordered_sentences, predictions):\n",
    "            # Initialize and assign labels to each class in each datapoint prediction\n",
    "            text_sent = Sentence(text)\n",
    "            for k, v in id2label.items():\n",
    "                text_sent.add_label(label_type='sc', value=v, score=pred[k])\n",
    "            results.append(text_sent)\n",
    "\n",
    "        # Order results back into original order\n",
    "        results = [results[index] for index in original_order_index]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _tokenize(\n",
    "        self, sentences: Union[List[Sentence], Sentence, List[str], str]\n",
    "    ) -> TensorDataset:\n",
    "        \"\"\" Batch tokenizes text and produces a `TensorDataset` with them \"\"\"\n",
    "\n",
    "        # TODO: __call__ from tokenizer base class in the transformers library could automate/handle this\n",
    "        tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "            sentences,\n",
    "            return_tensors='pt',\n",
    "            max_length=None,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        # Bart, XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use token_type_ids\n",
    "        if isinstance(\n",
    "            self.model,\n",
    "            (\n",
    "                BertForSequenceClassification,\n",
    "                XLNetForSequenceClassification,\n",
    "                AlbertForSequenceClassification,\n",
    "            ),\n",
    "        ):\n",
    "            dataset = TensorDataset(\n",
    "                tokenized_text['input_ids'],\n",
    "                tokenized_text['attention_mask'],\n",
    "                tokenized_text['token_type_ids'],\n",
    "            )\n",
    "        else:\n",
    "            dataset = TensorDataset(\n",
    "                tokenized_text['input_ids'], tokenized_text['attention_mask']\n",
    "            )\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2104: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "import flair\n",
    "example_text = \"This didn't work at all\"\n",
    "\n",
    "classifier = TransformersSequenceClassifier.load(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "sentences = classifier.predict(text=example_text,mini_batch_size=1,\n",
    ")\n",
    "\n",
    "preds = sentences[0].get_labels()\n",
    "\n",
    "truth_lbls = [\n",
    "    flair.data.Label('1 star', 0.8421),\n",
    "    flair.data.Label('2 stars', 0.1379),\n",
    "    flair.data.Label('3 stars', 0.018),\n",
    "    flair.data.Label('4 stars', 0.0012),\n",
    "    flair.data.Label('5 stars', 0.0007)\n",
    "]\n",
    "\n",
    "for pred, truth in zip(preds, truth_lbls):\n",
    "    test_eq(pred.value, truth.value)\n",
    "    test_close(pred.score, truth.score, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FlairSequenceClassifier(AdaptiveModel):\n",
    "    \"\"\"Adaptive Model for Flair's Sequence Classifier...very basic\n",
    "\n",
    "    Usage:\n",
    "    ```python\n",
    "    >>> classifier = FlairSequenceClassifier.load('sentiment')\n",
    "    >>> classifier.predict(text='Example text', mini_batch_size=32)\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **model_name_or_path** - A key string of one of Flair's pre-trained Sequence Classifier Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name_or_path: str):\n",
    "        self.classifier = TextClassifier.load(model_name_or_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: Union[HFModelResult, FlairModelResult, str]) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading a constructing this classifier\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Flair's pre-trained Sequence Classifier Model or a `HFModelResult`\n",
    "        \"\"\"\n",
    "        if risinstance([HFModelResult, FlairModelResult], model_name_or_path): \n",
    "            model_name_or_path = model_name_or_path.name\n",
    "        classifier = cls(model_name_or_path)\n",
    "        return classifier\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Flair classifier\n",
    "        \"\"\"\n",
    "        if isinstance(text, (Sentence, str)):\n",
    "            text = [text]\n",
    "        if isinstance(text[0], str):\n",
    "            text = [Sentence(s) for s in text]\n",
    "\n",
    "        self.classifier.predict(\n",
    "            sentences=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-18 18:07:56,539 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_4.pt not found in cache, downloading to /tmp/tmpu8nq4xsk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265512723/265512723 [00:12<00:00, 21270767.17B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-18 18:08:09,366 copying /tmp/tmpu8nq4xsk to cache at /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-18 18:08:09,622 removing temp file /tmp/tmpu8nq4xsk\n",
      "2021-06-18 18:08:09,650 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ac35a0f4474bfeb7f76e0b9a8b34b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79d31ef4d514c6c8145a454a2ccd362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f0ca47692d4bfda2582f1caf3d088b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import flair\n",
    "example_text = \"This didn't work at all\"\n",
    "\n",
    "\n",
    "classifier = FlairSequenceClassifier.load('sentiment')\n",
    "\n",
    "sentences = classifier.predict(text=example_text,mini_batch_size=1,\n",
    ")\n",
    "\n",
    "pred = sentences[0].get_labels()[0]\n",
    "\n",
    "test_eq(pred.value, 'NEGATIVE')\n",
    "test_close(pred.score, 0.999, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from adaptnlp.model_hub import HFModelHub, FlairModelHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasySequenceClassifier:\n",
    "    \"\"\"Sequence classification models\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> classifier = EasySequenceClassifier()\n",
    "    >>> classifier.tag_text(text='text you want to label', model_name_or_path='en-sentiment')\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sequence_classifiers: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "        self.hf_hub = HFModelHub()\n",
    "        self.flair_hub = FlairModelHub()\n",
    "\n",
    "    def tag_text(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        model_name_or_path: Union[str, FlairModelResult, HFModelResult] = 'en-sentiment',\n",
    "        mini_batch_size: int = 32,\n",
    "        detail_level:DetailLevel = DetailLevel.Low, # A level of detail to return\n",
    "        class_names:list = None, # A list of labels\n",
    "        **kwargs,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Tags a text sequence with labels the sequence classification models have been trained on\n",
    "\n",
    "        * **text** - String, list of strings, `Sentence`, or list of `Sentence`s to be classified\n",
    "        * **model_name_or_path** - The model name key or model path\n",
    "        * **mini_batch_size** - The mini batch size for running inference\n",
    "        * **&ast;&ast;kwargs** - (Optional) Keyword Arguments for Flair's `TextClassifier.predict()` method params\n",
    "        **return** A list of Flair's `Sentence`'s\n",
    "        \"\"\"\n",
    "        # Load Text Classifier Model and Pytorch Module into tagger dict\n",
    "        name = getattr(model_name_or_path, 'name', model_name_or_path)\n",
    "        if not self.sequence_classifiers[name]:\n",
    "            \"\"\"\n",
    "            self.sequence_classifiers[name] = TextClassifier.load(\n",
    "                model_name_or_path\n",
    "            )\n",
    "            \"\"\"\n",
    "            if risinstance([FlairModelResult, HFModelResult], model_name_or_path):\n",
    "                try:\n",
    "                    self.sequence_classifiers[name] = FlairSequenceClassifier.load(name)\n",
    "                except:\n",
    "                    self.sequence_classifiers[name] = TransformersSequenceClassifier.load(name)\n",
    "                    \n",
    "            elif risinstance([str, Path], model_name_or_path) and (Path(model_name_or_path).exists() and Path(model_name_or_path).is_dir()):\n",
    "                # Load in previously existing model\n",
    "                try:\n",
    "                    self.sequence_classifiers[name] = FlairSequenceClassifier.load(name)\n",
    "                except:\n",
    "                    self.sequence_classifiers[name] = TransformersSequenceClassifier.load(name)\n",
    "                \n",
    "            else:\n",
    "                # Flair\n",
    "                res = self.flair_hub.search_model_by_name(name, user_uploaded=True)\n",
    "                if len(res) < 1:\n",
    "                    # No models found\n",
    "                    res = self.hf_hub.search_model_by_name(model_name_or_path, user_uploaded=True)\n",
    "                    if len(res) < 1:\n",
    "                        logger.info(\"Not a valid `model_name_or_path` param\")\n",
    "                        return [Sentence('')]\n",
    "                    else:\n",
    "                        name = res[0].name.replace('flairNLP', 'flair')\n",
    "                        self.sequence_classifiers[res[0].name] = TransformersSequenceClassifier.load(name)\n",
    "                else:\n",
    "                    name = res[0].name.replace('flairNLP/', '')\n",
    "                    self.sequence_classifiers[name] = FlairSequenceClassifier.load(name) # Returning the first should always be non-fast\n",
    "\n",
    "        classifier = self.sequence_classifiers[name]\n",
    "        out = classifier.predict(\n",
    "            text=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if detail_level is None: return out\n",
    "        res = SequenceResult(out, class_names)\n",
    "        return res.to_dict(detail_level)\n",
    "        \n",
    "\n",
    "    def tag_all(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        detail_level:DetailLevel = DetailLevel.Low,\n",
    "        class_names:list = None, # A list of labels\n",
    "        **kwargs,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Tags text with all labels from all sequence classification models\n",
    "\n",
    "        * **text** - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * **mini_batch_size** - The mini batch size for running inference\n",
    "        * **&ast;&ast;kwargs** - (Optional) Keyword Arguments for Flair's `TextClassifier.predict()` method params\n",
    "        * **return** - A list of Flair's `Sentence`'s\n",
    "        \"\"\"\n",
    "        sentences = text\n",
    "        for tagger_name in self.sequence_classifiers.keys():\n",
    "            sentences = self.tag_text(\n",
    "                sentences,\n",
    "                model_name_or_path=tagger_name,\n",
    "                mini_batch_size=mini_batch_size,\n",
    "                detail_level = None\n",
    "                **kwargs,\n",
    "            )\n",
    "        res = SequenceResult(out, class_names)\n",
    "        return res.to_dict(detail_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-11 17:29:34,947 loading file nlptown/bert-base-multilingual-uncased-sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2104: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "hub = HFModelHub()\n",
    "model = hub.search_model_by_name(\"nlptown/bert-base\", user_uploaded=True)[0]\n",
    "classifier = EasySequenceClassifier()\n",
    "sentences = classifier.tag_text(text=example_text, \n",
    "                               model_name_or_path=model,\n",
    "                               mini_batch_size=1)\n",
    "for pred, truth in zip(preds, truth_lbls):\n",
    "    test_eq(pred.value, truth.value)\n",
    "    test_close(pred.score, truth.score, 1e-4)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
