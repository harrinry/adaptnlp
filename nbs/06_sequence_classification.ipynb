{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/venv/lib/python3.8/site-packages (4.5.1)\n",
      "Requirement already satisfied: filelock in /opt/venv/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/venv/lib/python3.8/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: requests in /opt/venv/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /opt/venv/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/venv/lib/python3.8/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/venv/lib/python3.8/site-packages (from transformers) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/venv/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging in /opt/venv/lib/python3.8/site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/venv/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/venv/lib/python3.8/site-packages (from requests->transformers) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/venv/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /opt/venv/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/venv/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp sequence_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification\n",
    "> Sequence Classification API for Transformers and Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from typing import List, Dict, Union, Tuple, Callable\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import datasets\n",
    "from datasets import ClassLabel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from flair.data import Sentence, DataPoint\n",
    "from flair.models import TextClassifier\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    BertPreTrainedModel,\n",
    "    DistilBertPreTrainedModel,\n",
    "    XLMPreTrainedModel,\n",
    "    XLNetPreTrainedModel,\n",
    "    ElectraPreTrainedModel,\n",
    "    BertForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from adaptnlp.model import AdaptiveModel\n",
    "from adaptnlp.model_hub import HFModelResult\n",
    "\n",
    "from fastcore.basics import risinstance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersSequenceClassifier(AdaptiveModel):\n",
    "    \"\"\"Adaptive model for Transformer's Sequence Classification Model\n",
    "\n",
    "    Usage:\n",
    "    ```python\n",
    "    >>> classifier = TransformersSequenceClassifier.load('transformers-sc-model')\n",
    "    >>> classifier.predict(text='Example text', mini_batch_size=32)\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **tokenizer** - A tokenizer object from Huggingface's transformers (TODO)and tokenizers\n",
    "    * **model** - A transformers Sequence Classsifciation model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n",
    "        # Load up model and tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        super().__init__()\n",
    "        self.set_model(model)\n",
    "\n",
    "        # Load empty trainer\n",
    "        self.trainer = None\n",
    "\n",
    "        # Setup cuda and automatic allocation of model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: Union[HFModelResult, str]) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading and constructing this classifier\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Transformer's pre-trained Sequence Classifier Model or a `HFModelResult`\n",
    "        \"\"\"\n",
    "        if isinstance(model_name_or_path, HFModelResult): model_name_or_path = model_name_or_path.name\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
    "        classifier = cls(tokenizer, model)\n",
    "        return classifier\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers classifier\n",
    "        \"\"\"\n",
    "        id2label = self.model.config.id2label\n",
    "        sentences = text\n",
    "        results: List[Sentence] = []\n",
    "\n",
    "        if not sentences: return sentences\n",
    "\n",
    "        if risinstance([DataPoint, str], sentences):\n",
    "            sentences = [sentences]\n",
    "\n",
    "        # filter empty sentences\n",
    "        if isinstance(sentences[0], Sentence):\n",
    "            sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "        if len(sentences) == 0:\n",
    "            return sentences\n",
    "\n",
    "        # reverse sort all sequences by their length\n",
    "        rev_order_len_index = sorted(\n",
    "            range(len(sentences)), key=lambda k: len(sentences[k]), reverse=True\n",
    "        )\n",
    "        original_order_index = sorted(\n",
    "            range(len(rev_order_len_index)), key=lambda k: rev_order_len_index[k]\n",
    "        )\n",
    "\n",
    "        reordered_sentences: List[Union[DataPoint, str]] = [\n",
    "            sentences[index] for index in rev_order_len_index\n",
    "        ]\n",
    "\n",
    "        # Turn all Sentence objects into strings\n",
    "        if isinstance(reordered_sentences[0], Sentence):\n",
    "            str_reordered_sentences = [\n",
    "                sentence.to_original_text() for sentence in sentences\n",
    "            ]\n",
    "        else:\n",
    "            str_reordered_sentences = reordered_sentences\n",
    "\n",
    "        dataset = self._tokenize(str_reordered_sentences)\n",
    "        dl = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "        predictions: List[Tuple[str, float]] = []\n",
    "\n",
    "        outputs, _ = super().get_preds(dl=dl)\n",
    "        logits = torch.cat([o['logits'] for o in outputs])\n",
    "        predictions = torch.softmax(logits, dim=1).tolist()\n",
    "\n",
    "        for text, pred in zip(str_reordered_sentences, predictions):\n",
    "            # Initialize and assign labels to each class in each datapoint prediction\n",
    "            text_sent = Sentence(text)\n",
    "            for k, v in id2label.items():\n",
    "                text_sent.add_label(label_type='sc', value=v, score=pred[k])\n",
    "            results.append(text_sent)\n",
    "\n",
    "        # Order results back into original order\n",
    "        results = [results[index] for index in original_order_index]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _tokenize(\n",
    "        self, sentences: Union[List[Sentence], Sentence, List[str], str]\n",
    "    ) -> TensorDataset:\n",
    "        \"\"\" Batch tokenizes text and produces a `TensorDataset` with them \"\"\"\n",
    "\n",
    "        # TODO: __call__ from tokenizer base class in the transformers library could automate/handle this\n",
    "        tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "            sentences,\n",
    "            return_tensors='pt',\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        # Bart, XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use token_type_ids\n",
    "        if isinstance(\n",
    "            self.model,\n",
    "            (\n",
    "                BertForSequenceClassification,\n",
    "                XLNetForSequenceClassification,\n",
    "                AlbertForSequenceClassification,\n",
    "            ),\n",
    "        ):\n",
    "            dataset = TensorDataset(\n",
    "                tokenized_text['input_ids'],\n",
    "                tokenized_text['attention_mask'],\n",
    "                tokenized_text['token_type_ids'],\n",
    "            )\n",
    "        else:\n",
    "            dataset = TensorDataset(\n",
    "                tokenized_text['input_ids'], tokenized_text['attention_mask']\n",
    "            )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        training_args: TrainingArguments,\n",
    "        train_dataset: datasets.Dataset,\n",
    "        eval_dataset: datasets.Dataset,\n",
    "        text_col_nm: str = 'text',\n",
    "        label_col_nm: str = 'label',\n",
    "        compute_metrics: Callable = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Trains and/or finetunes the sequence classification model\n",
    "\n",
    "        * **training_args** - Transformers `TrainingArguments` object model\n",
    "        * **train_dataset** - Training `Dataset` class object from the datasets library\n",
    "        * **eval_dataset** - Eval `Dataset` class object from the datasets library\n",
    "        * **text_col_nm** - Name of the text feature column used as training data (Default 'text')\n",
    "        * **label_col_nm** - Name of the label feature column (Default 'label')\n",
    "        * **compute_metrics** - Custom metrics function callable for `transformers.Trainer`'s compute metrics\n",
    "        * **return** - None\n",
    "        \"\"\"\n",
    "        # Set default metrics if None\n",
    "        if not compute_metrics:\n",
    "            compute_metrics = self._default_metrics\n",
    "\n",
    "        # Set datasets.Dataset label values in sequence classifier configuration\n",
    "        ## Important NOTE: Updating configurations do not update the sequence classification head module layer\n",
    "        ## We are manually initializing a new linear layer for the 'new' labels being trained\n",
    "        class_label = train_dataset.features[label_col_nm]\n",
    "        config_data = {\n",
    "            'num_labels': class_label.num_classes,\n",
    "            'id2label': {v: n for v, n in enumerate(class_label.names)},\n",
    "            'label2id': {n: v for v, n in enumerate(class_label.names)},\n",
    "        }\n",
    "        self.model.config.update(config_data)\n",
    "        self._mutate_model_head(class_label=class_label)\n",
    "\n",
    "        # Batch map datasets as torch tensors with tokenizer\n",
    "        def tokenize(batch):\n",
    "            return self.tokenizer(batch[text_col_nm], padding=True, truncation=True)\n",
    "\n",
    "        train_dataset = train_dataset.map(\n",
    "            tokenize, batch_size=len(train_dataset), batched=True\n",
    "        )\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            tokenize, batch_size=len(eval_dataset), batched=True\n",
    "        )\n",
    "\n",
    "        # Rename label col name to match model forward signature of 'labels' or ['label','label_ids'] since these are addressed by the default collator from transformers\n",
    "        train_dataset.rename_column_(\n",
    "            original_column_name=label_col_nm, new_column_name='labels'\n",
    "        )\n",
    "        eval_dataset.rename_column_(\n",
    "            original_column_name=label_col_nm, new_column_name='labels'\n",
    "        )\n",
    "\n",
    "        # Set format as torch tensors for training\n",
    "        train_dataset.set_format(\n",
    "            'torch', columns=['input_ids', 'attention_mask', 'labels']\n",
    "        )\n",
    "        eval_dataset.set_format(\n",
    "            'torch', columns=['input_ids', 'attention_mask', 'labels']\n",
    "        )\n",
    "\n",
    "        # Instantiate transformers trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        # Train and serialize\n",
    "        self.trainer.train()\n",
    "        self.trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    def evaluate(self) -> Dict[str, float]:\n",
    "        \"\"\"Evaluates model specified\n",
    "\n",
    "        * **model_name_or_path** - The model name key or model path\n",
    "        \"\"\"\n",
    "        if not self.trainer:\n",
    "            logger.info('No trainer loaded, must run `classifier.train(...)` first')\n",
    "            ValueError('Trainer not found, must run train() method')\n",
    "        return self.trainer.evaluate()\n",
    "\n",
    "    def _mutate_model_head(self, class_label: ClassLabel) -> None:\n",
    "        \"\"\"Manually intialize new linear layers for prediction heads on specific language models that we're trying to train on\"\"\"\n",
    "        if isinstance(self.model, (BertPreTrainedModel, DistilBertPreTrainedModel)):\n",
    "            self.model.classifier = nn.Linear(\n",
    "                self.model.config.hidden_size, class_label.num_classes\n",
    "            )\n",
    "            self.model.num_labels = class_label.num_classes\n",
    "        elif isinstance(self.model, XLMPreTrainedModel):\n",
    "            self.model.num_labels = class_label.num_classes\n",
    "        elif isinstance(self.model, XLNetPreTrainedModel):\n",
    "            self.model.logits_proj = nn.Linear(\n",
    "                self.model.config.d_model, class_label.num_classes\n",
    "            )\n",
    "            self.model.num_labels = class_label.num_classes\n",
    "        elif isinstance(self.model, ElectraPreTrainedModel):\n",
    "            self.model.num_labels = class_label.num_classes\n",
    "        else:\n",
    "            logger.info(f'Sorry, can not train on a model of type {type(self.model)}')\n",
    "\n",
    "    # Setup default metrics for sequence classification training\n",
    "    def _default_metrics(self, pred) -> Dict:\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average=None\n",
    "        )\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "import flair\n",
    "example_text = \"This didn't work at all\"\n",
    "\n",
    "classifier = TransformersSequenceClassifier.load(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "sentences = classifier.predict(text=example_text,mini_batch_size=1,\n",
    ")\n",
    "\n",
    "preds = sentences[0].get_labels()\n",
    "\n",
    "truth_lbls = [\n",
    "    flair.data.Label('1 star', 0.8421),\n",
    "    flair.data.Label('2 stars', 0.1379),\n",
    "    flair.data.Label('3 stars', 0.018),\n",
    "    flair.data.Label('4 stars', 0.0012),\n",
    "    flair.data.Label('5 stars', 0.0007)\n",
    "]\n",
    "\n",
    "for pred, truth in zip(preds, truth_lbls):\n",
    "    test_eq(pred.value, truth.value)\n",
    "    test_close(pred.score, truth.score, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FlairSequenceClassifier(AdaptiveModel):\n",
    "    \"\"\"Adaptive Model for Flair's Sequence Classifier...very basic\n",
    "\n",
    "    Usage:\n",
    "    ```python\n",
    "    >>> classifier = FlairSequenceClassifier.load('sentiment')\n",
    "    >>> classifier.predict(text='Example text', mini_batch_size=32)\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **model_name_or_path** - A key string of one of Flair's pre-trained Sequence Classifier Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name_or_path: str):\n",
    "        self.classifier = TextClassifier.load(model_name_or_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: Union[HFModelResult, str]) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading a constructing this classifier\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Flair's pre-trained Sequence Classifier Model or a `HFModelResult`\n",
    "        \"\"\"\n",
    "        if isinstance(model_name_or_path, HFModelResult): model_name_or_path = model_name_or_path.name\n",
    "        classifier = cls(model_name_or_path)\n",
    "        return classifier\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Flair classifier\n",
    "        \"\"\"\n",
    "        if isinstance(text, (Sentence, str)):\n",
    "            text = [text]\n",
    "        if isinstance(text[0], str):\n",
    "            text = [Sentence(s) for s in text]\n",
    "\n",
    "        self.classifier.predict(\n",
    "            sentences=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "        return text\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15 19:43:20,854 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import flair\n",
    "example_text = \"This didn't work at all\"\n",
    "\n",
    "\n",
    "classifier = FlairSequenceClassifier.load('sentiment')\n",
    "\n",
    "sentences = classifier.predict(text=example_text,mini_batch_size=1,\n",
    ")\n",
    "\n",
    "pred = sentences[0].get_labels()[0]\n",
    "\n",
    "test_eq(pred.value, 'NEGATIVE')\n",
    "test_close(pred.score, 0.999, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from adaptnlp.model_hub import HFModelSearchHub, FlairModelSearchHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasySequenceClassifier:\n",
    "    \"\"\"Sequence classification models\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> classifier = EasySequenceClassifier()\n",
    "    >>> classifier.tag_text(text='text you want to label', model_name_or_path='en-sentiment')\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sequence_classifiers: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "        self.hf_hub = HFModelSearchHub()\n",
    "        self.flair_hub = FlairModelSearchHub()\n",
    "\n",
    "    def tag_text(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        model_name_or_path: str = 'en-sentiment',\n",
    "        mini_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Tags a text sequence with labels the sequence classification models have been trained on\n",
    "\n",
    "        * **text** - String, list of strings, `Sentence`, or list of `Sentence`s to be classified\n",
    "        * **model_name_or_path** - The model name key or model path\n",
    "        * **mini_batch_size** - The mini batch size for running inference\n",
    "        * **&ast;&ast;kwargs** - (Optional) Keyword Arguments for Flair's `TextClassifier.predict()` method params\n",
    "        **return** A list of Flair's `Sentence`'s\n",
    "        \"\"\"\n",
    "        # Load Text Classifier Model and Pytorch Module into tagger dict\n",
    "        if not self.sequence_classifiers[model_name_or_path]:\n",
    "            \"\"\"\n",
    "            self.sequence_classifiers[model_name_or_path] = TextClassifier.load(\n",
    "                model_name_or_path\n",
    "            )\n",
    "            \"\"\"\n",
    "            # First check regular Flair:\n",
    "            models = self.flair_hub.search_model_by_name(model_name_or_path, user_uploaded=True)\n",
    "            flair = True\n",
    "            if len(models) < 1:\n",
    "                # Then the rest of HuggingFace\n",
    "                models = self.hf_hub.search_model_by_name(model_name_or_path, user_uploaded=True)\n",
    "                flair = False\n",
    "                if len(models) < 1:\n",
    "                    logger.info(\n",
    "                    f'{model_name_or_path} is not a valid model name.'\n",
    "                    )\n",
    "                    return [Sentence('')]\n",
    "            if flair:\n",
    "                self.sequence_classifiers[\n",
    "                    model_name_or_path\n",
    "                ] = FlairSequenceClassifier.load(model_name_or_path)\n",
    "            else:\n",
    "                self.sequence_classifiers[\n",
    "                    model_name_or_path\n",
    "                ] = TransformersSequenceClassifier.load(model_name_or_path)\n",
    "\n",
    "        classifier = self.sequence_classifiers[model_name_or_path]\n",
    "        return classifier.predict(\n",
    "            text=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def tag_all(\n",
    "        self,\n",
    "        text: Union[List[Sentence], Sentence, List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"Tags text with all labels from all sequence classification models\n",
    "\n",
    "        * **text** - Text input, it can be a string or any of Flair's `Sentence` input formats\n",
    "        * **mini_batch_size** - The mini batch size for running inference\n",
    "        * **&ast;&ast;kwargs** - (Optional) Keyword Arguments for Flair's `TextClassifier.predict()` method params\n",
    "        * **return** - A list of Flair's `Sentence`'s\n",
    "        \"\"\"\n",
    "        sentences = text\n",
    "        for tagger_name in self.sequence_classifiers.keys():\n",
    "            sentences = self.tag_text(\n",
    "                sentences,\n",
    "                model_name_or_path=tagger_name,\n",
    "                mini_batch_size=mini_batch_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "        return sentences\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        training_args: TrainingArguments,\n",
    "        train_dataset: Union[str, Path, datasets.Dataset],\n",
    "        eval_dataset: Union[str, Path, datasets.Dataset],\n",
    "        model_name_or_path: str = 'bert-base-uncased',\n",
    "        text_col_nm: str = 'text',\n",
    "        label_col_nm: str = 'label',\n",
    "        label_names: List[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Trains and/or finetunes the sequence classification model\n",
    "\n",
    "        * **model_name_or_path** - The model name key or model path\n",
    "        * **training_args** - Transformers `TrainingArguments` object model\n",
    "        * **train_dataset** - Training `Dataset` class object from the datasets library or path to CSV file (labels must be int values)\n",
    "        * **eval_dataset** - Eval `Dataset` class object from the datasets library or path to CSV file (labels must be int values)\n",
    "        * **text_col_nm** - Name of the text feature column used as training data (Default 'text')\n",
    "        * **label_col_nm** - Name of the label feature column (Default 'label')\n",
    "        * **label_names** - (Only when loading CSV) An ordered list of label strings with int label mapped to string via. index value\n",
    "        * **return** - None\n",
    "        \"\"\"\n",
    "\n",
    "        # Dynamically load sequence classifier\n",
    "        if not self.sequence_classifiers[model_name_or_path]:\n",
    "            try:\n",
    "                self.sequence_classifiers[\n",
    "                    model_name_or_path\n",
    "                ] = TransformersSequenceClassifier.load(model_name_or_path)\n",
    "            except ValueError:\n",
    "                logger.info('Try transformers model')\n",
    "\n",
    "        classifier = self.sequence_classifiers[model_name_or_path]\n",
    "\n",
    "        # Check if csv filepath or `datasets.Dataset`\n",
    "        if not isinstance(train_dataset, datasets.Dataset):\n",
    "            train_dataset = self._csv2dataset(\n",
    "                data_path=train_dataset,\n",
    "                label_col_nm=label_col_nm,\n",
    "                label_names=label_names,\n",
    "            )\n",
    "        if not isinstance(eval_dataset, datasets.Dataset):\n",
    "            eval_dataset = self._csv2dataset(\n",
    "                data_path=eval_dataset,\n",
    "                label_col_nm=label_col_nm,\n",
    "                label_names=label_names,\n",
    "            )\n",
    "\n",
    "        classifier.train(\n",
    "            training_args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            text_col_nm=text_col_nm,\n",
    "            label_col_nm=label_col_nm,\n",
    "        )\n",
    "\n",
    "    def release_model(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "    ) -> None:\n",
    "        \"\"\"Unload model from classifier and empty cuda mem cache (may leave residual cache per pytorch documentation on torch.cuda.empty_cache())\n",
    "\n",
    "        * **model_name_or_path** - The model name or key path that you want to unload and release memory from\n",
    "        \"\"\"\n",
    "        if self.sequence_classifiers[model_name_or_path].trainer:\n",
    "            del self.sequence_classifiers[model_name_or_path].trainer\n",
    "        del self.sequence_classifiers[model_name_or_path]\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        model_name_or_path: str = 'bert-base-uncased',\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Evaluates model specified\n",
    "\n",
    "        * **model_name_or_path** - The model name key or model path\n",
    "        \"\"\"\n",
    "\n",
    "        # Dynamically load sequence classifier\n",
    "        if not self.sequence_classifiers[model_name_or_path]:\n",
    "            try:\n",
    "                self.sequence_classifiers[\n",
    "                    model_name_or_path\n",
    "                ] = TransformersSequenceClassifier.load(model_name_or_path)\n",
    "            except ValueError:\n",
    "                logger.info('Try transformers model')\n",
    "\n",
    "        classifier = self.sequence_classifiers[model_name_or_path]\n",
    "\n",
    "        return classifier.evaluate()\n",
    "\n",
    "    def _csv2dataset(\n",
    "        self,\n",
    "        data_path: Union[str, Path],\n",
    "        label_col_nm: str,\n",
    "        label_names: List[str],\n",
    "    ) -> datasets.Dataset:\n",
    "        \"\"\"Loads CSV path as an datasets.Dataset for downstream use\"\"\"\n",
    "        if not label_names:\n",
    "            raise ValueError(\n",
    "                'Must pass in `label_names` parameter for training when loading in CSV datasets.'\n",
    "            )\n",
    "        class_label = datasets.ClassLabel(\n",
    "            num_classes=len(label_names), names=label_names\n",
    "        )\n",
    "        dataset = datasets.load_dataset('csv', data_files=data_path)\n",
    "        dataset = dataset['train']\n",
    "        dataset.features[label_col_nm] = class_label\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
