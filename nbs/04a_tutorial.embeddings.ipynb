{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Easy Embeddings\n",
    "> Using EasyWord, Stacked, and Document Embeddings in the AdaptNLP framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Available Models with Hubs\n",
    "\n",
    "We can search for available models to utilize with Embeddings with the `HFModelHub` and `FlairModelHub`. We'll see an example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp import EasyWordEmbeddings, EasyStackedEmbeddings, EasyDocumentEmbeddings\n",
    "from adaptnlp.model_hub import HFModelHub, FlairModelHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model Name: distilgpt2, Tasks: [text-generation],\n",
       " Model Name: gpt2-large, Tasks: [text-generation],\n",
       " Model Name: gpt2-medium, Tasks: [text-generation],\n",
       " Model Name: gpt2-xl, Tasks: [text-generation],\n",
       " Model Name: gpt2, Tasks: [text-generation]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub = HFModelHub()\n",
    "models = hub.search_model_by_name('gpt2'); models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we'll use the `gpt2` base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Name: gpt2, Tasks: [text-generation]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models[-1]; model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing Embeddings using `EasyWordEmbeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll use some basic example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"This is Albert.  My last name is Einstein.  I like physics and atoms.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then instantiate our embeddings tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = EasyWordEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our `gpt2` model we grabbed earlier to generate some flair `Sentences`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sentences = embeddings.embed_text(example_text, model_name_or_path=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These flair `Sentences` hold the embeddings inside of each `token`. So to get access to them we need to look at a specific sentence, its specific token, and call `.get_embedding()`. For instance below is the embedding representation of \"Albert\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Albert\n",
      "Model: gpt2\n",
      "Embedding: tensor([-3.9810, -0.5063, -2.2954, -1.3400,  0.1948, -0.7453,  1.4224,  0.2852,\n",
      "         0.5815,  0.7180], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "token = sentences[0][2]\n",
    "print(f'Original text: {token.text}')\n",
    "print('Model: gpt2')\n",
    "print(f'Embedding: {token.get_embedding()[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using different models is extremely easy to do. Let's try using BERT embeddings with the `bert-base-cased` model instead.\n",
    "\n",
    "Rather than passing in a `HFModelResult` or `FlairModelResult`, we can also just pass in the raw string name of the model as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = embeddings.embed_text(example_text, model_name_or_path='bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the last example, we can look at the embeddings in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Albert\n",
      "Model: bert-base-cased\n",
      "Embedding: tensor([-0.0846, -0.2399,  0.2524, -0.4409, -0.2508, -0.6320, -0.1890,  0.2085,\n",
      "        -0.8265, -0.7632], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "token = sentences[0][2]\n",
    "print(f'Original text: {token.text}')\n",
    "print('Model: bert-base-cased')\n",
    "print(f'Embedding: {token.get_embedding()[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a final example with roBERTa embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = embeddings.embed_text(example_text, model_name_or_path=\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our generated embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Albert\n",
      "Model: roberta-base\n",
      "Embedding: tensor([ 0.1772,  0.0369, -0.0483,  0.2290, -0.4860,  0.3483,  0.2176, -0.0787,\n",
      "        -0.2275, -0.4035], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "token = sentences[0][2]\n",
    "print(f'Original text: {token.text}')\n",
    "print(f'Model: roberta-base')\n",
    "print(f'Embedding: {token.get_embedding()[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing Stacked Embeddings with `EasyStackedEmbeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EasyStackedEmbeddings` allows you to use a variable number of language models to produce our embeddings shown above. For our example we'll combine the `bert-base-cased` and `distilbert-base-cased` models.\n",
    "\n",
    "First we'll instantiate our `EasyStackedEmbeddings`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May need a couple moments to instantiate...\n"
     ]
    }
   ],
   "source": [
    "embeddings = EasyStackedEmbeddings(\"bert-base-cased\", \"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then generate our stacked word embeddings through our `embed_text` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = embeddings.embed_text(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our results below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Albert\n",
      "Models: bert-base-cased, distilbert-base-cased\n",
      "Embedding: tensor([-0.0846, -0.2399,  0.2524, -0.4409, -0.2508, -0.6320, -0.1890,  0.2085,\n",
      "        -0.8265, -0.7632], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "token = sentences[0][2]\n",
    "print(f'Original text: {token.text}')\n",
    "print(f'Models: bert-base-cased, distilbert-base-cased')\n",
    "print(f'Embedding: {token.get_embedding()[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embeddings with `EasyDocumentEmbeddings`\n",
    "\n",
    "Similar to the `EasyStackedEmbeddings`, `EasyDocumentEmbeddings` allows you to pool the embeddings from multiple models together with `embed_pool` and `embed_rnn`.\n",
    "\n",
    "We'll use our `bert-base-cased` and `distilbert-base-cased` models again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May need a couple moments to instantiate...\n",
      "Pooled embedding loaded\n",
      "RNN embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings = EasyDocumentEmbeddings(\"bert-base-cased\", \"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will use the `embed_pool` method to generate `DocumentPoolEmbeddings`. These do an average over all the word embeddings in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = embeddings.embed_pool(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result rather than having embeddings by token, we have embeddings *by document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is Albert . My last name is Einstein . I like physics and atoms .\n",
      "Models: bert-base-cased, distilbert-base-cased\n",
      "Embedding: tensor([-0.2397,  0.2154,  0.1053,  0.3809, -0.2323,  0.2913, -0.1869,  0.0963,\n",
      "        -0.0407, -0.2648], device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[0]\n",
    "print(f'Original text: {sentence.to_tokenized_string()}')\n",
    "print(f'Models: bert-base-cased, distilbert-base-cased')\n",
    "print(f'Embedding: {sentence.get_embedding()[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate `DocumentRNNEmbeddings` as well. Document RNN Embeddings run an RNN over all the words in the sentence and use the final state of the RNN as the embedding.\n",
    "\n",
    "First we'll call `embed_rnn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = embeddings.embed_rnn(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then look at our generated embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is Albert . My last name is Einstein . I like physics and atoms .\n",
      "Models: bert-base-cased, distilbert-base-cased\n",
      "Embedding: tensor([ 0.5235, -0.2955, -0.3608,  0.4746, -0.0441, -0.2596,  0.5656,  0.0506,\n",
      "        -0.2100,  0.0992], device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[0]\n",
    "print(f'Original text: {sentence.to_tokenized_string()}')\n",
    "print(f'Models: bert-base-cased, distilbert-base-cased')\n",
    "print(f'Embedding: {sentence.get_embedding()[:10]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
