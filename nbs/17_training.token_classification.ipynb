{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ffd5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training.token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33013810",
   "metadata": {},
   "source": [
    "# Token Classification Tuning\n",
    "> Data and Tuning API for Token Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13989e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbverbose.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7ec3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.xtras import Path, range_of\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.data.transforms import get_files\n",
    "\n",
    "from adaptnlp.training.core import *\n",
    "from adaptnlp.training.arrow_utils import TextNoNewLineDatasetReader\n",
    "from adaptnlp.inference.token_classification import TransformersTokenTagger, TokenClassificationResult, DetailLevel\n",
    "\n",
    "from transformers import default_data_collator, AutoModelForTokenClassification, AutoTokenizer\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "from typing import List\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abff10e",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b824338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def encode_tags(tags, encodings):\n",
    "    # create an empty array of -100\n",
    "    encoded_labels = np.ones(len(encodings['offset_mapping']),dtype=int) * -100\n",
    "    offset_array = np.array(encodings['offset_mapping'])\n",
    "    \n",
    "    # set labels whose first offset position is 0 and the second is not 0\n",
    "    encoded_labels[(offset_array[:,0] == 0) & (offset_array[:,1] != 0)] = tags\n",
    "        \n",
    "    return list(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4dd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _batch_tokenize(batch, tokenizer, tokenize_kwargs):\n",
    "    updated_tags = [] # list of lists\n",
    "    updated_batch = {} # dict of lists\n",
    "    \n",
    "    for tokens, tags in zip(batch['tokens'], batch['ner_tags']):\n",
    "        encodings = tokenizer(tokens, **tokenize_kwargs)\n",
    "        updated_tags.append(encode_tags(tags, encodings))\n",
    "        \n",
    "        for feat in encodings.keys():\n",
    "            if (feat != 'offset_mapping'):\n",
    "                if feat in updated_batch:\n",
    "                    updated_batch[feat].append(encodings[feat])\n",
    "                else:\n",
    "                    updated_batch[feat] = [encodings[feat]]\n",
    "                \n",
    "    updated_batch['labels'] = updated_tags\n",
    "    return updated_batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc482a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenClassificationDatasets(TaskDatasets):\n",
    "    \"\"\"\n",
    "    A set of datasets designed for token classification\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dset:Dataset, # A training dataset\n",
    "        valid_dset:Dataset, # A validation dataset\n",
    "        tokenizer_name:str, # The name of a tokenizer\n",
    "        tokenize:bool, # Whether to tokenize immediately\n",
    "        tokenize_kwargs:dict, # kwargs for the tokenize function\n",
    "        auto_kwargs:dict, # AutoTokenizer.from_pretrained kwargs\n",
    "        remove_columns:list, # The columns to remove when tokenizing\n",
    "        entity_mapping:dict, # A mapping of entity names to encoded labels\n",
    "    ):\n",
    "        \n",
    "        \"Constructs TaskDatasets, should not be called explicitly\"\n",
    "        super().__init__(\n",
    "            train_dset,\n",
    "            valid_dset,\n",
    "            tokenizer_name,\n",
    "            tokenize,\n",
    "            _batch_tokenize,\n",
    "            tokenize_kwargs,\n",
    "            auto_kwargs,\n",
    "            remove_columns,\n",
    "        )\n",
    "        self.entity_mapping = entity_mapping\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dfs(\n",
    "        cls,\n",
    "        train_df:pd.DataFrame, # A training dataframe\n",
    "        token_col:str, # The name of the token column\n",
    "        tag_col:str, # The name of the tag column\n",
    "        entity_mapping:dict, # A mapping of entity names to encoded labels\n",
    "        tokenizer_name:str, # The name of the tokenizer\n",
    "        tokenize:bool=True, # Whether to tokenize immediately\n",
    "        valid_df=None, # An optional validation dataframe\n",
    "        split_func=None, # Optionally a splitting function similar to RandomSplitter\n",
    "        split_pct=.2, # What % to split the train_df\n",
    "        tokenize_kwargs:dict={}, # kwargs for the tokenize function\n",
    "        auto_kwargs:dict={} # kwargs for the AutoTokenizer.from_pretrained constructor\n",
    "    ):\n",
    "        \"Builds `TokenClassificationDatasets` from a `DataFrame` or set of `DataFrames`\"\n",
    "        if split_func is None: split_func = RandomSplitter(split_pct)\n",
    "        if valid_df is None:\n",
    "            train_idxs, valid_idxs = split_func(train_df)\n",
    "            valid_df = train_df.iloc[valid_idxs]\n",
    "            train_df = train_df.iloc[train_idxs]\n",
    "            \n",
    "        train_df = train_df[[token_col,tag_col]]\n",
    "        valid_df = valid_df[[token_col,tag_col]]\n",
    "        train_df = train_df.rename(columns={token_col:'tokens', tag_col: 'ner_tags'})\n",
    "        valid_df = valid_df.rename(columns={token_col:'tokens', tag_col: 'ner_tags'})\n",
    "        \n",
    "        train_dset = Dataset.from_dict(train_df.to_dict('list'))\n",
    "        valid_dset = Dataset.from_dict(valid_df.to_dict('list'))\n",
    "        \n",
    "        return cls(\n",
    "            train_dset,\n",
    "            valid_dset,\n",
    "            tokenizer_name,\n",
    "            tokenize,\n",
    "            tokenize_kwargs,\n",
    "            auto_kwargs,\n",
    "            remove_columns=['tokens', 'ner_tags'],\n",
    "            entity_mapping = entity_mapping\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csvs(\n",
    "        cls,\n",
    "        train_csv:Path, # A training csv file\n",
    "        token_col:str, # The name of the token column\n",
    "        tag_col:str, # The name of the tag column\n",
    "        entity_mapping:dict, # A mapping of entity names to encoded labels\n",
    "        tokenizer_name:str, # The name of the tokenizer\n",
    "        tokenize:bool=True, # Whether to tokenize immediately\n",
    "        valid_csv:Path=None, # An optional validation csv\n",
    "        split_func=None, # Optionally a splitting function similar to RandomSplitter\n",
    "        split_pct=.2, # What % to split the train df\n",
    "        tokenize_kwargs:dict={}, # kwargs for the tokenize function\n",
    "        auto_kwargs:dict={}, # kwargs for the AutoTokenizer.from_pretrained constructor\n",
    "        **kwargs, # kwargs for `pd.read_csv`\n",
    "    ):\n",
    "        \"Builds `SequenceClassificationDatasets` from a single csv or set of csvs. A convience constructor for `from_dfs`\"\n",
    "        train_df = pd.read_csv(train_csv, **kwargs)\n",
    "        if valid_csv is not None: valid_df = pd.read_csv(valid_csv, **kwargs)\n",
    "        else: valid_df = None\n",
    "        return cls.from_dfs(\n",
    "            train_df,\n",
    "            token_col,\n",
    "            tag_col,\n",
    "            entity_mapping,\n",
    "            tokenizer_name,\n",
    "            tokenize,\n",
    "            valid_df,\n",
    "            split_func,\n",
    "            split_pct,\n",
    "            tokenize_kwargs,\n",
    "            auto_kwargs\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @delegates(DataLoaders)\n",
    "    def dataloaders(\n",
    "        self,\n",
    "        batch_size:int=16, # A batch size\n",
    "        shuffle_train:bool=True, # Whether to shuffle the training dataset\n",
    "        collate_fn:callable=None, # A custom collation function\n",
    "        **kwargs # Torch DataLoader kwargs\n",
    "    ):\n",
    "        \"Build DataLoaders from `self`\"\n",
    "        dls = super().dataloaders(batch_size, shuffle_train, collate_fn, **kwargs)\n",
    "        dls.entity_mapping = self.entity_mapping\n",
    "        return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "protecting-teaching",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TokenClassificationDatasets.from_dfs\" class=\"doc_header\"><code>TokenClassificationDatasets.from_dfs</code><a href=\"__main__.py#L31\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TokenClassificationDatasets.from_dfs</code>(**`train_df`**:`DataFrame`, **`token_col`**:`str`, **`tag_col`**:`str`, **`entity_mapping`**:`dict`, **`tokenizer_name`**:`str`, **`tokenize`**:`bool`=*`True`*, **`valid_df`**=*`None`*, **`split_func`**=*`None`*, **`split_pct`**=*`0.2`*, **`tokenize_kwargs`**:`dict`=*`{}`*, **`auto_kwargs`**:`dict`=*`{}`*)\n",
       "\n",
       "Builds `TokenClassificationDatasets` from a `DataFrame` or set of `DataFrames`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`train_df`** : *`<class 'pandas.core.frame.DataFrame'>`*\t<p>A training dataframe</p>\n",
       "\n",
       "\n",
       " - **`token_col`** : *`<class 'str'>`*\t<p>The name of the token column</p>\n",
       "\n",
       "\n",
       " - **`tag_col`** : *`<class 'str'>`*\t<p>The name of the tag column</p>\n",
       "\n",
       "\n",
       " - **`entity_mapping`** : *`<class 'dict'>`*\t<p>A mapping of entity names to encoded labels</p>\n",
       "\n",
       "\n",
       " - **`tokenizer_name`** : *`<class 'str'>`*\t<p>The name of the tokenizer</p>\n",
       "\n",
       "\n",
       " - **`tokenize`** : *`<class 'bool'>`*, *optional*\t<p>Whether to tokenize immediately</p>\n",
       "\n",
       "\n",
       " - **`valid_df`** : *`<class 'NoneType'>`*, *optional*\t<p>An optional validation dataframe</p>\n",
       "\n",
       "\n",
       " - **`split_func`** : *`<class 'NoneType'>`*, *optional*\t<p>Optionally a splitting function similar to RandomSplitter</p>\n",
       "\n",
       "\n",
       " - **`split_pct`** : *`<class 'float'>`*, *optional*\t<p>What % to split the train_df</p>\n",
       "\n",
       "\n",
       " - **`tokenize_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>kwargs for the tokenize function</p>\n",
       "\n",
       "\n",
       " - **`auto_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>kwargs for the AutoTokenizer.from_pretrained constructor</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TokenClassificationDatasets.from_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "sensitive-marina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TokenClassificationDatasets.from_csvs\" class=\"doc_header\"><code>TokenClassificationDatasets.from_csvs</code><a href=\"__main__.py#L72\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TokenClassificationDatasets.from_csvs</code>(**`train_csv`**:`Path`, **`token_col`**:`str`, **`tag_col`**:`str`, **`entity_mapping`**:`dict`, **`tokenizer_name`**:`str`, **`tokenize`**:`bool`=*`True`*, **`valid_csv`**:`Path`=*`None`*, **`split_func`**=*`None`*, **`split_pct`**=*`0.2`*, **`tokenize_kwargs`**:`dict`=*`{}`*, **`auto_kwargs`**:`dict`=*`{}`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Builds [`SequenceClassificationDatasets`](/adaptnlp/training.sequence_classification.html#SequenceClassificationDatasets) from a single csv or set of csvs. A convience constructor for `from_dfs`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`train_csv`** : *`<class 'pathlib.Path'>`*\t<p>A training csv file</p>\n",
       "\n",
       "\n",
       " - **`token_col`** : *`<class 'str'>`*\t<p>The name of the token column</p>\n",
       "\n",
       "\n",
       " - **`tag_col`** : *`<class 'str'>`*\t<p>The name of the tag column</p>\n",
       "\n",
       "\n",
       " - **`entity_mapping`** : *`<class 'dict'>`*\t<p>A mapping of entity names to encoded labels</p>\n",
       "\n",
       "\n",
       " - **`tokenizer_name`** : *`<class 'str'>`*\t<p>The name of the tokenizer</p>\n",
       "\n",
       "\n",
       " - **`tokenize`** : *`<class 'bool'>`*, *optional*\t<p>Whether to tokenize immediately</p>\n",
       "\n",
       "\n",
       " - **`valid_csv`** : *`<class 'pathlib.Path'>`*, *optional*\t<p>An optional validation csv</p>\n",
       "\n",
       "\n",
       " - **`split_func`** : *`<class 'NoneType'>`*, *optional*\t<p>Optionally a splitting function similar to RandomSplitter</p>\n",
       "\n",
       "\n",
       " - **`split_pct`** : *`<class 'float'>`*, *optional*\t<p>What % to split the train df</p>\n",
       "\n",
       "\n",
       " - **`tokenize_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>kwargs for the tokenize function</p>\n",
       "\n",
       "\n",
       " - **`auto_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>kwargs for the AutoTokenizer.from_pretrained constructor</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TokenClassificationDatasets.from_csvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-beads",
   "metadata": {},
   "source": [
    "When passing in kwargs if anything should go to the `tokenize` function they should go to `tokenize_kwargs`, and if it should go to the `Auto` class constructor, they should go to `auto_kwargs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b59f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/ubuntu/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No value for `max_length` set, automatically adjusting to the size of the model and including truncation\n",
      "Sequence length set to: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931c511f6d62475f93f17d26aba1f6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742d0372f0c643539f3a144c9ea58dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from datasets import load_dataset\n",
    "\n",
    "# converting original HF dataset to pandas dataframe\n",
    "dsets = load_dataset('conll2003')\n",
    "dset = dsets['train']\n",
    "dset.set_format(type='pandas')\n",
    "df = dset[:]\n",
    "df = df[['tokens', 'ner_tags']]\n",
    "\n",
    "# create entity mapping dict\n",
    "entity_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# set tokenizer arguments\n",
    "tokenize_kwargs = {\n",
    "    'truncation':True, \n",
    "    'is_split_into_words':True, \n",
    "    'padding':'max_length', \n",
    "    'return_offsets_mapping':True\n",
    "}\n",
    "\n",
    "tset = TokenClassificationDatasets.from_dfs(\n",
    "    df,\n",
    "    'tokens',\n",
    "    'ner_tags',\n",
    "    entity_mapping,\n",
    "    tokenizer_name = 'distilbert-base-uncased',\n",
    "    tokenize=True,\n",
    "    tokenize_kwargs = tokenize_kwargs\n",
    ")\n",
    "\n",
    "test_eq(len(tset.train), 11233)\n",
    "test_eq(len(tset.valid), 2808)\n",
    "test_eq(tset.train[0].keys(), ['attention_mask', 'input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bizarre-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/ubuntu/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No value for `max_length` set, automatically adjusting to the size of the model and including truncation\n",
      "Sequence length set to: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67230b4bf1784693babdca45b3e74a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b9f27f786e4733b0820771321efb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from datasets import load_dataset\n",
    "\n",
    "# converting original HF dataset to pandas dataframe\n",
    "dsets = load_dataset('conll2003')\n",
    "dset = dsets['train']\n",
    "dset.set_format(type='pandas')\n",
    "df = dset[:]\n",
    "df = df[['tokens', 'ner_tags']]\n",
    "train_df = df.iloc[:11233]\n",
    "valid_df = df.iloc[11233:]\n",
    "\n",
    "# create entity mapping dict\n",
    "entity_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# set tokenizer arguments\n",
    "tokenize_kwargs = {\n",
    "    'truncation':True, \n",
    "    'is_split_into_words':True, \n",
    "    'padding':'max_length', \n",
    "    'return_offsets_mapping':True\n",
    "}\n",
    "\n",
    "tset = TokenClassificationDatasets.from_dfs(\n",
    "    train_df,\n",
    "    'tokens',\n",
    "    'ner_tags',\n",
    "    entity_mapping,\n",
    "    valid_df=valid_df,\n",
    "    tokenizer_name = 'distilbert-base-uncased',\n",
    "    tokenize=True,\n",
    "    tokenize_kwargs = tokenize_kwargs\n",
    ")\n",
    "\n",
    "test_eq(len(tset.train), 11233)\n",
    "test_eq(len(tset.valid), 2808)\n",
    "test_eq(tset.train[0].keys(), ['attention_mask', 'input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "accessory-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/ubuntu/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No value for `max_length` set, automatically adjusting to the size of the model and including truncation\n",
      "Sequence length set to: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d39d3b5cfa47ecaa7320f3ed494236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d261d5e670248d2821596870378c8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from datasets import load_dataset\n",
    "\n",
    "# converting original HF dataset to csv\n",
    "dsets = load_dataset('conll2003')\n",
    "dset = dsets['train']\n",
    "dset.set_format(type='pandas')\n",
    "df = dset[:]\n",
    "df = df[['tokens', 'ner_tags']]\n",
    "df['ner_tags'] = df['ner_tags'].apply(lambda x : list(x))\n",
    "df['tokens'] = df['tokens'].apply(lambda x : list(x))\n",
    "df.to_csv('/tmp/conll2003.csv', index=False)\n",
    "\n",
    "# create entity mapping dict\n",
    "entity_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# set tokenizer arguments\n",
    "tokenize_kwargs = {\n",
    "    'truncation':True, \n",
    "    'is_split_into_words':True, \n",
    "    'padding':'max_length', \n",
    "    'return_offsets_mapping':True\n",
    "}\n",
    "\n",
    "converters={'col1': literal_eval}\n",
    "\n",
    "tset = TokenClassificationDatasets.from_csvs(\n",
    "    '/tmp/conll2003.csv',\n",
    "    'tokens',\n",
    "    'ner_tags',\n",
    "    entity_mapping,\n",
    "    tokenizer_name = 'distilbert-base-uncased',\n",
    "    tokenize=True,\n",
    "    tokenize_kwargs = tokenize_kwargs,\n",
    "    converters={'tokens': literal_eval, 'ner_tags': literal_eval}  # kwarg to pd.read_csv\n",
    ")\n",
    "\n",
    "test_eq(len(tset.train), 11233)\n",
    "test_eq(len(tset.valid), 2808)\n",
    "test_eq(tset.train[0].keys(), ['attention_mask', 'input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "american-mainstream",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/ubuntu/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No value for `max_length` set, automatically adjusting to the size of the model and including truncation\n",
      "Sequence length set to: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7f339811e94fa582a2d6d2cfc55d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f7ea1074834915bb99089ffcabeab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from datasets import load_dataset\n",
    "\n",
    "# converting original HF dataset to csv\n",
    "dsets = load_dataset('conll2003')\n",
    "dset = dsets['train']\n",
    "dset.set_format(type='pandas')\n",
    "df = dset[:]\n",
    "df = df[['tokens', 'ner_tags']]\n",
    "df['ner_tags'] = df['ner_tags'].apply(lambda x : list(x))\n",
    "df['tokens'] = df['tokens'].apply(lambda x : list(x))\n",
    "train_df = df.iloc[:11233]\n",
    "valid_df = df.iloc[11233:]\n",
    "train_df.to_csv('/tmp/train.csv', index=False)\n",
    "valid_df.to_csv('/tmp/valid.csv', index=False)\n",
    "\n",
    "# create entity mapping dict\n",
    "entity_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# set tokenizer arguments\n",
    "tokenize_kwargs = {\n",
    "    'truncation':True, \n",
    "    'is_split_into_words':True, \n",
    "    'padding':'max_length', \n",
    "    'return_offsets_mapping':True\n",
    "}\n",
    "\n",
    "converters={'col1': literal_eval}\n",
    "\n",
    "tset = TokenClassificationDatasets.from_csvs(\n",
    "    '/tmp/train.csv',\n",
    "    'tokens',\n",
    "    'ner_tags',\n",
    "    entity_mapping,\n",
    "    valid_csv = '/tmp/valid.csv',\n",
    "    tokenizer_name = 'distilbert-base-uncased',\n",
    "    tokenize=True,\n",
    "    tokenize_kwargs = tokenize_kwargs,\n",
    "    converters={'tokens': literal_eval, 'ner_tags': literal_eval}  # kwarg to pd.read_csv\n",
    ")\n",
    "\n",
    "test_eq(len(tset.train), 11233)\n",
    "test_eq(len(tset.valid), 2808)\n",
    "test_eq(tset.train[0].keys(), ['attention_mask', 'input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243cb632",
   "metadata": {},
   "source": [
    "## Token Classification Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amateur-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class NERMetrics():\n",
    "    \"\"\"\n",
    "    Computes multi-label classification metrics for NER, using seqeval metric from HuggingFace\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dls:DataLoaders, # A set of DataLoaders\n",
    "        entity_mapping:dict=None, # A mapping of entity names to encoded labels\n",
    "    ):\n",
    "        if hasattr(dls, 'entity_mapping'): self.label_list = list(dls.entity_mapping.values())\n",
    "        else: raise ValueError('Could not extrapolate labels list from DataLoaders, please pass it in as a param')\n",
    "            \n",
    "        self.metric = load_metric(\"seqeval\")\n",
    "    \n",
    "    # source: https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb#scrollTo=-1bS-ATSqPq2\n",
    "    def compute_metrics(self, preds, labels):\n",
    "        predictions = preds.argmax(2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions = [\n",
    "            [self.label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [self.label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        results = self.metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        return results\n",
    "    \n",
    "    def accuracy(self, preds, labels):\n",
    "        results = self.compute_metrics(preds, labels)\n",
    "        return results['overall_accuracy']\n",
    "    \n",
    "    def precision(self, preds, labels):\n",
    "        results = self.compute_metrics(preds, labels)\n",
    "        return results['overall_precision']\n",
    "    \n",
    "    def recall(self, preds, labels):\n",
    "        results = self.compute_metrics(preds, labels)\n",
    "        return results['overall_recall']\n",
    "    \n",
    "    def f1(self, preds, labels):\n",
    "        results = self.compute_metrics(preds, labels)\n",
    "        return results['overall_f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0796d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenClassificationTuner(AdaptiveTuner):\n",
    "    \"\"\"\n",
    "    An `AdaptiveTuner` with good defaults for Token Classification tasks\n",
    "    \n",
    "    **Valid kwargs and defaults:**\n",
    "      - `lr`:float = 0.001\n",
    "      - `splitter`:function = `trainable_params`\n",
    "      - `cbs`:list = None\n",
    "      - `path`:Path = None\n",
    "      - `model_dir`:Path = 'models'\n",
    "      - `wd`:float = None\n",
    "      - `wd_bn_bias`:bool = False\n",
    "      - `train_bn`:bool = True\n",
    "      - `moms`: tuple(float) = (0.95, 0.85, 0.95)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dls:DataLoaders, # A set of DataLoaders\n",
    "        model_name:str, # A HuggingFace model\n",
    "        tokenizer = None, # A HuggingFace tokenizer\n",
    "        loss_func = CrossEntropyLossFlat(), # A loss function\n",
    "        metrics = ['accuracy', 'f1'], # Metrics to monitor the training with\n",
    "        opt_func = Adam, # A fastai or torch Optimizer\n",
    "        additional_cbs = None, # Additional Callbacks to have always tied to the Tuner\n",
    "        expose_fastai_api = False, # Whether to expose the fastai API\n",
    "        num_classes:int=None, # The number of classes\n",
    "        **kwargs, # kwargs for `Learner.__init__`\n",
    "    ):\n",
    "        additional_cbs = listify(additional_cbs)\n",
    "        for arg in 'dls,model,loss_func,metrics,opt_func,cbs,expose_fastai'.split(','): \n",
    "            if arg in kwargs.keys(): kwargs.pop(arg) # Pop all existing kwargs\n",
    "        if hasattr(dls, 'entity_mapping'): num_classes = len(dls.entity_mapping)\n",
    "        if num_classes is None: raise ValueError('Could not extrapolate number of classes, please pass it in as a param')\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_classes, id2label=entity_mapping)\n",
    "        if tokenizer is None: tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        NER_metrics = NERMetrics(dls)\n",
    "        new_metrics = []\n",
    "        for met in metrics:\n",
    "            if met == 'accuracy':\n",
    "                new_metrics.append(NER_metrics.accuracy)\n",
    "            elif met == 'precision':\n",
    "                new_metrics.append(NER_metrics.precision)\n",
    "            elif met == 'recall':\n",
    "                new_metrics.append(NER_metrics.recall)\n",
    "            elif met == 'f1':\n",
    "                new_metrics.append(NER_metrics.f1)\n",
    "            else:\n",
    "                raise ValueError('Metric not supported. Please enter one or more of the following: accuracy, precision, recall, f1')\n",
    "        \n",
    "        super().__init__(\n",
    "            expose_fastai_api,\n",
    "            dls = dls, \n",
    "            model = model, \n",
    "            tokenizer = tokenizer,\n",
    "            loss_func = loss_func, \n",
    "            metrics = new_metrics, \n",
    "            opt_func = opt_func, \n",
    "            cbs=additional_cbs, \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "    def predict(\n",
    "        self,\n",
    "        text:Union[List[str], str], # Some text or list of texts to do inference with\n",
    "        bs:int=64, # A batch size to use for multiple texts\n",
    "        grouped_entities: bool = True, # Return whole entity span strings\n",
    "        detail_level:DetailLevel = DetailLevel.Low, # A detail level to return on the predictions\n",
    "    ) -> dict: # A dictionary of filtered predictions\n",
    "        \"Predict some `text` for token classification with the currently loaded model\"\n",
    "        if getattr(self, '_inferencer', None) is None: self._inferencer = TransformersTokenTagger(self.tokenizer, self.model)\n",
    "        \n",
    "        preds = self._inferencer.predict(text, bs, grouped_entities, detail_level)\n",
    "        \n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "qualified-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TokenClassificationTuner.predict\" class=\"doc_header\"><code>TokenClassificationTuner.predict</code><a href=\"__main__.py#L66\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TokenClassificationTuner.predict</code>(**`text`**:`Union`\\[`List`\\[`str`\\], `str`\\], **`bs`**:`int`=*`64`*, **`grouped_entities`**:`bool`=*`True`*, **`detail_level`**:`DetailLevel`=*`'low'`*)\n",
       "\n",
       "Predict some `text` for token classification with the currently loaded model\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`text`** : *`typing.Union[typing.List[str], str]`*\t<p>Some text or list of texts to do inference with</p>\n",
       "\n",
       "\n",
       " - **`bs`** : *`<class 'int'>`*, *optional*\t<p>A batch size to use for multiple texts</p>\n",
       "\n",
       "\n",
       " - **`grouped_entities`** : *`<class 'bool'>`*, *optional*\t<p>Return whole entity span strings</p>\n",
       "\n",
       "\n",
       " - **`detail_level`** : *`<class 'fastcore.basics.DetailLevel'>`*, *optional*\t<p>A detail level to return on the predictions</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`<class 'dict'>`*\t<p>A dictionary of filtered predictions</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TokenClassificationTuner.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb42122f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dls = tset.dataloaders()\n",
    "\n",
    "tuner = TokenClassificationTuner(dls, 'distilbert-base-uncased', metrics=['accuracy',\n",
    "                                                                         'precision',\n",
    "                                                                         'recall',\n",
    "                                                                         'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "342afcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–ˆ\r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = tuner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6570de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch     train_loss  valid_loss  accuracy  precision  recall    f1        time    \n",
      "â–ˆ\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.069511    0.075891    0.979530  0.900202   0.890779  0.894904  14:16     \n",
      "â–ˆ\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1         0.038251    0.050270    0.985926  0.932494   0.930558  0.931157  14:10     \n",
      "2         0.015244    0.049998    0.987354  0.937108   0.943109  0.939752  14:26     \n"
     ]
    }
   ],
   "source": [
    "from adaptnlp import Strategy\n",
    "\n",
    "tuner.tune(3, lr=lr, strategy=Strategy.OneCycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7de1b238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–ˆ\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('tags',\n",
       "              [[{'entity': 'B-ORG',\n",
       "                 'score': 0.9966050982475281,\n",
       "                 'word': 'nov'},\n",
       "                {'entity': 'I-ORG',\n",
       "                 'score': 0.9790855646133423,\n",
       "                 'word': '##etta'},\n",
       "                {'entity': 'B-LOC',\n",
       "                 'score': 0.9930502772331238,\n",
       "                 'word': 'mclean'}]])])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.predict('The company Novetta is based in McLean, Virginia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
