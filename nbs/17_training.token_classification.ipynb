{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ffd5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training.token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33013810",
   "metadata": {},
   "source": [
    "# Token Classification Tuning\n",
    "> Data and Tuning API for Token Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13989e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbverbose.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.xtras import Path, range_of\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.data.transforms import get_files\n",
    "\n",
    "from adaptnlp.training.core import *\n",
    "from adaptnlp.training.arrow_utils import TextNoNewLineDatasetReader\n",
    "from adaptnlp.inference.token_classification import TransformersTokenTagger, TokenClassificationResult, DetailLevel\n",
    "\n",
    "from transformers import default_data_collator, AutoModelForTokenClassification, AutoTokenizer\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "from typing import List\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d793376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_all_ = ['NERMetric']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abff10e",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b824338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def encode_tags(tags, encodings):\n",
    "    # create an empty array of -100\n",
    "    encoded_labels = np.ones(len(encodings['offset_mapping']),dtype=int) * -100\n",
    "    offset_array = np.array(encodings['offset_mapping'])\n",
    "    \n",
    "    # set labels whose first offset position is 0 and the second is not 0\n",
    "    encoded_labels[(offset_array[:,0] == 0) & (offset_array[:,1] != 0)] = tags\n",
    "        \n",
    "    return list(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4dd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _batch_tokenize(batch, tokenizer, tokenize_kwargs):\n",
    "    updated_tags = [] # list of lists\n",
    "    updated_batch = {} # dict of lists\n",
    "    \n",
    "    for tokens, tags in zip(batch['tokens'], batch['ner_tags']):\n",
    "        encodings = tokenizer(tokens, **tokenize_kwargs)\n",
    "        updated_tags.append(encode_tags(tags, encodings))\n",
    "        \n",
    "        for feat in encodings.keys():\n",
    "            if (feat != 'offset_mapping'):\n",
    "                if feat in updated_batch:\n",
    "                    updated_batch[feat].append(encodings[feat])\n",
    "                else:\n",
    "                    updated_batch[feat] = [encodings[feat]]\n",
    "                \n",
    "    updated_batch['labels'] = updated_tags\n",
    "    return updated_batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc482a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenClassificationDatasets(TaskDatasets):\n",
    "    \"\"\"\n",
    "    A set of datasets designed for token classification\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dset:Dataset, # A training dataset\n",
    "        valid_dset:Dataset, # A validation dataset\n",
    "        tokenizer_name:str, # The name of a tokenizer\n",
    "        tokenize:bool, # Whether to tokenize immediately\n",
    "        tokenize_kwargs:dict, # kwargs for the tokenize function\n",
    "        auto_kwargs:dict, # AutoTokenizer.from_pretrained kwargs\n",
    "        remove_columns:list, # The columns to remove when tokenizing\n",
    "        entity_mapping:dict, # A mapping of entity names to encoded labels\n",
    "    ):\n",
    "        \n",
    "        \"Constructs TaskDatasets, should not be called explicitly\"\n",
    "        super().__init__(\n",
    "            train_dset,\n",
    "            valid_dset,\n",
    "            tokenizer_name,\n",
    "            tokenize,\n",
    "            _batch_tokenize,\n",
    "            tokenize_kwargs,\n",
    "            auto_kwargs,\n",
    "            remove_columns,\n",
    "        )\n",
    "        self.entity_mapping = entity_mapping\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dfs(\n",
    "        cls,\n",
    "        train_df:pd.DataFrame, # A training dataframe\n",
    "        token_col:str, # The name of the token column\n",
    "        tag_col:str, # The name of the tag column\n",
    "        entity_mapping:dict, # A mapping of entity names to encoded labels\n",
    "        tokenizer_name:str, # The name of the tokenizer\n",
    "        tokenize:bool=True, # Whether to tokenize immediately\n",
    "        valid_df=None, # An optional validation dataframe\n",
    "        split_func=None, # Optionally a splitting function similar to RandomSplitter\n",
    "        split_pct=.2, # What % to split the train_df\n",
    "        tokenize_kwargs:dict={}, # kwargs for the tokenize function\n",
    "        auto_kwargs:dict={} # kwargs for the AutoTokenizer.from_pretrained constructor\n",
    "    ):\n",
    "        \"Builds `TokenClassificationDatasets` from a `DataFrame` or set of `DataFrames`\"\n",
    "        if split_func is None: split_func = RandomSplitter(split_pct)\n",
    "        if valid_df is None:\n",
    "            train_idxs, valid_idxs = split_func(train_df)\n",
    "            valid_df = train_df.iloc[valid_idxs]\n",
    "            train_df = train_df.iloc[train_idxs]\n",
    "            \n",
    "        train_df = train_df[[token_col,tag_col]]\n",
    "        valid_df = valid_df[[token_col,tag_col]]\n",
    "        train_df = train_df.rename(columns={token_col:'tokens', tag_col: 'ner_tags'})\n",
    "        valid_df = valid_df.rename(columns={token_col:'tokens', tag_col: 'ner_tags'})\n",
    "        \n",
    "        train_dset = Dataset.from_dict(train_df.to_dict('list'))\n",
    "        valid_dset = Dataset.from_dict(valid_df.to_dict('list'))\n",
    "        \n",
    "        return cls(\n",
    "            train_dset,\n",
    "            valid_dset,\n",
    "            tokenizer_name,\n",
    "            tokenize,\n",
    "            tokenize_kwargs,\n",
    "            auto_kwargs,\n",
    "            remove_columns=['tokens', 'ner_tags'],\n",
    "            entity_mapping = entity_mapping\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csvs(\n",
    "        cls,\n",
    "        train_csv:Path, # A training csv file\n",
    "        token_col:str, # The name of the token column\n",
    "        tag_col:str, # The name of the tag column\n",
    "        entity_mapping:dict, # A mapping of entity names to encoded labels\n",
    "        tokenizer_name:str, # The name of the tokenizer\n",
    "        tokenize:bool=True, # Whether to tokenize immediately\n",
    "        valid_csv:Path=None, # An optional validation csv\n",
    "        split_func=None, # Optionally a splitting function similar to RandomSplitter\n",
    "        split_pct=.2, # What % to split the train df\n",
    "        tokenize_kwargs:dict={}, # kwargs for the tokenize function\n",
    "        auto_kwargs:dict={}, # kwargs for the AutoTokenizer.from_pretrained constructor\n",
    "        **kwargs, # kwargs for `pd.read_csv`\n",
    "    ):\n",
    "        \"Builds `SequenceClassificationDatasets` from a single csv or set of csvs. A convience constructor for `from_dfs`\"\n",
    "        train_df = pd.read_csv(train_csv, **kwargs)\n",
    "        if valid_csv is not None: valid_df = pd.read_csv(valid_csv, **kwargs)\n",
    "        else: valid_df = None\n",
    "        return cls.from_dfs(\n",
    "            train_df,\n",
    "            token_col,\n",
    "            tag_col,\n",
    "            entity_mapping,\n",
    "            tokenizer_name,\n",
    "            tokenize,\n",
    "            valid_df,\n",
    "            split_func,\n",
    "            split_pct,\n",
    "            tokenize_kwargs,\n",
    "            auto_kwargs\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @delegates(DataLoaders)\n",
    "    def dataloaders(\n",
    "        self,\n",
    "        batch_size:int=16, # A batch size\n",
    "        shuffle_train:bool=True, # Whether to shuffle the training dataset\n",
    "        collate_fn:callable=None, # A custom collation function\n",
    "        **kwargs # Torch DataLoader kwargs\n",
    "    ):\n",
    "        \"Build DataLoaders from `self`\"\n",
    "        dls = super().dataloaders(batch_size, shuffle_train, collate_fn, **kwargs)\n",
    "        dls.entity_mapping = self.entity_mapping\n",
    "        return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-teaching",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TokenClassificationDatasets.from_dfs\" class=\"doc_header\"><code>TokenClassificationDatasets.from_dfs</code><a href=\"__main__.py#L31\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TokenClassificationDatasets.from_dfs</code>(**`train_df`**:`DataFrame`, **`token_col`**:`str`, **`tag_col`**:`str`, **`entity_mapping`**:`dict`, **`tokenizer_name`**:`str`, **`tokenize`**:`bool`=*`True`*, **`valid_df`**=*`None`*, **`split_func`**=*`None`*, **`split_pct`**=*`0.2`*, **`tokenize_kwargs`**:`dict`=*`{}`*, **`auto_kwargs`**:`dict`=*`{}`*)\n",
       "\n",
       "Builds [`TokenClassificationDatasets`](/adaptnlp/training.token_classification.html#TokenClassificationDatasets) from a `DataFrame` or set of `DataFrames`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`train_df`** : *`<class 'pandas.core.frame.DataFrame'>`*\t<p>A training dataframe</p>\n",
       "\n",
       "\n",
       " - **`token_col`** : *`<class 'str'>`*\t<p>The name of the token column</p>\n",
       "\n",
       "\n",
       " - **`tag_col`** : *`<class 'str'>`*\t<p>The name of the tag column</p>\n",
       "\n",
       "\n",
       " - **`entity_mapping`** : *`<class 'dict'>`*\t<p>A mapping of entity names to encoded labels</p>\n",
       "\n",
       "\n",
       " - **`tokenizer_name`** : *`<class 'str'>`*\t<p>The name of the tokenizer</p>\n",
       "\n",
       "\n",
       " - **`tokenize`** : *`<class 'bool'>`*, *optional*\t<p>Whether to tokenize immediately</p>\n",
       "\n",
       "\n",
       " - **`valid_df`** : *`<class 'NoneType'>`*, *optional*\t<p>An optional validation dataframe</p>\n",
       "\n",
       "\n",
       " - **`split_func`** : *`<class 'NoneType'>`*, *optional*\t<p>Optionally a splitting function similar to RandomSplitter</p>\n",
       "\n",
       "\n",
       " - **`split_pct`** : *`<class 'float'>`*, *optional*\t<p>What % to split the train_df</p>\n",
       "\n",
       "\n",
       " - **`tokenize_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>kwargs for the tokenize function</p>\n",
       "\n",
       "\n",
       " - **`auto_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>kwargs for the AutoTokenizer.from_pretrained constructor</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TokenClassificationDatasets.from_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-marina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TokenClassificationDatasets.from_csvs\" class=\"doc_header\"><code>TokenClassificationDatasets.from_csvs</code><a href=\"__main__.py#L72\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TokenClassificationDatasets.from_csvs</code>(**`train_csv`**:`Path`, **`token_col`**:`str`, **`tag_col`**:`str`, **`entity_mapping`**:`dict`, **`tokenizer_name`**:`str`, **`tokenize`**:`bool`=*`True`*, **`valid_csv`**:`Path`=*`None`*, **`split_func`**=*`None`*, **`split_pct`**=*`0.2`*, **`tokenize_kwargs`**:`dict`=*`{}`*, **`auto_kwargs`**:`dict`=*`{}`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Builds [`SequenceClassificationDatasets`](/adaptnlp/training.sequence_classification.html#SequenceClassificationDatasets) from a single csv or set of csvs. A convience constructor for `from_dfs`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`train_csv`** : *`<class 'pathlib.Path'>`*\t<p>A training csv file</p>\n",
       "\n",
       "\n",
       " - **`token_col`** : *`<class 'str'>`*\t<p>The name of the token column</p>\n",
       "\n",
       "\n",
       " - **`tag_col`** : *`<class 'str'>`*\t<p>The name of the tag column</p>\n",
       "\n",
       "\n",
       " - **`entity_mapping`** : *`<class 'dict'>`*\t<p>A mapping of entity names to encoded labels</p>\n",
       "\n",
       "\n",
       " - **`tokenizer_name`** : *`<class 'str'>`*\t<p>The name of the tokenizer</p>\n",
       "\n",
       "\n",
       " - **`tokenize`** : *`<class 'bool'>`*, *optional*\t<p>Whether to tokenize immediately</p>\n",
       "\n",
       "\n",
       " - **`valid_csv`** : *`<class 'pathlib.Path'>`*, *optional*\t<p>An optional validation csv</p>\n",
       "\n",
       "\n",
       " - **`split_func`** : *`<class 'NoneType'>`*, *optional*\t<p>Optionally a splitting function similar to RandomSplitter</p>\n",
       "\n",
       "\n",
       " - **`split_pct`** : *`<class 'float'>`*, *optional*\t<p>What % to split the train df</p>\n",
       "\n",
       "\n",
       " - **`tokenize_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>kwargs for the tokenize function</p>\n",
       "\n",
       "\n",
       " - **`auto_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>kwargs for the AutoTokenizer.from_pretrained constructor</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TokenClassificationDatasets.from_csvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-beads",
   "metadata": {},
   "source": [
    "When passing in kwargs if anything should go to the `tokenize` function they should go to `tokenize_kwargs`, and if it should go to the `Auto` class constructor, they should go to `auto_kwargs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f7237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53875b8054e84b3fbd6c56260b316fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2603.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e40790567d4ea7afdbbd34fcb4902a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1781.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset conll2003/conll2003 (download: 4.63 MiB, generated: 9.78 MiB, post-processed: Unknown size, total: 14.41 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67ef74fe55441609c4f71c11c40ff1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=649539.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05f90579a1f4400b250748b34c6593c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=162714.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db61342412434098af5d4ab7db1c6af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=145897.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4ff29d1454499199b3e93a649cef77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7391b9317784bfeaf7d3a4d5287f358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=483.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8954b10b92640a4a402115aab5a01d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d31cdd9a574794a288cd3964cb3ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No value for `max_length` set, automatically adjusting to the size of the model and including truncation\n",
      "Sequence length set to: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12e415b49a04df7a454f84df6efacf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a4534ab07e4f4fb49a34b565fb937c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from datasets import load_dataset\n",
    "\n",
    "# converting original HF dataset to pandas dataframe\n",
    "dsets = load_dataset('conll2003')\n",
    "dset = dsets['train']\n",
    "dset.set_format(type='pandas')\n",
    "df = dset[:]\n",
    "df = df[['tokens', 'ner_tags']]\n",
    "\n",
    "# create entity mapping dict\n",
    "entity_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# set tokenizer arguments\n",
    "tokenize_kwargs = {\n",
    "    'truncation':True, \n",
    "    'is_split_into_words':True, \n",
    "    'padding':'max_length', \n",
    "    'return_offsets_mapping':True\n",
    "}\n",
    "\n",
    "tset = TokenClassificationDatasets.from_dfs(\n",
    "    df,\n",
    "    'tokens',\n",
    "    'ner_tags',\n",
    "    entity_mapping,\n",
    "    tokenizer_name = 'distilbert-base-uncased',\n",
    "    tokenize=True,\n",
    "    tokenize_kwargs = tokenize_kwargs\n",
    ")\n",
    "\n",
    "test_eq(len(tset.train), 11233)\n",
    "test_eq(len(tset.valid), 2808)\n",
    "test_eq(tset.train[0].keys(), ['attention_mask', 'input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No value for `max_length` set, automatically adjusting to the size of the model and including truncation\n",
      "Sequence length set to: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d180b56f6919466bbf206d5f08c0cb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a763c77d8ec2448995ab7234167436c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from datasets import load_dataset\n",
    "\n",
    "# converting original HF dataset to pandas dataframe\n",
    "dsets = load_dataset('conll2003')\n",
    "dset = dsets['train']\n",
    "dset.set_format(type='pandas')\n",
    "df = dset[:]\n",
    "df = df[['tokens', 'ner_tags']]\n",
    "train_df = df.iloc[:11233]\n",
    "valid_df = df.iloc[11233:]\n",
    "\n",
    "# create entity mapping dict\n",
    "entity_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# set tokenizer arguments\n",
    "tokenize_kwargs = {\n",
    "    'truncation':True, \n",
    "    'is_split_into_words':True, \n",
    "    'padding':'max_length', \n",
    "    'return_offsets_mapping':True\n",
    "}\n",
    "\n",
    "tset = TokenClassificationDatasets.from_dfs(\n",
    "    train_df,\n",
    "    'tokens',\n",
    "    'ner_tags',\n",
    "    entity_mapping,\n",
    "    valid_df=valid_df,\n",
    "    tokenizer_name = 'distilbert-base-uncased',\n",
    "    tokenize=True,\n",
    "    tokenize_kwargs = tokenize_kwargs\n",
    ")\n",
    "\n",
    "test_eq(len(tset.train), 11233)\n",
    "test_eq(len(tset.valid), 2808)\n",
    "test_eq(tset.train[0].keys(), ['attention_mask', 'input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No value for `max_length` set, automatically adjusting to the size of the model and including truncation\n",
      "Sequence length set to: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1507cefa312c4e6987ac99fa4b943f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0584774f4db4ce99d345e564cf2afee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from datasets import load_dataset\n",
    "\n",
    "# converting original HF dataset to csv\n",
    "dsets = load_dataset('conll2003')\n",
    "dset = dsets['train']\n",
    "dset.set_format(type='pandas')\n",
    "df = dset[:]\n",
    "df = df[['tokens', 'ner_tags']]\n",
    "df['ner_tags'] = df['ner_tags'].apply(lambda x : list(x))\n",
    "df['tokens'] = df['tokens'].apply(lambda x : list(x))\n",
    "df.to_csv('/tmp/conll2003.csv', index=False)\n",
    "\n",
    "# create entity mapping dict\n",
    "entity_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# set tokenizer arguments\n",
    "tokenize_kwargs = {\n",
    "    'truncation':True, \n",
    "    'is_split_into_words':True, \n",
    "    'padding':'max_length', \n",
    "    'return_offsets_mapping':True\n",
    "}\n",
    "\n",
    "converters={'col1': literal_eval}\n",
    "\n",
    "tset = TokenClassificationDatasets.from_csvs(\n",
    "    '/tmp/conll2003.csv',\n",
    "    'tokens',\n",
    "    'ner_tags',\n",
    "    entity_mapping,\n",
    "    tokenizer_name = 'distilbert-base-uncased',\n",
    "    tokenize=True,\n",
    "    tokenize_kwargs = tokenize_kwargs,\n",
    "    converters={'tokens': literal_eval, 'ner_tags': literal_eval}  # kwarg to pd.read_csv\n",
    ")\n",
    "\n",
    "test_eq(len(tset.train), 11233)\n",
    "test_eq(len(tset.valid), 2808)\n",
    "test_eq(tset.train[0].keys(), ['attention_mask', 'input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-mainstream",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No value for `max_length` set, automatically adjusting to the size of the model and including truncation\n",
      "Sequence length set to: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c7960322a6427f9c5d460f6458370f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8d69b6c8384071b1d9dd318eaccbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from datasets import load_dataset\n",
    "\n",
    "# converting original HF dataset to csv\n",
    "dsets = load_dataset('conll2003')\n",
    "dset = dsets['train']\n",
    "dset.set_format(type='pandas')\n",
    "df = dset[:]\n",
    "df = df[['tokens', 'ner_tags']]\n",
    "df['ner_tags'] = df['ner_tags'].apply(lambda x : list(x))\n",
    "df['tokens'] = df['tokens'].apply(lambda x : list(x))\n",
    "train_df = df.iloc[:11233]\n",
    "valid_df = df.iloc[11233:]\n",
    "train_df.to_csv('/tmp/train.csv', index=False)\n",
    "valid_df.to_csv('/tmp/valid.csv', index=False)\n",
    "\n",
    "# create entity mapping dict\n",
    "entity_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# set tokenizer arguments\n",
    "tokenize_kwargs = {\n",
    "    'truncation':True, \n",
    "    'is_split_into_words':True, \n",
    "    'padding':'max_length', \n",
    "    'return_offsets_mapping':True\n",
    "}\n",
    "\n",
    "converters={'col1': literal_eval}\n",
    "\n",
    "tset = TokenClassificationDatasets.from_csvs(\n",
    "    '/tmp/train.csv',\n",
    "    'tokens',\n",
    "    'ner_tags',\n",
    "    entity_mapping,\n",
    "    valid_csv = '/tmp/valid.csv',\n",
    "    tokenizer_name = 'distilbert-base-uncased',\n",
    "    tokenize=True,\n",
    "    tokenize_kwargs = tokenize_kwargs,\n",
    "    converters={'tokens': literal_eval, 'ner_tags': literal_eval}  # kwarg to pd.read_csv\n",
    ")\n",
    "\n",
    "test_eq(len(tset.train), 11233)\n",
    "test_eq(len(tset.valid), 2808)\n",
    "test_eq(tset.train[0].keys(), ['attention_mask', 'input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243cb632",
   "metadata": {},
   "source": [
    "## Token Classification Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqEvalMetrics():\n",
    "    \"\"\"\n",
    "    Multi-label classification metrics for NER, using seqeval metric from HuggingFace\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        entity_mapping:dict, # A mapping of entity names to encoded labels\n",
    "    ):  \n",
    "        self.metric = load_metric(\"seqeval\")\n",
    "        self.label_list = list(entity_mapping.values())\n",
    "    \n",
    "    # source: https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb#scrollTo=-1bS-ATSqPq2\n",
    "    def compute_metrics(\n",
    "        self, \n",
    "        preds, # Predictions output by the model\n",
    "        labels # Ground truth target values\n",
    "    ):\n",
    "        \"Computes multi-label classification metrics for NER\"\n",
    "        predictions = preds.argmax(2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions = [\n",
    "            [self.label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [self.label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        results = self.metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        return results\n",
    "    \n",
    "    def accuracy(\n",
    "        self, \n",
    "        preds, # Predictions output by the model\n",
    "        labels # Ground truth target values\n",
    "    ):\n",
    "        \"Computes multi-label accuracy for NER\"\n",
    "        results = self.compute_metrics(preds, labels)\n",
    "        return results['overall_accuracy']\n",
    "    \n",
    "    def precision(\n",
    "        self, \n",
    "        preds, # Predictions output by the model\n",
    "        labels # Ground truth target values\n",
    "    ):\n",
    "        \"Computes multi-label precision for NER\"\n",
    "        results = self.compute_metrics(preds, labels)\n",
    "        return results['overall_precision']\n",
    "    \n",
    "    def recall(\n",
    "        self, \n",
    "        preds, # Predictions output by the model\n",
    "        labels # Ground truth target values\n",
    "    ):\n",
    "        \"Computes multi-label recall for NER\"\n",
    "        results = self.compute_metrics(preds, labels)\n",
    "        return results['overall_recall']\n",
    "    \n",
    "    def f1(\n",
    "        self, \n",
    "        preds, # Predictions output by the model\n",
    "        labels # Ground truth target values\n",
    "    ):\n",
    "        \"Computes multi-label F1 for NER\"\n",
    "        results = self.compute_metrics(preds, labels)\n",
    "        return results['overall_f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce878a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mk_class('NERMetric', **{prop.title():prop for prop in dir(SeqEvalMetrics) \n",
    "                         if not prop.startswith('_') and prop != 'compute_metrics'},\n",
    "        doc=\"Class for all valid NER metrics usable during fine-tuning with typo-proofing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00585a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"NERMetric\" class=\"doc_header\"><code>class</code> <code>NERMetric</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>NERMetric</code>(**\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Class for all valid NER metrics usable during fine-tuning with typo-proofing\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`args`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(NERMetric, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9d54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported metrics:\n",
      "* Accuracy\n",
      "* F1\n",
      "* Precision\n",
      "* Recall\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f'Supported metrics:')\n",
    "for val in dir(NERMetric):\n",
    "    if not val.startswith('_'):\n",
    "        print(f'* {val.title()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0796d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenClassificationTuner(AdaptiveTuner):\n",
    "    \"\"\"\n",
    "    An `AdaptiveTuner` with good defaults for Token Classification tasks\n",
    "    \n",
    "    **Valid kwargs and defaults:**\n",
    "      - `lr`:float = 0.001\n",
    "      - `splitter`:function = `trainable_params`\n",
    "      - `cbs`:list = None\n",
    "      - `path`:Path = None\n",
    "      - `model_dir`:Path = 'models'\n",
    "      - `wd`:float = None\n",
    "      - `wd_bn_bias`:bool = False\n",
    "      - `train_bn`:bool = True\n",
    "      - `moms`: tuple(float) = (0.95, 0.85, 0.95)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dls:DataLoaders, # A set of DataLoaders\n",
    "        model_name:str, # A HuggingFace model\n",
    "        tokenizer = None, # A HuggingFace tokenizer\n",
    "        loss_func = CrossEntropyLossFlat(), # A loss function\n",
    "        metrics:List[NERMetric] = [NERMetric.Accuracy, NERMetric.F1], # Metrics to monitor the training with\n",
    "        opt_func = Adam, # A fastai or torch Optimizer\n",
    "        additional_cbs = None, # Additional Callbacks to have always tied to the Tuner\n",
    "        expose_fastai_api = False, # Whether to expose the fastai API\n",
    "        num_classes:int=None, # The number of classes\n",
    "        entity_mapping:dict=None, # A mapping of entity names to encoded labels\n",
    "        **kwargs, # kwargs for `Learner.__init__`\n",
    "    ):\n",
    "        additional_cbs = listify(additional_cbs)\n",
    "        for arg in 'dls,model,loss_func,metrics,opt_func,cbs,expose_fastai'.split(','): \n",
    "            if arg in kwargs.keys(): kwargs.pop(arg) # Pop all existing kwargs\n",
    "        if hasattr(dls, 'entity_mapping'): num_classes = len(dls.entity_mapping)\n",
    "        else: raise ValueError('Could not extract entity mapping from DataLoaders, please pass it in as a param')\n",
    "        if num_classes is None: raise ValueError('Could not extrapolate number of classes, please pass it in as a param')\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=num_classes, \n",
    "            id2label=dls.entity_mapping\n",
    "        )\n",
    "        if tokenizer is None: tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self._base_metrics = SeqEvalMetrics(dls.entity_mapping)\n",
    "        new_metrics = []\n",
    "        for met in metrics:\n",
    "            ner_met = getattr(NERMetric, met.title(), None)\n",
    "            if not ner_met: raise ValueError('Metric not supported')\n",
    "            else: new_metrics.append(getattr(self._base_metrics, ner_met))\n",
    "        \n",
    "        super().__init__(\n",
    "            expose_fastai_api,\n",
    "            dls = dls, \n",
    "            model = model, \n",
    "            tokenizer = tokenizer,\n",
    "            loss_func = loss_func, \n",
    "            metrics = new_metrics, \n",
    "            opt_func = opt_func, \n",
    "            cbs=additional_cbs, \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "    def predict(\n",
    "        self,\n",
    "        text:Union[List[str], str], # Some text or list of texts to do inference with\n",
    "        bs:int=64, # A batch size to use for multiple texts\n",
    "        grouped_entities: bool = True, # Return whole entity span strings\n",
    "        detail_level:DetailLevel = DetailLevel.Low, # A detail level to return on the predictions\n",
    "    ) -> dict: # A dictionary of filtered predictions\n",
    "        \"Predict some `text` for token classification with the currently loaded model\"\n",
    "        if getattr(self, '_inferencer', None) is None: self._inferencer = TransformersTokenTagger(self.tokenizer, self.model)\n",
    "        \n",
    "        preds = self._inferencer.predict(text, bs, grouped_entities, detail_level)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TokenClassificationTuner.predict\" class=\"doc_header\"><code>TokenClassificationTuner.predict</code><a href=\"__main__.py#L65\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TokenClassificationTuner.predict</code>(**`text`**:`Union`\\[`List`\\[`str`\\], `str`\\], **`bs`**:`int`=*`64`*, **`grouped_entities`**:`bool`=*`True`*, **`detail_level`**:`DetailLevel`=*`'low'`*)\n",
       "\n",
       "Predict some `text` for token classification with the currently loaded model\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`text`** : *`typing.Union[typing.List[str], str]`*\t<p>Some text or list of texts to do inference with</p>\n",
       "\n",
       "\n",
       " - **`bs`** : *`<class 'int'>`*, *optional*\t<p>A batch size to use for multiple texts</p>\n",
       "\n",
       "\n",
       " - **`grouped_entities`** : *`<class 'bool'>`*, *optional*\t<p>Return whole entity span strings</p>\n",
       "\n",
       "\n",
       " - **`detail_level`** : *`<class 'fastcore.basics.DetailLevel'>`*, *optional*\t<p>A detail level to return on the predictions</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`<class 'dict'>`*\t<p>A dictionary of filtered predictions</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TokenClassificationTuner.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42122f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "dls = tset.dataloaders()\n",
    "\n",
    "metrics = [NERMetric.Accuracy, NERMetric.Precision, NERMetric.Recall, NERMetric.F1]\n",
    "\n",
    "tuner = TokenClassificationTuner(dls, 'distilbert-base-uncased', \n",
    "                                 metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342afcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█\r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "lr = tuner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6570de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/3 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='703' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/703 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_269/487201369.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0madaptnlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneCycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jovyan/adaptnlp/adaptnlp/training/core.py\u001b[0m in \u001b[0;36mtune\u001b[0;34m(self, epochs, lr, strategy, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'epochs,lr,cbs'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdelegates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/fastai/callback/schedule.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    111\u001b[0m     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n\u001b[1;32m    112\u001b[0m               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mParamScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_hypers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelFitException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_cleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_end_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_one_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/adaptnlp/adaptnlp/training/core.py\u001b[0m in \u001b[0;36m_do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# See if `to_device` fixes this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'loss'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         outputs = self.distilbert(\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             )\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \"\"\"\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m# Self-Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         sa_output = self.attention(\n\u001b[0m\u001b[1;32m    272\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# Mask heads if we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from adaptnlp import Strategy\n",
    "\n",
    "tuner.tune(3, lr=lr, strategy=Strategy.OneCycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1b238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('tags',\n",
       "              [[{'entity': 'B-ORG',\n",
       "                 'score': 0.9966050982475281,\n",
       "                 'word': 'nov'},\n",
       "                {'entity': 'I-ORG',\n",
       "                 'score': 0.9790855646133423,\n",
       "                 'word': '##etta'},\n",
       "                {'entity': 'B-LOC',\n",
       "                 'score': 0.9930502772331238,\n",
       "                 'word': 'mclean'}]])])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "tuner.predict('The company Novetta is based in McLean, Virginia')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
