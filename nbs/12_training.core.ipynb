{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2449173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training.core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e0771",
   "metadata": {},
   "source": [
    "# Training Foundations\n",
    "> Basic classes and helpers for modularized training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbverbose.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.xtras import Path, range_of # pathlib `Path` with extra bits\n",
    "from fastcore.foundation import mask2idxs, L\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.basics import mk_class, listify\n",
    "\n",
    "from fastai.torch_core import display_df\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.core import CancelStepException\n",
    "from fastai.callback.hook import Learner\n",
    "from fastai.callback.progress import Learner\n",
    "from fastai.callback.schedule import Learner\n",
    "\n",
    "from functools import partial\n",
    "# Patch'd Learner functionalities\n",
    "\n",
    "from fastai.data.core import DataLoaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import default_data_collator, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc446184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_all_ = ['Strategy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ParentLabeller:\n",
    "    \"\"\"\n",
    "    Extracts class based on filename's parent at `level`\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        level=1 # The level up from `fname` to find the label\n",
    "    ):\n",
    "        self.level = level\n",
    "        \n",
    "    def __call__(self, o:Path): return self._do_level(o, self.level)\n",
    "    \n",
    "    def _do_level(self, o:Path, level:int):\n",
    "        \"Goes down one level on parent\"\n",
    "        def _inner(a): return a.parent\n",
    "        if level == 1: return o.parent.name\n",
    "        else: return self._do_level(_inner(o), level - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path = Path('a/b/c/d/text.txt')\n",
    "get_p = ParentLabeller()\n",
    "test_eq('d', get_p(path))\n",
    "get_p.level = 2\n",
    "test_eq('c', get_p(path))\n",
    "get_p.level = 3\n",
    "test_eq('b', get_p(path))\n",
    "get_p.level = 4\n",
    "test_eq('a', get_p(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41788ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ColReader:\n",
    "    \"\"\"\n",
    "    Reads `cols` in `row` with potential `pref` and `suff`\n",
    "    Based on the fastai class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cols, # Some column names to use\n",
    "        pref:str='', # A prefix\n",
    "        suff:str='', # A suffix\n",
    "        label_delim:str=None, # A label delimiter\n",
    "    ):\n",
    "        self.pref = str(pref) + os.path.sep if isinstance(pref, Path) else pref\n",
    "        self.suff, self.label_delim = suff, label_delim\n",
    "        self.cols = L(cols)\n",
    "    \n",
    "    def _do_one(self, r, c):\n",
    "        o = r[c] if isinstance(c,int) else r[c] if c=='name' or c=='cat' else getattr(r,c)\n",
    "        if len(self.pref)==0 and len(self.suff)==0 and self.label_delim is None: return o\n",
    "        if self.label_delim is None: return f'{self.pref}{o}{self.suff}'\n",
    "        else: return o.split(self.label_delim) if len(o)>0 else []\n",
    "    \n",
    "    def __call__(self, o):\n",
    "        if len(self.cols) == 1: return self._do_one(o, self.cols[0])\n",
    "        return L(self._do_one(o,c) for c in self.cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994171b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([[0, 'a'], [1, 'b'], [2, 'c']], columns=['number', 'letter'])\n",
    "num_reader = ColReader('number')\n",
    "let_reader = ColReader('letter')\n",
    "\n",
    "test_eq(list(num_reader(df)), [0,1,2])\n",
    "test_eq(list(let_reader(df)), ['a', 'b', 'c'])\n",
    "\n",
    "# Test we will return two lists\n",
    "reader = ColReader(['number', 'letter'])\n",
    "n,l = reader(df)\n",
    "test_eq(list(n), [0,1,2])\n",
    "test_eq(list(l), ['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee36e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Categorize:\n",
    "    \"\"\"\n",
    "    Collection of categories with reverse mapping in `o2i`\n",
    "    Based on the fastai class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        names, # An interable collection of items to create a vocab from\n",
    "        sort=True # Whether to make the items sorted\n",
    "    ):\n",
    "        names = L(names)\n",
    "        self.classes = L(o for o in names.unique() if o == o)\n",
    "        if sort: self.classes = self.classes.sorted()\n",
    "        self.o2i = dict(self.classes.val2idx())\n",
    "        \n",
    "    def map_objs(\n",
    "        self, \n",
    "        objs # Some iterable collection\n",
    "    ) -> L:\n",
    "        \"Map `objs` to IDs\"\n",
    "        return L(self.o2i[o] for o in objs)\n",
    "    \n",
    "    def map_ids(\n",
    "        self, \n",
    "        ids # Some ids correlating to `self.classes`\n",
    "    ) -> L:\n",
    "        \"Map `ids` to objects in vocab\"\n",
    "        return L(self.classes[o] for o in ids)\n",
    "    \n",
    "    def __call__(self, o): \n",
    "        \"Label encode a single `o`\"\n",
    "        return int(self.o2i[o])\n",
    "    \n",
    "    def decode(\n",
    "        self, \n",
    "        o # A key in self.classes\n",
    "    ): \n",
    "        \"Decodes `o` by looking in `self.classes`\"\n",
    "        return self.classes[o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f760a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "cat = Categorize(['a','b','c'])\n",
    "\n",
    "test_eq(cat('a'), 0)\n",
    "\n",
    "test_eq(cat.map_objs(['a','b','c']), L(0,1,2))\n",
    "test_eq(cat.map_ids([0,1,2]), L('a','b','c'))\n",
    "test_eq(cat.decode(0), 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130fb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Categorize.map_objs\" class=\"doc_header\"><code>Categorize.map_objs</code><a href=\"__main__.py#L17\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Categorize.map_objs</code>(**`objs`**)\n",
       "\n",
       "Map `objs` to IDs\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`objs`** : *`<class 'inspect._empty'>`*\t<p>Some iterable collection</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`<class 'fastcore.foundation.L'>`*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Categorize.map_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0dac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Categorize.map_ids\" class=\"doc_header\"><code>Categorize.map_ids</code><a href=\"__main__.py#L24\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Categorize.map_ids</code>(**`ids`**)\n",
       "\n",
       "Map `ids` to objects in vocab\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`ids`** : *`<class 'inspect._empty'>`*\t<p>Some ids correlating to `self.classes`</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`<class 'fastcore.foundation.L'>`*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Categorize.map_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de1527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Categorize.decode\" class=\"doc_header\"><code>Categorize.decode</code><a href=\"__main__.py#L35\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Categorize.decode</code>(**`o`**)\n",
       "\n",
       "Decodes `o` by looking in `self.classes`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`o`** : *`<class 'inspect._empty'>`*\t<p>A key in self.classes</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Categorize.decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiCategorize(Categorize):\n",
    "    \"\"\"\n",
    "    Collection of multi-categories with reverse mapping in `o2i`\n",
    "    Based on the fastai class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        names, # An interable collection of items to create a vocab from\n",
    "    ):\n",
    "        super().__init__(names=names, sort=names==None)\n",
    "    \n",
    "    def __call__(self, o): \n",
    "        \"Label encode a single `o`\"\n",
    "        if not all(nm in self.o2i.keys() for nm in o):\n",
    "            diff = [e for e in o if e not in self.o2i.keys()]\n",
    "            diff_str = \"', '\".join(diff)\n",
    "            raise KeyError(f\"Labels '{diff_str}' were not included in the training dataset\")\n",
    "            \n",
    "        return [int(self.o2i[o_]) for o_ in o]\n",
    "    \n",
    "    def decode(\n",
    "        self, \n",
    "        o # A list of keys in self.classes\n",
    "    ) -> list: \n",
    "        \"Decodes `o` by looking in `self.classes`\"\n",
    "        return [self.classes[o_] for o_ in o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4accfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"MultiCategorize.decode\" class=\"doc_header\"><code>MultiCategorize.decode</code><a href=\"__main__.py#L22\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>MultiCategorize.decode</code>(**`o`**)\n",
       "\n",
       "Decodes `o` by looking in `self.classes`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`o`** : *`<class 'inspect._empty'>`*\t<p>A list of keys in self.classes</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`<class 'list'>`*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MultiCategorize.decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1468ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "multi_cat = MultiCategorize(['a','b','c'])\n",
    "\n",
    "test_eq(multi_cat(['a','c']), [0,2])\n",
    "test_eq(multi_cat.decode([0,1]), ['a','b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a6c0e",
   "metadata": {},
   "source": [
    "## Splitters\n",
    "\n",
    "Functions designed for splitting your data\n",
    "\n",
    "To write your own you should make a function that returns two `L`'s of indicies (or `lists` work as well)\n",
    "\n",
    "For example, if I have a dataset of 5 items, we start with `[0,1,2,3,4]`. If I wanted to write a split function to split the first three and last two items into train and validation, I can write it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_func(idxs): return L(idxs[:3]), L(idxs[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09fa404",
   "metadata": {},
   "source": [
    "And we can see it work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70252472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#3) [0,1,2], (#2) [3,4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_func([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def RandomSplitter(valid_pct=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Creates a function that splits some items between train and validation with `valid_pct` randomly\n",
    "    Based on the fastai class\n",
    "    \"\"\"\n",
    "    def _inner(o):\n",
    "        if seed is not None: torch.manual_seed(seed)\n",
    "        rand_idx = L(list(torch.randperm(len(o)).numpy()))\n",
    "        cut = int(valid_pct * len(o))\n",
    "        return rand_idx[cut:], rand_idx[:cut]\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "splitter = RandomSplitter(valid_pct=0.2)\n",
    "items = [0,1,2,3,4,5,6,7,8,9]\n",
    "res = splitter(items)\n",
    "test_eq(len(res[0]), 8)\n",
    "test_eq(len(res[1]), 2)\n",
    "test_eq(len(res), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _base_tok(item, tokenizer, tokenize_kwargs): return tokenizer(item, **tokenize_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TaskDatasets:\n",
    "    \"\"\"\n",
    "    A set of datasets for a particular task, with a simple API.\n",
    "    \n",
    "    Note: This is the base API, `items` should be a set of regular text and model-ready labels,\n",
    "          including label or one-hot encoding being applied.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dset, # A train `Dataset` object\n",
    "        valid_dset, # A validation `Dataset` object\n",
    "        tokenizer_name:str = None, # The string name of a `HuggingFace` tokenizer or model. If `None`, will not tokenize the dataset.\n",
    "        tokenize:bool = True, # Whether to tokenize the dataset immediatly\n",
    "        tokenize_func:callable = None, # A function to tokenize an item with\n",
    "        tokenize_kwargs:dict = {}, # Some kwargs for when we call the tokenizer\n",
    "        auto_kwargs:dict = {}, # Some kwargs when calling `AutoTokenizer.from_pretrained`\n",
    "        remove_cols:Union[str,List[str]] = None, # What columns to remove\n",
    "    ):\n",
    "        self.train = train_dset\n",
    "        self.valid = valid_dset\n",
    "        self.tokenizer = None\n",
    "        self.remove_cols = listify(remove_cols)\n",
    "        if tokenizer_name is not None: self.set_tokenizer(tokenizer_name, **auto_kwargs)\n",
    "        if tokenize_func is not None: self.tok_func = tokenize_func\n",
    "        else: self.tok_func = _base_tok\n",
    "        if self.tokenizer:\n",
    "            if 'max_length' in tokenize_kwargs.keys() and self.tokenizer.model_max_length >= tokenize_kwargs['max_length']: pass\n",
    "            elif 'max_length' in tokenize_kwargs.keys() and self.tokenizer.model_max_length < tokenize_kwargs['max_length']:\n",
    "                print(\"Warning: `max_length` is larger than the pretrained model\")\n",
    "            elif 'max_length' not in tokenize_kwargs.keys():\n",
    "                print(\"No value for `max_length` set, automatically adjusting to the size of the model and including truncation\")\n",
    "                tokenize_kwargs['max_length'] = self.tokenizer.model_max_length\n",
    "                tokenize_kwargs['truncation'] = True\n",
    "                print(f\"Sequence length set to: {tokenize_kwargs['max_length']}\")\n",
    "        self.tokenize_kwargs = tokenize_kwargs\n",
    "        if tokenize and self.tokenizer is not None: self._tokenize()\n",
    "        elif tokenize and self.tokenizer is None:\n",
    "            print(\"Tried to tokenize a dataset without a tokenizer. Please set a tokenizer with `set_tokenizer` and call `_tokenize()`\")\n",
    "        \n",
    "            \n",
    "    def __getitem__(self, idx): return self.train[idx]\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        \"Tokenize dataset in `self.items` with `kwargs` for `tokenize()`\"\n",
    "        if not self.tokenizer: raise ValueError(\"Tried to tokenize a dataset without a tokenizer. Please add a tokenizer with `set_tokenizer(tokenizer_name` and try again\")\n",
    "        f = partial(self.tok_func, tokenizer=self.tokenizer, tokenize_kwargs=self.tokenize_kwargs)\n",
    "        self.train = self.train.map(f,batched=True,remove_columns = self.remove_cols)\n",
    "        self.valid = self.valid.map(f,batched=True,remove_columns = self.remove_cols)\n",
    "    \n",
    "    @delegates(AutoTokenizer.from_pretrained)\n",
    "    def set_tokenizer(\n",
    "        self,\n",
    "        tokenizer_name:str, # A string name of a `HuggingFace` tokenizer or model\n",
    "        override_existing:bool = False, # Whether to override an existing tokenizer\n",
    "        **kwargs # kwargs to go to `AutoTokenizer.from_pretrained`\n",
    "    ):\n",
    "        \"Sets a new `AutoTokenizer` to `self.tokenizer`\"\n",
    "        if self.tokenizer and not override_existing:\n",
    "            print(f'Warning! You are trying to override an existing tokenizer: {self.tokenizer.name_or_path}. Pass `override_existing=True` to use a new tokenizer')\n",
    "            return\n",
    "        elif self.tokenizer and override_existing:\n",
    "            print(f'Setting new tokenizer to {tokenizer_name}')\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, **kwargs)\n",
    "        except:\n",
    "            raise ValueError(f'{tokenizer_name} is not a valid pretrained model on the HuggingFace Hub or a local model')\n",
    "    \n",
    "    def set_classes(\n",
    "        self,\n",
    "        classes:list, # An ordered list of class names\n",
    "    ):\n",
    "        \"Override the class labels in `self.categorize` it exists, otherwise make it\"\n",
    "        if hasasttr(self, 'categorize'):\n",
    "            self.categorize.classes = L(classes).sorted()\n",
    "            self.categorize.o2i = dict(self.categorize.val2idx())\n",
    "        else:\n",
    "            self.categorize = Categorize(classes)\n",
    "            \n",
    "    \n",
    "    @delegates(DataLoaders)\n",
    "    def dataloaders(\n",
    "        self, \n",
    "        batch_size:int=8, # A batch size\n",
    "        shuffle_train:bool=True, # Whether to shuffle the training dataset\n",
    "        collate_fn:callable = None, # A custom collation function\n",
    "        **kwargs # Torch DataLoader kwargs\n",
    "    ):\n",
    "        \"Creates `DataLoaders` from the dataset\"\n",
    "        if collate_fn is None: collate_fn = default_data_collator\n",
    "        train_dl = DataLoader(self.train, shuffle=shuffle_train, collate_fn=collate_fn, batch_size=batch_size, **kwargs)\n",
    "        valid_dl = DataLoader(self.valid, shuffle=False, collate_fn=collate_fn, batch_size=batch_size, **kwargs)\n",
    "        return AdaptiveDataLoaders(train_dl, valid_dl, tokenizer=self.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db65d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TaskDatasets.set_tokenizer\" class=\"doc_header\"><code>TaskDatasets.set_tokenizer</code><a href=\"__main__.py#L51\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TaskDatasets.set_tokenizer</code>(**`tokenizer_name`**:`str`, **`override_existing`**:`bool`=*`False`*)\n",
       "\n",
       "Sets a new `AutoTokenizer` to `self.tokenizer`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`tokenizer_name`** : *`<class 'str'>`*\t<p>A string name of a `HuggingFace` tokenizer or model</p>\n",
       "\n",
       "\n",
       " - **`override_existing`** : *`<class 'bool'>`*, *optional*\t<p>Whether to override an existing tokenizer</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TaskDatasets.set_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41654afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TaskDatasets.dataloaders\" class=\"doc_header\"><code>TaskDatasets.dataloaders</code><a href=\"__main__.py#L81\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TaskDatasets.dataloaders</code>(**`batch_size`**:`int`=*`8`*, **`shuffle_train`**:`bool`=*`True`*, **`collate_fn`**:`callable`=*`None`*, **`path`**=*`'.'`*, **`device`**=*`None`*)\n",
       "\n",
       "Creates `DataLoaders` from the dataset\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`batch_size`** : *`<class 'int'>`*, *optional*\t<p>A batch size</p>\n",
       "\n",
       "\n",
       " - **`shuffle_train`** : *`<class 'bool'>`*, *optional*\t<p>Whether to shuffle the training dataset</p>\n",
       "\n",
       "\n",
       " - **`collate_fn`** : *`<built-in function callable>`*, *optional*\t<p>A custom collation function</p>\n",
       "\n",
       "\n",
       " - **`path`** : *`<class 'str'>`*, *optional*\n",
       "\n",
       " - **`device`** : *`<class 'NoneType'>`*, *optional*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TaskDatasets.dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07495e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveDataLoaders(DataLoaders):\n",
    "    \"A set of `DataLoaders` that keeps track of a `tokenizer`\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        *loaders, # A variable list of DataLoaders\n",
    "        tokenizer=None, # A Transformers tokenizer object\n",
    "        path='.', # A path to be stored in `self.path`\n",
    "        device=None # A device for the tensors, such as \"cpu\" or \"cuda:0\"\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        super().__init__(*loaders, path=path, device=device)\n",
    "        \n",
    "    def show_batch(\n",
    "        self,\n",
    "        ds_idx:int=0, # Index of the DataLoader to show, 0 = training, 1 = validation\n",
    "        n:int=5, # Number of examples to show\n",
    "        raw:bool=False, # Prints the raw inputs and targets without decoding\n",
    "    ):\n",
    "        \"Show a batch of data\"\n",
    "        dl = self[ds_idx]\n",
    "        batch = next(iter(self[ds_idx]))\n",
    "        \n",
    "        if n > len(batch['input_ids']):\n",
    "            print('`n` is larger than one batch, printing entire batch')\n",
    "            n = len(batch['input_ids'])\n",
    "        if n < 1:\n",
    "            raise ValueError('Tried to show zero samples, please enter a value for `n` greater than 0')\n",
    "        \n",
    "        if not self.tokenizer and not raw:\n",
    "            print(\"Cannot decode without a tokenizer, printing raw outputs..\")\n",
    "        \n",
    "        if raw or not self.tokenizer:\n",
    "            new_cols = []\n",
    "            for nm in batch.keys():\n",
    "                new_cols.append(nm)\n",
    "                new_cols.append(f'{nm} Shape')\n",
    "            df = pd.DataFrame(columns=new_cols)\n",
    "            for i in range(n):\n",
    "                new_row = []\n",
    "                for key in batch.keys():\n",
    "                    new_row += [batch[key][i].cpu().numpy()]\n",
    "                    new_row += [batch[key][i].shape]\n",
    "                df.loc[i] = new_row\n",
    "            \n",
    "        else:\n",
    "            inputs = self.tokenizer.batch_decode(\n",
    "                batch['input_ids'],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            lbls = batch['labels']\n",
    "            \n",
    "            df = pd.DataFrame(columns=['Input', 'Label'])\n",
    "            if hasattr(self, 'categorize'):\n",
    "                lbls = [self.categorize.decode(o.cpu().numpy()) for o in lbls]\n",
    "            elif len(lbls.shape) < len(batch['input_ids'].shape):\n",
    "                # Not a language model, but we can't decode\n",
    "                lbls = lbls\n",
    "            else:\n",
    "                # It's a language model\n",
    "                lbls = self.tokenizer.batch_decode(lbls, skip_special_tokens=True)\n",
    "            for i in range(n):\n",
    "                df.loc[i] = [inputs[i], lbls[i]]\n",
    "        display_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628dbd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"AdaptiveDataLoaders.show_batch\" class=\"doc_header\"><code>AdaptiveDataLoaders.show_batch</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>AdaptiveDataLoaders.show_batch</code>(**`ds_idx`**:`int`=*`0`*, **`n`**:`int`=*`5`*, **`raw`**:`bool`=*`False`*)\n",
       "\n",
       "Show a batch of data\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`ds_idx`** : *`<class 'int'>`*, *optional*\t<p>Index of the DataLoader to show, 0 = training, 1 = validation</p>\n",
       "\n",
       "\n",
       " - **`n`** : *`<class 'int'>`*, *optional*\t<p>Number of examples to show</p>\n",
       "\n",
       "\n",
       " - **`raw`** : *`<class 'bool'>`*, *optional*\t<p>Prints the raw inputs and targets without decoding</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AdaptiveDataLoaders.show_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d84b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _AdaptiveLearner(Learner):\n",
    "    \"\"\"\n",
    "    A base fastai `Learner` that overrides `_split` and `_do_one_batch` to\n",
    "    have it work with HuggingFace datasets and models\n",
    "    \"\"\"\n",
    "    def _split(self, b):\n",
    "        \"Assign `self.xb` to model input and labels\"\n",
    "        self.xb = b\n",
    "        if 'labels' in b.keys(): self.yb = b['labels'].unsqueeze(0)\n",
    "    \n",
    "    def _do_one_batch(self):\n",
    "        \"Move a batch of data to a device, get predictions, calculate the loss, and perform backward pass\"\n",
    "        self.xb = {k:v.to(self.device) for k,v in self.xb.items()} # See if `to_device` fixes this\n",
    "        self.yb = self.yb.to(self.device)\n",
    "        out = self.model(**self.xb)\n",
    "        self.pred = out['logits'].to(self.device)\n",
    "        self('after_pred')\n",
    "        self.loss_grad = out['loss'].to(self.device)\n",
    "        self.loss = self.loss_grad.clone()\n",
    "        self('after_loss')\n",
    "        if not self.training or not len(self.yb): return\n",
    "        self('before_backward')\n",
    "        self.loss_grad.backward()\n",
    "        self._with_events(self.opt.step, 'step', CancelStepException)\n",
    "        self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d99bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mk_class('Strategy', **{'OneCycle':'fit_one_cycle', 'CosineAnnealing':'fit_flat_cos', 'SGDR':'fit_sgdr'}, doc='Class for fitting strategies with typo-proofing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665440df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"Strategy\" class=\"doc_header\"><code>class</code> <code>Strategy</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>Strategy</code>(**\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Class for fitting strategies with typo-proofing\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`args`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Strategy, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f47523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveTuner:\n",
    "    \"A base `Tuner` that interfaces with `AdaptiveLearner` with specific exposed functions\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        expose_fastai:bool=False, # Whether to expose the entire API in `self`\n",
    "        tokenizer = None, # A HuggingFace tokenizer\n",
    "        **kwargs # kwargs for `_AdaptiveLearner`\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self._tuner = _AdaptiveLearner(**kwargs)\n",
    "\n",
    "        exposed_attrs = ['dls', 'model', 'loss_func', 'metrics']\n",
    "        for attr in exposed_attrs:\n",
    "            setattr(self, attr, getattr(self._tuner, attr))\n",
    "        if expose_fastai:\n",
    "            cls = self.__class__\n",
    "            self.__class__ = cls.__class__(\"AdaptiveTuner\", (cls, _AdaptiveLearner), kwargs)\n",
    "            \n",
    "    def tune(\n",
    "        self,\n",
    "        epochs:int, # Number of iterations to train for\n",
    "        lr:float = None, # If None, finds a new learning rate and uses suggestion_method\n",
    "        strategy:Strategy = Strategy.OneCycle, # A fitting method\n",
    "        callbacks:list = [], # Extra fastai Callbacks\n",
    "        **kwargs ## kwargs for the fit function\n",
    "    ):\n",
    "        \"Fine tune `self.model` for `epochs` with an `lr` and `strategy`\"\n",
    "        func = getattr(self, strategy, getattr(self._tuner, strategy, None))\n",
    "        for attr in 'epochs,lr,cbs'.split(): \n",
    "            if attr in kwargs.keys(): kwargs.pop(attr)\n",
    "        func(epochs, lr, cbs=callbacks, **kwargs)\n",
    "        \n",
    "    @delegates(Learner.lr_find)\n",
    "    def lr_find(\n",
    "        self, \n",
    "        **kwargs # Learner.lr_find kwargs\n",
    "    ): \n",
    "        \"Runs fastai's `LR Finder`\"\n",
    "        return self._tuner.lr_find(**kwargs)\n",
    "    \n",
    "    def save(\n",
    "        self, \n",
    "        save_directory # A folder to save our model to\n",
    "    ):\n",
    "        \"Save a pretrained model to a `save_directory`\"\n",
    "        if rank_distrib(): return # Don't save if child proc\n",
    "        self.model.save_pretrained(save_directory)\n",
    "        self.tokenizer.save_pretrained(save_directory)\n",
    "        return save_directory\n",
    "    \n",
    "    def load(\n",
    "        self, \n",
    "        path:Union[Path,str], # A location to load a tokenizer and weights from\n",
    "        device = None # A valid device such as `cpu` or `cuda:0`\n",
    "    ):\n",
    "        \"Loads a pretrained model with `AutoModel.from_pretrained` from `path` and loads it to device\"\n",
    "        if device is None and hasattr(self.dls, 'device'): device = self.dls.device\n",
    "        self.model = AutoModel.from_pretrained(path)\n",
    "        self.model = self.model.to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        return self\n",
    "    \n",
    "    def predict(\n",
    "        self, \n",
    "        text:Union[List[str], str] # Some text or list of texts to inference with\n",
    "    ): \n",
    "        \"Predict some `text` with the current model. Needs to be implemented for each task separately\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def export(\n",
    "        self, \n",
    "        save_directory:Union[Path,str] # A folder to export our model to\n",
    "    ): \n",
    "        \"Exports the current model and tokenizer information to `save_directory`\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a19ca",
   "metadata": {},
   "source": [
    "Since `fastai` is a _very_ lightweight framework that is easily approachable and incorporates state-of-the-art ideas, `AdaptNLP` bridges the gap between HuggingFace and fastai, allowing you to train with their framework through the `*Tuner` classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c509d",
   "metadata": {},
   "source": [
    "The constructor of the `AdaptiveTuner` class has an optional `expose_fastai_api` parameter. When set to `True`, the `Tuner` inherits fastai's `Learner`, so every attribute of the `Learner` is available to you. This is only recommended for those very familiar with the fastai API.\n",
    "\n",
    "Otherwise, you have access to eight functions in each class:\n",
    "  - `tune`\n",
    "  - `lr_find`\n",
    "  - `predict`\n",
    "  - `save`\n",
    "  - `load`\n",
    "  - `export`\n",
    "  \n",
    "All task fine-tuners should inherit the `AdaptiveTuner`, write good defaults, and override any specific needs as dictated by the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731630d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"AdaptiveTuner.tune\" class=\"doc_header\"><code>AdaptiveTuner.tune</code><a href=\"__main__.py#L20\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>AdaptiveTuner.tune</code>(**`epochs`**:`int`, **`lr`**:`float`=*`None`*, **`strategy`**:`Strategy`=*`'fit_one_cycle'`*, **`callbacks`**:`list`=*`[]`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Fine tune `self.model` for `epochs` with an `lr` and `strategy`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`epochs`** : *`<class 'int'>`*\t<p>Number of iterations to train for</p>\n",
       "\n",
       "\n",
       " - **`lr`** : *`<class 'float'>`*, *optional*\t<p>If None, finds a new learning rate and uses suggestion_method</p>\n",
       "\n",
       "\n",
       " - **`strategy`** : *`<class 'fastcore.basics.Strategy'>`*, *optional*\t<p>A fitting method</p>\n",
       "\n",
       "\n",
       " - **`callbacks`** : *`<class 'list'>`*, *optional*\t<p>Extra fastai Callbacks</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AdaptiveTuner.tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ecd6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"AdaptiveTuner.lr_find\" class=\"doc_header\"><code>AdaptiveTuner.lr_find</code><a href=\"__main__.py#L34\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>AdaptiveTuner.lr_find</code>(**`start_lr`**=*`1e-07`*, **`end_lr`**=*`10`*, **`num_it`**=*`100`*, **`stop_div`**=*`True`*, **`show_plot`**=*`True`*, **`suggest_funcs`**=*`valley`*)\n",
       "\n",
       "Runs fastai's `LR Finder`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`start_lr`** : *`<class 'float'>`*, *optional*\n",
       "\n",
       " - **`end_lr`** : *`<class 'int'>`*, *optional*\n",
       "\n",
       " - **`num_it`** : *`<class 'int'>`*, *optional*\n",
       "\n",
       " - **`stop_div`** : *`<class 'bool'>`*, *optional*\n",
       "\n",
       " - **`show_plot`** : *`<class 'bool'>`*, *optional*\n",
       "\n",
       " - **`suggest_funcs`** : *`<class 'function'>`*, *optional*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AdaptiveTuner.lr_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df527f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"AdaptiveTuner.save\" class=\"doc_header\"><code>AdaptiveTuner.save</code><a href=\"__main__.py#L42\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>AdaptiveTuner.save</code>(**`save_directory`**)\n",
       "\n",
       "Save a pretrained model to a `save_directory`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`save_directory`** : *`<class 'inspect._empty'>`*\t<p>A folder to save our model to</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AdaptiveTuner.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb52e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"AdaptiveTuner.load\" class=\"doc_header\"><code>AdaptiveTuner.load</code><a href=\"__main__.py#L52\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>AdaptiveTuner.load</code>(**`path`**:`Union`\\[`Path`, `str`\\], **`device`**=*`None`*)\n",
       "\n",
       "Loads a pretrained model with `AutoModel.from_pretrained` from `path` and loads it to device\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`path`** : *`typing.Union[pathlib.Path, str]`*\t<p>A location to load a tokenizer and weights from</p>\n",
       "\n",
       "\n",
       " - **`device`** : *`<class 'NoneType'>`*, *optional*\t<p>A valid device such as `cpu` or `cuda:0`</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AdaptiveTuner.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790171e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"AdaptiveTuner.predict\" class=\"doc_header\"><code>AdaptiveTuner.predict</code><a href=\"__main__.py#L64\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>AdaptiveTuner.predict</code>(**`text`**:`Union`\\[`List`\\[`str`\\], `str`\\])\n",
       "\n",
       "Predict some `text` with the current model. Needs to be implemented for each task separately\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`text`** : *`typing.Union[typing.List[str], str]`*\t<p>Some text or list of texts to inference with</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AdaptiveTuner.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5865a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"AdaptiveTuner.export\" class=\"doc_header\"><code>AdaptiveTuner.export</code><a href=\"__main__.py#L71\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>AdaptiveTuner.export</code>(**`save_directory`**:`Union`\\[`Path`, `str`\\])\n",
       "\n",
       "Exports the current model and tokenizer information to `save_directory`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`save_directory`** : *`typing.Union[pathlib.Path, str]`*\t<p>A folder to export our model to</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AdaptiveTuner.export)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
