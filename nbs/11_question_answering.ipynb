{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp question_answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Title (change me)\n",
    "> Default description (change me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from typing import Tuple, List, Union, Dict\n",
    "from collections import OrderedDict, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    XLNetForQuestionAnswering,\n",
    "    XLMForQuestionAnswering,\n",
    "    CamembertForQuestionAnswering,\n",
    "    DistilBertForQuestionAnswering,\n",
    "    RobertaForQuestionAnswering,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    SquadExample,\n",
    "    squad_convert_examples_to_features,\n",
    ")\n",
    "from transformers.data.processors.squad import SquadResult\n",
    "\n",
    "from adaptnlp.model import AdaptiveModel, DataLoader\n",
    "from adaptnlp.transformers.squad_metrics import (\n",
    "    compute_predictions_log_probs,\n",
    "    compute_predictions_logits,\n",
    ")\n",
    "\n",
    "from fastcore.basics import risinstance, nested_attr, Self, patch\n",
    "\n",
    "from fastai_minima.callback.core import Callback\n",
    "from fastai_minima.utils import apply, to_detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class QACallback(Callback):\n",
    "    \"Basic Question Answering Data Callback\"\n",
    "    order = -2\n",
    "    _qa_models = [XLMForQuestionAnswering, RobertaForQuestionAnswering, DistilBertForQuestionAnswering]\n",
    "\n",
    "    def __init__(self, xmodel_instances, features):\n",
    "        self.xmodel_instances = xmodel_instances\n",
    "        self.features = features\n",
    "\n",
    "    def before_batch(self):\n",
    "        \"Adjusts `token_type_ids` if model is in `_qa_models`\"\n",
    "        if risinstance(self._qa_models, self.learn.model): del self.learn.inputs[\"token_type_ids\"]\n",
    "        if len(self.xb) > 3: self.example_indices = self.xb[3]\n",
    "\n",
    "        if isinstance(self.learn.model, self.xmodel_instances):\n",
    "            self.learn.inputs.update({'cls_index': self.xb[4], 'p_mask': self.xb[5]})\n",
    "            # for lang_id-sensitive xlm models\n",
    "            if nested_attr(self.learn.model, 'config.lang2id', False):\n",
    "                # Set language id as 0 for now\n",
    "                self.learn.inputs.update(\n",
    "                    {\n",
    "                        'langs': (\n",
    "                        torch.ones(self.xb[0].shape, dtype=torch.int64) * 0\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "    def after_pred(self):\n",
    "        \"Generate SquadResults\"\n",
    "        for i, example_index in enumerate(self.example_indices):\n",
    "            eval_feature = self.features[example_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "            output = [self.pred[output][i] for output in self.pred]\n",
    "            output = apply(Self.numpy(), to_detach(output))\n",
    "\n",
    "            if isinstance(self.learn.model, self.xmodel_instances):\n",
    "                # Some models like the ones in `self.xmodel_instances` use 5 arguments for their predictions\n",
    "                start_logits = output[0]\n",
    "                start_top_index = output[1]\n",
    "                end_logits = output[2]\n",
    "                end_top_index = output[3]\n",
    "                cls_logits = output[4]\n",
    "\n",
    "                self.learn.pred = SquadResult(\n",
    "                    unique_id,\n",
    "                    start_logits,\n",
    "                    end_logits,\n",
    "                    start_top_index=start_top_index,\n",
    "                    end_top_index=end_top_index,\n",
    "                    cls_logits=cls_logits\n",
    "                )\n",
    "            else:\n",
    "                start_logits, end_logits = output\n",
    "                self.learn.pred = SquadResult(unique_id, start_logits, end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersQuestionAnswering(AdaptiveModel):\n",
    "    \"\"\"Adaptive Model for Transformers Question Answering Model\n",
    "\n",
    "    **Parameters**\n",
    "\n",
    "    * **tokenizer** - A tokenizer object from Huggingface's transformers (TODO)and tokenizers *\n",
    "    * **model** - A transformer Question Answering model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n",
    "        # Load up model and tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        super().__init__()\n",
    "\n",
    "        # Sets internal model\n",
    "        self.set_model(model)\n",
    "        self.xmodel_instances = (XLNetForQuestionAnswering, XLMForQuestionAnswering)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading and constructing this model\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Transformer's pre-trained Question Answering (SQUAD) models\n",
    "        \"\"\"\n",
    "        # QA tokenizers not compatible with fast tokenizers yet\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path)\n",
    "        qa_model = cls(tokenizer, model)\n",
    "        return qa_model\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        query: Union[List[str], str],\n",
    "        context: Union[List[str], str],\n",
    "        n_best_size: int = 5,\n",
    "        mini_batch_size: int = 32,\n",
    "        max_answer_length: int = 10,\n",
    "        do_lower_case: bool = False,\n",
    "        version_2_with_negative: bool = False,\n",
    "        verbose_logging: bool = False,\n",
    "        null_score_diff_threshold: float = 0.0,\n",
    "        max_seq_length: int = 512,\n",
    "        doc_stride: int = 128,\n",
    "        max_query_length: int = 64,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Tuple[str, List[OrderedDict]], Tuple[OrderedDict, OrderedDict]]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained question answering model\n",
    "\n",
    "        * **query** - String or list of strings that specify the ordered questions corresponding to `context`\n",
    "        * **context** - String or list of strings that specify the ordered contexts corresponding to `query`\n",
    "        * **n_best_size** - Number of top n results you want\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **max_answer_length** - Maximum token length for answers that are returned\n",
    "        * **do_lower_case** - Set as `True` if using uncased QA models\n",
    "        * **version_2_with_negative** - Set as True if using QA model with SQUAD2.0\n",
    "        * **verbose_logging** - Set True if you want prediction verbose loggings\n",
    "        * **null_score_diff_threshold** - Threshold for predicting null(no answer) in Squad 2.0 Model.  Default is 0.0.  Raise this if you want fewer null answers\n",
    "        * **max_seq_length** - Maximum context token length. Check model configs to see max sequence length the model was trained with\n",
    "        * **doc_stride** - Number of token strides to take when splitting up conext into chunks of size `max_seq_length`\n",
    "        * **max_query_length** - Maximum token length for queries\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers model (mostly for saving evaluations)\n",
    "        \"\"\"\n",
    "        # Make string input consistent as list\n",
    "        if isinstance(query, str):\n",
    "            query = [query]\n",
    "            context = [context]\n",
    "        assert len(query) == len(context)\n",
    "\n",
    "        examples = self._mini_squad_processor(query=query, context=context)\n",
    "        features, dataset = squad_convert_examples_to_features(\n",
    "            examples,\n",
    "            self.tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset='pt',\n",
    "            threads=1,\n",
    "        )\n",
    "        all_results = []\n",
    "\n",
    "        dl = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "\n",
    "        cb = QACallback(self.xmodel_instances, features)\n",
    "\n",
    "        all_results, _ = super().get_preds(dl=dl, cbs=[cb])\n",
    "\n",
    "        if isinstance(self.model, self.xmodel_instances):\n",
    "            start_n_top = (\n",
    "                self.model.config.start_n_top\n",
    "                if hasattr(self.model, 'config')\n",
    "                else self.model.module.config.start_n_top\n",
    "            )\n",
    "            end_n_top = (\n",
    "                self.model.config.end_n_top\n",
    "                if hasattr(self.model, 'config')\n",
    "                else self.model.module.config.end_n_top\n",
    "            )\n",
    "\n",
    "            answers, n_best = compute_predictions_log_probs(\n",
    "                examples,\n",
    "                features,\n",
    "                all_results,\n",
    "                n_best_size,\n",
    "                max_answer_length,\n",
    "                start_n_top,\n",
    "                end_n_top,\n",
    "                version_2_with_negative,\n",
    "                self.tokenizer,\n",
    "                verbose_logging,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            answers, n_best = compute_predictions_logits(\n",
    "                examples,\n",
    "                features,\n",
    "                all_results,\n",
    "                n_best_size,\n",
    "                max_answer_length,\n",
    "                do_lower_case,\n",
    "                verbose_logging,\n",
    "                version_2_with_negative,\n",
    "                null_score_diff_threshold,\n",
    "                self.tokenizer,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        return answers, n_best\n",
    "\n",
    "    def _mini_squad_processor(\n",
    "        self, query: List[str], context: List[str]\n",
    "    ) -> List[SquadExample]:\n",
    "        \"\"\"Squad data processor to create `SquadExamples`\n",
    "\n",
    "        * **query** - List of query strings, must be same length as `context`\n",
    "        * **context** - List of context strings, must be same length as `query`\n",
    "\n",
    "        \"\"\"\n",
    "        assert len(query) == len(context)\n",
    "        examples = []\n",
    "        title = 'qa'\n",
    "        is_impossible = False\n",
    "        answer_text = None\n",
    "        start_position_character = None\n",
    "        answers = ['answer']\n",
    "        for idx, (q, c) in enumerate(zip(query, context)):\n",
    "            example = SquadExample(\n",
    "                qas_id=str(idx),\n",
    "                question_text=q,\n",
    "                context_text=c,\n",
    "                answer_text=answer_text,\n",
    "                start_position_character=start_position_character,\n",
    "                title=title,\n",
    "                is_impossible=is_impossible,\n",
    "                answers=answers,\n",
    "            )\n",
    "            examples.append(example)\n",
    "        return examples\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyQuestionAnswering:\n",
    "    \"\"\"Question Answering Module\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> qa = adaptnlp.EasyQuestionAnswering()\n",
    "    >>> qa.predict_qa(query='What is life?', context='Life is NLP.', n_best_size=5, mini_batch_size=1)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "\n",
    "    def predict_qa(\n",
    "        self,\n",
    "        query: Union[List[str], str],\n",
    "        context: Union[List[str], str],\n",
    "        n_best_size: int = 5,\n",
    "        mini_batch_size: int = 32,\n",
    "        model_name_or_path: str = 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Tuple[str, List[OrderedDict]], Tuple[OrderedDict, OrderedDict]]:\n",
    "        \"\"\"Predicts top_n answer spans of query in regards to context\n",
    "\n",
    "        * **query** - String or list of strings that specify the ordered questions corresponding to `context`\n",
    "        * **context** - String or list of strings that specify the ordered contexts corresponding to `query`\n",
    "        * **n_best_size** - The top n answers returned\n",
    "        * **mini_batch_size** - Mini batch size for inference\n",
    "        * **model_name_or_path** - Path to QA model or name of QA model at huggingface.co/models\n",
    "        * **kwargs**(Optional) - Keyword arguments for `AdaptiveModel`s like `TransformersQuestionAnswering`\n",
    "\n",
    "        **return** - Either a list of string answers or a dict of the results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.models[model_name_or_path]:\n",
    "                self.models[model_name_or_path] = TransformersQuestionAnswering.load(\n",
    "                    model_name_or_path\n",
    "                )\n",
    "        except OSError:\n",
    "            logger.info(\n",
    "                f'{model_name_or_path} not a valid Transformers pre-trained QA model...check path or huggingface.co/models'\n",
    "            )\n",
    "            raise ValueError(\n",
    "                f'{model_name_or_path} is not a valid path or model name from huggingface.co/models'\n",
    "            )\n",
    "            return OrderedDict(), [OrderedDict()]\n",
    "\n",
    "        model = self.models[model_name_or_path]\n",
    "\n",
    "        # If query and context is just one instance, get rid of nested OrderedDict\n",
    "        if isinstance(query, str):\n",
    "            top_answer, top_n_answers = model.predict(\n",
    "                query=query,\n",
    "                context=context,\n",
    "                n_best_size=n_best_size,\n",
    "                mini_batch_size=mini_batch_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "            return top_answer['0'], top_n_answers['0']\n",
    "\n",
    "        return model.predict(\n",
    "            query=query,\n",
    "            context=context,\n",
    "            n_best_size=n_best_size,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            **kwargs,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
