{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd77c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training.tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d86dd",
   "metadata": {},
   "source": [
    "# Tuning API\n",
    "> Modules for task fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ed702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f0b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.learner import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.metrics import *\n",
    "from fastai.losses import *\n",
    "from fastai.optimizer import *\n",
    "# NOTE: Placeholder imports, remove once we are ready for release\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "from adaptnlp.training.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6333c4c",
   "metadata": {},
   "source": [
    "## Integration with [fastai](https://docs.fast.ai)\n",
    "\n",
    "Since `fastai` is a _very_ lightweight framework that is easily approachable and incorporates state-of-the-art ideas, `AdaptNLP` bridges the gap between HuggingFace and fastai, allowing you to train with their framework through the `*Tuner` classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff94f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _AdaptiveLearner(Learner):\n",
    "    \"\"\"\n",
    "    A base fastai `Learner` that overrides `_split` and `_do_one_batch` to\n",
    "    have it work with HuggingFace datasets and models\n",
    "    \"\"\"\n",
    "    def _split(self, b):\n",
    "        \"Assign `self.xb` to model input and labels\"\n",
    "        self.xb = b\n",
    "        if 'labels' in b.keys(): self.yb = b['labels'].unsqueeze(0)\n",
    "    \n",
    "    def _do_one_batch(self):\n",
    "        \"Move a batch of data to a device, get predictions, calculate the loss, and perform backward pass\"\n",
    "        self.xb = {k:v.to(self.device) for k,v in self.xb.items()} # See if `to_device` fixes this\n",
    "        self.yb = self.yb.to(self.device)\n",
    "        out = self.model(**self.xb)\n",
    "        self.pred = out['logits'].to(self.device)\n",
    "        self('after_pred')\n",
    "        self.loss_grad = out['loss'].to(self.device)\n",
    "        self.loss = self.loss_grad.clone()\n",
    "        self('after_loss')\n",
    "        if not self.training or not len(self.yb): return\n",
    "        self('before_backward')\n",
    "        self.loss_grad.backward()\n",
    "        self._with_events(self.opt.step, 'step', CancelStepException)\n",
    "        self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_a = {'OneCycle':'fit_one_cycle', 'CosineAnnealing':'fit_flat_cos', 'SGDR':'fit_sgdr'}\n",
    "mk_class('Strategy', **_a, doc_string='Class for fitting strategies with typo-proofing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveTuner:\n",
    "    \"\"\"\n",
    "    A base `Tuner` that interfaces with `AdaptiveLearner` with specific exposed functions\n",
    "    \"\"\"\n",
    "    @delegates(_AdaptiveLearner.__init__)\n",
    "    def __init__(self, expose_fastai:bool=False, **kwargs):\n",
    "        self._tuner = _AdaptiveLearner(**kwargs)\n",
    "\n",
    "        exposed_attrs = ['dls', 'model', 'loss_func', 'metrics']\n",
    "        for attr in exposed_attrs:\n",
    "            setattr(self, attr, getattr(self._tuner, attr))\n",
    "        if expose_fastai:\n",
    "            cls = self.__class__\n",
    "            self.__class__ = cls.__class__(\"AdaptiveTuner\", (cls, _AdaptiveLearner), kwargs)\n",
    "            \n",
    "    def tune(\n",
    "        self,\n",
    "        epochs:int, # Number of epochs to train for\n",
    "        lr:float = None, # If None, finds a new LR and uses suggestion_method\n",
    "        strategy:Strategy = Strategy.OneCycle,\n",
    "        callbacks = [], # Extra fastai Callbacks\n",
    "        **kwargs ## kwargs for the fit function\n",
    "        \n",
    "    ):\n",
    "        \"Fine tune `self.model` for `epochs` with an `lr` and `strategy`\"\n",
    "        func = getattr(self, strategy, getattr(self._tuner, strategy, None))\n",
    "        for attr in 'epochs,lr,cbs'.split(): \n",
    "            if attr in kwargs.keys(): kwargs.pop(attr)\n",
    "        func(epochs, lr, cbs=callbacks, **kwargs)\n",
    "        \n",
    "    @delegates(Learner.lr_find)\n",
    "    def lr_find(self, **kwargs): return self._tuner.lr_find(**kwargs)\n",
    "    \n",
    "    def save(self, file:Union[Path,str], with_opt=True, pickle_protocol=2): \n",
    "        file = join_path_file(kwargs['file'], self.path/self.model_dir, ext='.pth')\n",
    "        if rank_distrib(): return # Don't save if child proc\n",
    "        opt = getattr(self, 'opt', None)\n",
    "        if opt is None: with_opt = False\n",
    "        state = get_model(self.model).state_dict()\n",
    "        state = {'model':state}\n",
    "        if with_opt: state['opt'] = opt.state_dict()\n",
    "        state['model_name_or_path'] = self.model.name_or_path\n",
    "        torch.save(state, file, pickle_protocol=pickle_protocol)\n",
    "        return file\n",
    "    \n",
    "    def load(self, file:Union[Path,str], device=None, with_opt=True, strict=True):\n",
    "        if device is None and hasattr(self.dls, 'device'): device = self.dls.device\n",
    "        if self.opt is None: self.create_opt()\n",
    "        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "        distrib_barrier()\n",
    "        if isinstance(device, int): device = torch.device('cuda', device)\n",
    "        elif device is None: device='cpu'\n",
    "        state = torch.load(file, map_location=device)\n",
    "        hasopt = 'opt' in state.keys()\n",
    "        model_state = state['model']\n",
    "        get_model(self.model).load_state_dict(model_state, strict=strict)\n",
    "        if hasopt and with_opt:\n",
    "            try: self.opt.load_state_dict(state['opt'])\n",
    "            except:\n",
    "                if with_opt: warn(\"Could not load the optimizer state.\")\n",
    "        elif with_opt: warn(\"Saved file doesn't contain an optimizer state\")\n",
    "        return self\n",
    "    \n",
    "for attr in ['lr_find', 'save', 'load']: \n",
    "    setattr(getattr(AdaptiveTuner, attr), '__doc__', getattr(_AdaptiveLearner, attr).__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f09bef",
   "metadata": {},
   "source": [
    "The constructor of the `AdaptiveTuner` class has an optional `expose_fastai_api` parameter. When set to `True`, the `Tuner` inherits fastai's `Learner`, so every attribute of the `Learner` is available to you. This is only recommended for those very familiar with the fastai API.\n",
    "\n",
    "Otherwise, you have access to six* functions in each class:\n",
    "  - `tune`\n",
    "  - `lr_find`\n",
    "  - `predict` (Coming soon)\n",
    "  - `save`\n",
    "  - `load`\n",
    "  - `export` (Coming soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6468577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15560c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceClassificationTuner(AdaptiveTuner):\n",
    "    \"\"\"\n",
    "    An `AdaptiveTuner` with good defaults for Sequence Classification tasks\n",
    "    \n",
    "    **Valid kwargs and defaults:**\n",
    "      - `lr`:float = 0.001\n",
    "      - `splitter`:function = `trainable_params`\n",
    "      - `cbs`:list = None\n",
    "      - `path`:Path = None\n",
    "      - `model_dir`:Path = 'models'\n",
    "      - `wd`:float = None\n",
    "      - `wd_bn_bias`:bool = False\n",
    "      - `train_bn`:bool = True\n",
    "      - `moms`: tuple(float) = (0.95, 0.85, 0.95)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dls:DataLoaders, # A set of DataLoaders\n",
    "        model_name, # A HuggingFace model\n",
    "        loss_func = CrossEntropyLossFlat(), # A loss function\n",
    "        metrics = [accuracy, F1Score()], # Metrics to monitor the training with\n",
    "        opt_func = Adam, # A fastai or torch Optimizer\n",
    "        additional_cbs = None, # Additional Callbacks to have always tied to the Tuner,\n",
    "        expose_fastai_api = False, # Whether to expose the fastai API\n",
    "        **kwargs, # kwargs for `Learner.__init__`\n",
    "    ):\n",
    "        additional_cbs = listify(additional_cbs)\n",
    "        for arg in 'dls,model,loss_func,metrics,opt_func,cbs,expose_fastai'.split(','): \n",
    "            if arg in kwargs.keys(): kwargs.pop(arg) # Pop all existing kwargs\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(dls[0].categorize.classes))\n",
    "        \n",
    "        super().__init__(\n",
    "            expose_fastai_api,\n",
    "            dls = dls, \n",
    "            model = model, \n",
    "            loss_func = loss_func, \n",
    "            metrics = metrics, \n",
    "            opt_func = opt_func, \n",
    "            cbs=additional_cbs, \n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    @delegates(Learner.__init__)\n",
    "    @classmethod\n",
    "    def from_df(\n",
    "        cls,\n",
    "        df:pd.DataFrame, # A Pandas Dataframe or Path to a DataFrame\n",
    "        text_col:str = 'text', # Name of the column the text is stored\n",
    "        label_col:str = 'labels', # Name of the column the label(s) are stored\n",
    "        model_name:str = None, # The string name of a huggingFace model\n",
    "        split_func:callable = RandomSplitter(), # A function which splits the data\n",
    "        loss_func = CrossEntropyLossFlat(), # A loss function\n",
    "        metrics = [accuracy, F1Score()], # Metrics to monitor the training with\n",
    "        batch_size=8, # A batch size\n",
    "        collate_fn=default_data_collator, # An optional custom collate function\n",
    "        opt_func = Adam, # A fastai or torch Optimizer\n",
    "        additional_cbs = None, # Additional Callbacks to have always tied to the Tuner,\n",
    "        expose_fastai_api = False, # Whether to expose the fastai API\n",
    "        **kwargs # Learner kwargs\n",
    "    ):\n",
    "        \"Convience method to build a `SequenceClassificationTuner` from a Pandas Dataframe\"\n",
    "        try:\n",
    "            splits = split_func(df)\n",
    "        except:\n",
    "            splits = split_func(range_of(df))\n",
    "        dls = SequenceClassificationDatasets.from_df(\n",
    "            df,\n",
    "            text_col,\n",
    "            label_col,\n",
    "            splits,\n",
    "            tokenizer_name=model_name\n",
    "        ).dataloaders(batch_size, collate_fn)\n",
    "        \n",
    "        return cls(dls, model_name, loss_func, metrics, opt_func, additional_cbs, expose_fastai_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from transformers import AutoModelForMaskedLM, AutoModelForCausalLM, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8edea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mk_class('LMType', **{o:o.lower() for o in ['Masked', 'Causal', 'Seq2Seq']},\n",
    "        doc=\"All valid language model classes with typo-proofing\")\n",
    "\n",
    "_constructors = {\n",
    "    'masked':AutoModelForMaskedLM.from_pretrained,\n",
    "    'causal':AutoModelForCausalLM.from_pretrained,\n",
    "    'seq2seq':AutoModelForSeq2SeqLM.from_pretrained\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9fe52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LanguageModelTuner(AdaptiveTuner):\n",
    "    \"\"\"\n",
    "    An `AdaptiveTuner` with good defaults for Language Model fine-tuning\n",
    "    \n",
    "    **Valid kwargs and defaults:**\n",
    "      - `lr`:float = 0.001\n",
    "      - `splitter`:function = `trainable_params`\n",
    "      - `cbs`:list = None\n",
    "      - `path`:Path = None\n",
    "      - `model_dir`:Path = 'models'\n",
    "      - `wd`:float = None\n",
    "      - `wd_bn_bias`:bool = False\n",
    "      - `train_bn`:bool = True\n",
    "      - `moms`: tuple(float) = (0.95, 0.85, 0.95)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dls:DataLoaders, # A set of DataLoaders\n",
    "        model_name, # A HuggingFace model\n",
    "        language_model_type:LMType = LMType.Causal, # The type of language model to use\n",
    "        loss_func = CrossEntropyLossFlat(), # A loss function\n",
    "        metrics = [accuracy, Perplexity()], # Metrics to monitor the training with\n",
    "        opt_func = Adam, # A fastai or torch Optimizer\n",
    "        additional_cbs = None, # Additional Callbacks to have always tied to the Tuner,\n",
    "        expose_fastai_api = False, # Whether to expose the fastai API\n",
    "        **kwargs, # kwargs for `Learner.__init__`\n",
    "    ):\n",
    "        additional_cbs = listify(additional_cbs)\n",
    "        for arg in 'dls,model,loss_func,metrics,opt_func,cbs,expose_fastai'.split(','): \n",
    "            if arg in kwargs.keys(): kwargs.pop(arg) # Pop all existing kwargs\n",
    "                \n",
    "        if language_model_type is None: raise ValueError(\"Please specify the type of language model you want to use, such as `masked` or `causal`\")\n",
    "        if language_model_type not in _constructors.keys():\n",
    "            raise ValueError(\n",
    "                \"\"\"\n",
    "                Please enter a valid Langauge Model Type of:\n",
    "                  * `masked` or `LMType.Masked`\n",
    "                  * `causal` or `LMType.Causal`\n",
    "                  * `seq2seq` or `LMType.Seq2Seq`\n",
    "                \"\"\"\n",
    "            )\n",
    "        try:\n",
    "            model = _constructors[language_model_type](model_name)\n",
    "        except Exception as e:\n",
    "            message = e.args[0]\n",
    "            m = f\"Was not able to create a {language_model_type} instance of {model_name}. Please use a valid model for your task:\"\n",
    "            m += message\n",
    "            e.args = [m]\n",
    "            raise e\n",
    "        \n",
    "        super().__init__(\n",
    "            expose_fastai_api,\n",
    "            dls = dls, \n",
    "            model = model, \n",
    "            loss_func = loss_func, \n",
    "            metrics = metrics, \n",
    "            opt_func = opt_func, \n",
    "            cbs=additional_cbs, \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "    @delegates(Learner.__init__)\n",
    "    @classmethod\n",
    "    def from_df(\n",
    "        cls,\n",
    "        df:pd.DataFrame, # A Pandas Dataframe or Path to a DataFrame\n",
    "        text_col:str = 'text', # Name of the column the text is stored\n",
    "        model_name:str = None, # The string name of a huggingFace model\n",
    "        language_model_type:LMType = LMType.Causal, # The type of language model to use\n",
    "        split_func:callable = RandomSplitter(), # A function which splits the data\n",
    "        loss_func = CrossEntropyLossFlat(), # A loss function\n",
    "        metrics = [accuracy, Perplexity()], # Metrics to monitor the training with\n",
    "        batch_size=8, # A batch size\n",
    "        collate_fn=default_collate, # An optional custom collate function\n",
    "        opt_func = Adam, # A fastai or torch Optimizer\n",
    "        additional_cbs = None, # Additional Callbacks to have always tied to the Tuner,\n",
    "        expose_fastai_api = False, # Whether to expose the fastai API\n",
    "        **kwargs # Learner kwargs\n",
    "    ):\n",
    "        \"Convience method to build a `SequenceClassificationTuner` from a Pandas Dataframe\"\n",
    "        splits = split_func(range_of(df))\n",
    "        dls = LanguageModelDatasets.from_df(\n",
    "            df,\n",
    "            text_col,\n",
    "            splits,\n",
    "            tokenizer_name=model_name\n",
    "        ).dataloaders(batch_size, collate_fn)\n",
    "        \n",
    "        return cls(dls, model_name, language_model_type, loss_func, metrics, opt_func, additional_cbs, expose_fastai_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63893e3d",
   "metadata": {},
   "source": [
    "## Export - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cac163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_file_utils.ipynb.\n",
      "Converted 01_callback.ipynb.\n",
      "Converted 02_model_hub.ipynb.\n",
      "Converted 03_model.ipynb.\n",
      "Converted 04_embeddings.ipynb.\n",
      "Converted 04a_tutorial.embeddings.ipynb.\n",
      "Converted 05_token_classification.ipynb.\n",
      "Converted 05a_tutorial.token_tagging.ipynb.\n",
      "Converted 06_sequence_classification.ipynb.\n",
      "Converted 06a_tutorial.easy_sequence_classifier.ipynb.\n",
      "Converted 07_summarization.ipynb.\n",
      "Converted 07a_tutorial.summarization.ipynb.\n",
      "Converted 08_translation.ipynb.\n",
      "Converted 08a_tutorial.translation.ipynb.\n",
      "Converted 09_text_generation.ipynb.\n",
      "Converted 09a_tutorial.easy_text_generator.ipynb.\n",
      "Converted 10_question_answering.ipynb.\n",
      "Converted 10a_tutorial.question_answering.ipynb.\n",
      "Converted 13a_transformers.squad_metrics.ipynb.\n",
      "Converted 14_result.ipynb.\n",
      "Converted 15_training.data.ipynb.\n",
      "Converted 16_training.tuner.ipynb.\n",
      "Converted 20_tutorial.tuner.sequence_classification.ipynb.\n",
      "Converted 21_tutorial.tuner.language_model.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b818df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
