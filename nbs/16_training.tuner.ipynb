{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd77c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp training.tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d86dd",
   "metadata": {},
   "source": [
    "# Tuning API\n",
    "> Modules for task fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ed702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f0b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.learner import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.metrics import *\n",
    "from fastai.losses import *\n",
    "from fastai.optimizer import *\n",
    "# NOTE: Placeholder imports, remove once we are ready for release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6333c4c",
   "metadata": {},
   "source": [
    "## Integration with [fastai](https://docs.fast.ai)\n",
    "\n",
    "Since `fastai` is a _very_ lightweight framework that is easily approachable and incorporates state-of-the-art ideas, `AdaptNLP` bridges the gap between HuggingFace and fastai, allowing you to train with their framework through the `AdaptiveLearner` and `*Tuner` classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff94f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveLearner(Learner):\n",
    "    \"\"\"\n",
    "    A base fastai `Learner` that overrides `_split` and `_do_one_batch` to\n",
    "    have it work with HuggingFace datasets and models\n",
    "    \"\"\"\n",
    "    def _split(self, b):\n",
    "        \"Assign `self.xb` to model input and labels\"\n",
    "        self.xb = b\n",
    "        if 'labels' in b.keys(): self.yb = b['labels'].unsqueeze(0)\n",
    "    \n",
    "    def _do_one_batch(self):\n",
    "        \"Move a batch of data to a device, get predictions, calculate the loss, and perform backward pass\"\n",
    "        self.xb = {k:v.to(self.device) for k,v in self.xb.items()} # See if `to_device` fixes this\n",
    "        self.yb = self.yb.to(self.device)\n",
    "        out = self.model(**self.xb)\n",
    "        self.pred = out['logits'].to(self.device)\n",
    "        self('after_pred')\n",
    "        self.loss_grad = out['loss'].to(self.device)\n",
    "        self.loss = self.loss_grad.clone()\n",
    "        self('after_loss')\n",
    "        if not self.training or not len(self.yb): return\n",
    "        self('before_backward')\n",
    "        self.loss_grad.backward()\n",
    "        self._with_events(self.opt.step, 'step', CancelStepException)\n",
    "        self.opt.zero_grad()\n",
    "    \n",
    "    def predict(self, inp): raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4be3355",
   "metadata": {},
   "source": [
    "The `AdaptiveLearner` class is the base class you should use when fine-tuning on `HuggingFace` models and datasets. It assumes that the dataset will *always* be a dictionary, with labels always being a `labels` key. Futher task-specific `Learner`'s are detailed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6468577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15560c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceClassificationTuner(AdaptiveLearner):\n",
    "    \"\"\"\n",
    "    A `AdaptiveLearner` with good defaults for Sequence Classification tasks\n",
    "    \n",
    "    **Valid kwargs and defaults:**\n",
    "      - `lr`:float = 0.001\n",
    "      - `splitter`:function = `trainable_params`\n",
    "      - `cbs`:list = None\n",
    "      - `path`:Path = None\n",
    "      - `model_dir`:Path = 'models'\n",
    "      - `wd`:float = None\n",
    "      - `wd_bn_bias`:bool = False\n",
    "      - `train_bn`:bool = True\n",
    "      - `moms`: tuple(float) = (0.95, 0.85, 0.95)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dls:DataLoaders, # A set of DataLoaders\n",
    "        model, # A HuggingFace model\n",
    "        loss_func = CrossEntropyLossFlat(), # A loss function\n",
    "        metrics = [accuracy, F1Score()], # Metrics to monitor the training with\n",
    "        opt_func = Adam, # A fastai or torch Optimizer\n",
    "        additional_cbs = None, # Additional Callbacks to have always tied to the Tuner\n",
    "        **kwargs, # kwargs for `Learner.__init__`\n",
    "    ):\n",
    "        additional_cbs = listify(additional_cbs)\n",
    "        for arg in 'dls,model,loss_func,metrics,opt_func,cbs'.split(','): \n",
    "            if arg in kwargs.keys(): kwargs.pop(arg) # Pop all existing kwargs\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model, num_labels=len(dls[0].categorize.classes))\n",
    "        super().__init__(\n",
    "            dls, \n",
    "            model, \n",
    "            loss_func = loss_func, \n",
    "            metrics = metrics, \n",
    "            opt_func = opt_func, \n",
    "            cbs=additional_cbs, \n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9aca64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
