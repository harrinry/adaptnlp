{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "> Callbacks for predicting within AdaptNLP using the fastai framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbverbose.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.basics import store_attr\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "from fastai.callback.core import Callback, CancelBatchException\n",
    "\n",
    "from transformers import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GatherInputsCallback(Callback):\n",
    "    \"\"\"\n",
    "    Prepares basic input dictionary for HuggingFace Transformers\n",
    "\n",
    "    This `Callback` generates a very basic dictionary consisting of `input_ids`,\n",
    "    `attention_masks`, and `token_type_ids`, and saves it to the attribute `self.learn.inputs`.\n",
    "\n",
    "    If further data is expected or needed from the batch, the additional Callback(s) should have\n",
    "    an order of -2\n",
    "    \"\"\"\n",
    "    order = -3\n",
    "\n",
    "    def before_validate(self):\n",
    "        \"\"\"\n",
    "        Sets the number of inputs in `self.dls`\n",
    "        \"\"\"\n",
    "        x = self.dl.one_batch()\n",
    "        self.learn.dls.n_inp = len(x)\n",
    "\n",
    "    def before_batch(self):\n",
    "        \"\"\"\n",
    "        Turns `self.xb` from a tuple to a dictionary of either\n",
    "            `{\"input_ids\", \"attention_masks\", \"token_type_ids\"}`d\n",
    "        or\n",
    "            `{\"input_ids\", \"attention_masks\"}`\n",
    "        \"\"\"\n",
    "        inputs = {\n",
    "                \"input_ids\":self.learn.xb[0],\n",
    "                \"attention_mask\":self.learn.xb[1]\n",
    "        }\n",
    "\n",
    "        if len(self.learn.xb) > 2:\n",
    "            inputs[\"token_type_ids\"] = self.learn.xb[2]\n",
    "\n",
    "        self.learn.inputs = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GatherInputsCallback.before_validate\" class=\"doc_header\"><code>GatherInputsCallback.before_validate</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GatherInputsCallback.before_validate</code>()\n",
       "\n",
       "Sets the number of inputs in `self.dls`\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GatherInputsCallback.before_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GatherInputsCallback.before_batch\" class=\"doc_header\"><code>GatherInputsCallback.before_batch</code><a href=\"__main__.py#L21\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GatherInputsCallback.before_batch</code>()\n",
       "\n",
       "Turns `self.xb` from a tuple to a dictionary of either\n",
       "    `{\"input_ids\", \"attention_masks\", \"token_type_ids\"}`d\n",
       "or\n",
       "    `{\"input_ids\", \"attention_masks\"}`\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GatherInputsCallback.before_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SetInputsCallback(Callback):\n",
    "    \"\"\"\n",
    "    Callback which runs after `GatherInputsCallback` that sets `self.learn.xb`\n",
    "    \"\"\"\n",
    "    order = -1\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        as_dict=False # Whether to leave `self.xb` as a dictionary of values\n",
    "    ): store_attr()\n",
    "\n",
    "    def before_batch(self):\n",
    "        \"\"\"\n",
    "        Set `self.learn.xb` to `self.learn.inputs.values()`\n",
    "        \"\"\"\n",
    "        if not self.as_dict:\n",
    "            self.learn.xb = list(self.learn.inputs.values())\n",
    "        else:\n",
    "            self.learn.xb = self.learn.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SetInputsCallback.before_batch\" class=\"doc_header\"><code>SetInputsCallback.before_batch</code><a href=\"__main__.py#L13\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SetInputsCallback.before_batch</code>()\n",
       "\n",
       "Set `self.learn.xb` to `self.learn.inputs.values()`\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SetInputsCallback.before_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GeneratorCallback(Callback):\n",
    "    \"\"\"\n",
    "    Callback used for models that utilize `self.model.generate`\n",
    "    \"\"\"\n",
    "    \n",
    "    @delegates(PreTrainedModel.generate)\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_beams:int, # Number of beams for beam search\n",
    "        min_length:int, # Minimal length of the sequence generated\n",
    "        max_length:int, # Maximum length of the sequence generated\n",
    "        early_stopping:bool, # Whether to do early stopping\n",
    "        **kwargs\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def before_batch(self):\n",
    "        \"\"\"\n",
    "        Run model-specific inference\n",
    "        \"\"\"\n",
    "        \n",
    "        pred = self.learn.model.generate(\n",
    "            input_ids = self.xb['input_ids'],\n",
    "            attention_mask = self.xb['attention_mask'],\n",
    "            num_beams = self.num_beams,\n",
    "            min_length = self.min_length,\n",
    "            max_length = self.max_length,\n",
    "            early_stopping = self.early_stopping,\n",
    "            **self.kwargs\n",
    "        )\n",
    "        \n",
    "        self.learn.pred = pred\n",
    "        \n",
    "        raise CancelBatchException # skip original model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"GeneratorCallback\" class=\"doc_header\"><code>class</code> <code>GeneratorCallback</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>GeneratorCallback</code>(**`num_beams`**:`int`, **`min_length`**:`int`, **`max_length`**:`int`, **`early_stopping`**:`bool`, **`input_ids`**:`Optional`\\[`LongTensor`\\]=*`None`*, **`do_sample`**:`Optional`\\[`bool`\\]=*`None`*, **`temperature`**:`Optional`\\[`float`\\]=*`None`*, **`top_k`**:`Optional`\\[`int`\\]=*`None`*, **`top_p`**:`Optional`\\[`float`\\]=*`None`*, **`repetition_penalty`**:`Optional`\\[`float`\\]=*`None`*, **`bad_words_ids`**:`Optional`\\[`Iterable`\\[`int`\\]\\]=*`None`*, **`bos_token_id`**:`Optional`\\[`int`\\]=*`None`*, **`pad_token_id`**:`Optional`\\[`int`\\]=*`None`*, **`eos_token_id`**:`Optional`\\[`int`\\]=*`None`*, **`length_penalty`**:`Optional`\\[`float`\\]=*`None`*, **`no_repeat_ngram_size`**:`Optional`\\[`int`\\]=*`None`*, **`encoder_no_repeat_ngram_size`**:`Optional`\\[`int`\\]=*`None`*, **`num_return_sequences`**:`Optional`\\[`int`\\]=*`None`*, **`max_time`**:`Optional`\\[`float`\\]=*`None`*, **`max_new_tokens`**:`Optional`\\[`int`\\]=*`None`*, **`decoder_start_token_id`**:`Optional`\\[`int`\\]=*`None`*, **`use_cache`**:`Optional`\\[`bool`\\]=*`None`*, **`num_beam_groups`**:`Optional`\\[`int`\\]=*`None`*, **`diversity_penalty`**:`Optional`\\[`float`\\]=*`None`*, **`prefix_allowed_tokens_fn`**:`Optional`\\[`Callable`\\[`int`, `Tensor`, `List`\\[`int`\\]\\]\\]=*`None`*, **`output_attentions`**:`Optional`\\[`bool`\\]=*`None`*, **`output_hidden_states`**:`Optional`\\[`bool`\\]=*`None`*, **`output_scores`**:`Optional`\\[`bool`\\]=*`None`*, **`return_dict_in_generate`**:`Optional`\\[`bool`\\]=*`None`*, **`forced_bos_token_id`**:`Optional`\\[`int`\\]=*`None`*, **`forced_eos_token_id`**:`Optional`\\[`int`\\]=*`None`*, **`remove_invalid_values`**:`Optional`\\[`bool`\\]=*`None`*, **`synced_gpus`**:`Optional`\\[`bool`\\]=*`None`*) :: `Callback`\n",
       "\n",
       "Callback used for models that utilize `self.model.generate`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`num_beams`** : *`<class 'int'>`*\t<p>Number of beams for beam search</p>\n",
       "\n",
       "\n",
       " - **`min_length`** : *`<class 'int'>`*\t<p>Minimal length of the sequence generated</p>\n",
       "\n",
       "\n",
       " - **`max_length`** : *`<class 'int'>`*\t<p>Maximum length of the sequence generated</p>\n",
       "\n",
       "\n",
       " - **`early_stopping`** : *`<class 'bool'>`*\t<p>Whether to do early stopping</p>\n",
       "\n",
       "\n",
       " - **`input_ids`** : *`typing.Union[torch.LongTensor, NoneType]`*, *optional*\n",
       "\n",
       " - **`do_sample`** : *`typing.Union[bool, NoneType]`*, *optional*\n",
       "\n",
       " - **`temperature`** : *`typing.Union[float, NoneType]`*, *optional*\n",
       "\n",
       " - **`top_k`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`top_p`** : *`typing.Union[float, NoneType]`*, *optional*\n",
       "\n",
       " - **`repetition_penalty`** : *`typing.Union[float, NoneType]`*, *optional*\n",
       "\n",
       " - **`bad_words_ids`** : *`typing.Union[typing.Iterable[int], NoneType]`*, *optional*\n",
       "\n",
       " - **`bos_token_id`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`pad_token_id`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`eos_token_id`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`length_penalty`** : *`typing.Union[float, NoneType]`*, *optional*\n",
       "\n",
       " - **`no_repeat_ngram_size`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`encoder_no_repeat_ngram_size`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`num_return_sequences`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`max_time`** : *`typing.Union[float, NoneType]`*, *optional*\n",
       "\n",
       " - **`max_new_tokens`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`decoder_start_token_id`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`use_cache`** : *`typing.Union[bool, NoneType]`*, *optional*\n",
       "\n",
       " - **`num_beam_groups`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`diversity_penalty`** : *`typing.Union[float, NoneType]`*, *optional*\n",
       "\n",
       " - **`prefix_allowed_tokens_fn`** : *`typing.Union[typing.Callable[[int, torch.Tensor], typing.List[int]], NoneType]`*, *optional*\n",
       "\n",
       " - **`output_attentions`** : *`typing.Union[bool, NoneType]`*, *optional*\n",
       "\n",
       " - **`output_hidden_states`** : *`typing.Union[bool, NoneType]`*, *optional*\n",
       "\n",
       " - **`output_scores`** : *`typing.Union[bool, NoneType]`*, *optional*\n",
       "\n",
       " - **`return_dict_in_generate`** : *`typing.Union[bool, NoneType]`*, *optional*\n",
       "\n",
       " - **`forced_bos_token_id`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`forced_eos_token_id`** : *`typing.Union[int, NoneType]`*, *optional*\n",
       "\n",
       " - **`remove_invalid_values`** : *`typing.Union[bool, NoneType]`*, *optional*\n",
       "\n",
       " - **`synced_gpus`** : *`typing.Union[bool, NoneType]`*, *optional*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GeneratorCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
