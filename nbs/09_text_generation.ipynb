{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp inference.text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "> Text Generation API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import test_eq\n",
    "from nbverbose.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "from adaptnlp.model import AdaptiveModel, DataLoader\n",
    "from adaptnlp.model_hub import HFModelResult\n",
    "\n",
    "from fastai.torch_core import apply, default_device, to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTextGenerator(AdaptiveModel):\n",
    "    \"Adaptive model for Transformer's Language Models\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizer, # A tokenizer object from Huggingface's transformers (TODO)and tokenizers\n",
    "        model: PreTrainedModel #  A transformers Language model\n",
    "    ):\n",
    "        # Load up model and tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        super().__init__()\n",
    "\n",
    "        # Sets internal model\n",
    "        self.set_model(model)\n",
    "        \n",
    "         # Setup cuda and automatic allocation of model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls, \n",
    "        model_name_or_path: str # A key string of one of Transformer's pre-trained Language Model\n",
    "    ) -> AdaptiveModel:\n",
    "        \"Class method for loading and constructing this Model\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, pad_token=\"<PAD>\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "        generator = cls(tokenizer, model)\n",
    "        return generator\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[str], str], # Sentences to run inference on\n",
    "        mini_batch_size: int = 32, # Mini batch size\n",
    "        num_tokens_to_produce: int = 50, # Number of tokens you want to generate\n",
    "        **kwargs, # Optional arguments for the Transformers `PreTrainedModel.generate()` method\n",
    "    ) -> List[str]: # A list of predicted sentences\n",
    "        \"Predict method for running inference using the pre-trained sequence classifier model.  Keyword arguments for parameters of the method `Transformers.PreTrainedModel.generate()` can be used as well.\"\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Make all inputs lists\n",
    "            if isinstance(text, str):\n",
    "                text = [text]\n",
    "\n",
    "            dataset = self._tokenize(text)\n",
    "            dataloader = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "            results = []\n",
    "\n",
    "            logger.info(f'Running text generator on {len(dataset)} text sequences')\n",
    "            logger.info(f'Batch size = {mini_batch_size}')\n",
    "            for batch in progress_bar(dataloader):\n",
    "                self.model.eval()\n",
    "                batch = apply(to_device, batch)\n",
    "\n",
    "                if len(batch) == 3:\n",
    "                    inputs = {\n",
    "                        'input_ids': batch[0],\n",
    "                        'attention_masks': batch[1],\n",
    "                        'token_type_ids': batch[2],\n",
    "                    }\n",
    "                else:\n",
    "                    inputs = {\n",
    "                        'input_ids': batch[0],\n",
    "                        'attention_masks': batch[1],\n",
    "                    }\n",
    "                # model.generate() does not have batch inference implemented yet\n",
    "                generated_text = self._batch_generate(\n",
    "                    inputs=inputs,\n",
    "                    seq_len=batch[0].shape[1],\n",
    "                    num_tokens_to_produce=num_tokens_to_produce,\n",
    "                )\n",
    "                results += generated_text\n",
    "\n",
    "        return {\"generated_text\":results}\n",
    "\n",
    "    def _tokenize(self, text: Union[List[str], str]) -> TensorDataset:\n",
    "        \"\"\" Batch tokenizes text and produces a `TensorDataset` with text \"\"\"\n",
    "\n",
    "        tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "        )\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            tokenized_text[\"input_ids\"],\n",
    "            tokenized_text[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _batch_generate(\n",
    "        self, inputs: Dict, seq_len: int, num_tokens_to_produce: int\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generates text data with varying text sizes\"\"\"\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attn_mask = inputs[\"attention_masks\"]\n",
    "\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        eos_token_id = self.tokenizer.eos_token_id\n",
    "        eos_not_in_sents = torch.ones(input_ids.shape[0]).long().to(self.device)\n",
    "\n",
    "        # we need to get the token ids of the last non-padded value\n",
    "        last_non_masked_idx = torch.sum(attn_mask, dim=1) - 1\n",
    "        start_idx = (\n",
    "            (last_non_masked_idx)\n",
    "            .view(-1, 1)\n",
    "            .repeat(1, self.tokenizer.vocab_size)\n",
    "            .unsqueeze(1)\n",
    "        )\n",
    "\n",
    "        # get correct position ids\n",
    "        position_ids = torch.tensor(\n",
    "            [list(range(seq_len)) for i in range(input_ids.shape[0])]\n",
    "        ).to(self.device)\n",
    "        for i, position_ids_slice in enumerate(position_ids):\n",
    "            position_ids_slice[last_non_masked_idx[i] :] = position_ids_slice[\n",
    "                last_non_masked_idx[i]\n",
    "            ]\n",
    "\n",
    "        for step in range(num_tokens_to_produce):\n",
    "            outputs = self.model(\n",
    "                input_ids, attention_mask=attn_mask, position_ids=position_ids\n",
    "            )\n",
    "\n",
    "            # in the first decoding step, we want to use the 'real' last position for each sentence\n",
    "            if step == 0:\n",
    "                next_token_logits = outputs[0].gather(1, start_idx).squeeze(1)\n",
    "            else:\n",
    "                next_token_logits = outputs[0][:, -1, :]\n",
    "\n",
    "            next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # this updates which sentences have not seen an <EOS> token so far\n",
    "            # if one <EOS> token was seen the sentence is finished\n",
    "            eos_not_in_sents.mul_(next_tokens.ne(eos_token_id).long())\n",
    "\n",
    "            # either append a padding token here if <EOS> has been seen or append next token\n",
    "            tokens_to_add = next_tokens * (eos_not_in_sents) + pad_token_id * (\n",
    "                1 - eos_not_in_sents\n",
    "            )\n",
    "\n",
    "            # Update input_ids, attn_mask and position_ids\n",
    "            input_ids = torch.cat([input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "            attn_mask = torch.cat(\n",
    "                [attn_mask, torch.ones((attn_mask.shape[0], 1)).long().to(self.device)],\n",
    "                dim=1,\n",
    "            )\n",
    "            position_ids = torch.cat(\n",
    "                [position_ids, (position_ids[:, -1] + 1).unsqueeze(-1)], dim=1\n",
    "            )\n",
    "\n",
    "        return [\n",
    "            self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "            for output in input_ids\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TransformersTextGenerator.load\" class=\"doc_header\"><code>TransformersTextGenerator.load</code><a href=\"__main__.py#L21\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TransformersTextGenerator.load</code>(**`model_name_or_path`**:`str`)\n",
       "\n",
       "Class method for loading and constructing this Model\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`model_name_or_path`** : *`<class 'str'>`*\t<p>A key string of one of Transformer's pre-trained Language Model</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`<class 'adaptnlp.model.AdaptiveModel'>`*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TransformersTextGenerator.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb0353437884ed2b22acab0bdbdcf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0e64a0b5f44bd09ff1c02a3f4e1d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210d4a6b35b34829b1d0e983190f515e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2478176b2efb41678eaee60758bc26dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2c6693be684eac875b1d2df20a4c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "o = TransformersTextGenerator.load('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TransformersTextGenerator.predict\" class=\"doc_header\"><code>TransformersTextGenerator.predict</code><a href=\"__main__.py#L32\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TransformersTextGenerator.predict</code>(**`text`**:`Union`\\[`List`\\[`str`\\], `str`\\], **`mini_batch_size`**:`int`=*`32`*, **`num_tokens_to_produce`**:`int`=*`50`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Predict method for running inference using the pre-trained sequence classifier model.  Keyword arguments for parameters of the method `Transformers.PreTrainedModel.generate()` can be used as well.\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`text`** : *`typing.Union[typing.List[str], str]`*\t<p>Sentences to run inference on</p>\n",
       "\n",
       "\n",
       " - **`mini_batch_size`** : *`<class 'int'>`*, *optional*\t<p>Mini batch size</p>\n",
       "\n",
       "\n",
       " - **`num_tokens_to_produce`** : *`<class 'int'>`*, *optional*\t<p>Number of tokens you want to generate</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`typing.List[str]`*\t<p>A list of predicted sentences</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TransformersTextGenerator.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyTextGenerator:\n",
    "    \"Text Generation Module\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.generators: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        text: Union[List[str], str], # List of sentences to run inference on\n",
    "        model_name_or_path: [str, HFModelResult] = \"gpt2\", # A model id or path to a pre-trained model repository or custom trained model directory\n",
    "        mini_batch_size: int = 32, # Mini batch size\n",
    "        num_tokens_to_produce: int = 50, # Number of tokens you want to generate\n",
    "        **kwargs, # Optional arguments for the Transformers `PreTrainedModel.generate()` method\n",
    "    ) -> List[str]: # A list of predicted sentences\n",
    "        \"Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method `Transformers.PreTrainedModel.generate()` can be used as well.\"\n",
    "        name = getattr(model_name_or_path, 'name', model_name_or_path)\n",
    "        if not self.generators[name]:\n",
    "            self.generators[name] = TransformersTextGenerator.load(\n",
    "                name\n",
    "            )\n",
    "\n",
    "        generator = self.generators[name]\n",
    "        return generator.predict(\n",
    "            text=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            num_tokens_to_produce=num_tokens_to_produce,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EasyTextGenerator.generate\" class=\"doc_header\"><code>EasyTextGenerator.generate</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EasyTextGenerator.generate</code>(**`text`**:`Union`\\[`List`\\[`str`\\], `str`\\], **`model_name_or_path`**:`HFModelResult'>]`=*`'gpt2'`*, **`mini_batch_size`**:`int`=*`32`*, **`num_tokens_to_produce`**:`int`=*`50`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method `Transformers.PreTrainedModel.generate()` can be used as well.\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`text`** : *`typing.Union[typing.List[str], str]`*\t<p>List of sentences to run inference on</p>\n",
       "\n",
       "\n",
       " - **`model_name_or_path`** : *`[<class 'str'>, <class 'adaptnlp.model_hub.HFModelResult'>]`*, *optional*\t<p>A model id or path to a pre-trained model repository or custom trained model directory</p>\n",
       "\n",
       "\n",
       " - **`mini_batch_size`** : *`<class 'int'>`*, *optional*\t<p>Mini batch size</p>\n",
       "\n",
       "\n",
       " - **`num_tokens_to_produce`** : *`<class 'int'>`*, *optional*\t<p>Number of tokens you want to generate</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`typing.List[str]`*\t<p>A list of predicted sentences</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EasyTextGenerator.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "# Text from encyclopedia Britannica on Einstein\n",
    "text = \"What has happened?\"\n",
    "\n",
    "generator = EasyTextGenerator()\n",
    "generated_text = generator.generate(text, model_name_or_path=\"gpt2\", mini_batch_size=2, num_tokens_to_produce=50)\n",
    "test_eq(generated_text['generated_text'], ['What has happened?\\n\\nThe first thing that happened was that I was in a room with a bunch of people who were all very nice and nice people. I was sitting in a chair and they were all talking about how they were going to get a job and how'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from adaptnlp.model_hub import HFModelHub\n",
    "hub = HFModelHub()\n",
    "model = hub.search_model_by_name('gpt2')[-1]\n",
    "generated_text = generator.generate(text, model_name_or_path=model, mini_batch_size=2, num_tokens_to_produce=50)\n",
    "test_eq(generated_text['generated_text'], ['What has happened?\\n\\nThe first thing that happened was that I was in a room with a bunch of people who were all very nice and nice people. I was sitting in a chair and they were all talking about how they were going to get a job and how'])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
