{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "> Text Generation API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "from adaptnlp.model import AdaptiveModel, DataLoader\n",
    "from adaptnlp.model_hub import HFModelResult\n",
    "\n",
    "from fastai_minima.utils import apply, default_device, to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTextGenerator(AdaptiveModel):\n",
    "    \"\"\"Adaptive model for Transformer's Language Models\n",
    "\n",
    "    Usage:\n",
    "    ```python\n",
    "    >>> generator = TransformersTextGenerator.load(\"gpt2\")\n",
    "    >>> generator.generate(text=\"Example text\", mini_batch_size=32)\n",
    "    ```\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    * **tokenizer** - A tokenizer object from Huggingface's transformers (TODO)and tokenizers\n",
    "    * **model** - A transformers Language model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n",
    "        # Load up model and tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        super().__init__()\n",
    "\n",
    "        # Sets internal model\n",
    "        self.set_model(model)\n",
    "        \n",
    "         # Setup cuda and automatic allocation of model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading and constructing this Model\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Transformer's pre-trained Language Model\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, pad_token=\"<PAD>\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "        generator = cls(tokenizer, model)\n",
    "        return generator\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        text: Union[List[str], str],\n",
    "        mini_batch_size: int = 32,\n",
    "        num_tokens_to_produce: int = 50,\n",
    "        **kwargs,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model.  Keyword arguments\n",
    "        for parameters of the method `Transformers.PreTrainedModel.generate()` can be used as well.\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **num_tokens_to_produce** - Number of tokens you want to generate\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Make all inputs lists\n",
    "            if isinstance(text, str):\n",
    "                text = [text]\n",
    "\n",
    "            dataset = self._tokenize(text)\n",
    "            dataloader = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "            results = []\n",
    "\n",
    "            logger.info(f'Running text generator on {len(dataset)} text sequences')\n",
    "            logger.info(f'Batch size = {mini_batch_size}')\n",
    "            for batch in progress_bar(dataloader):\n",
    "                self.model.eval()\n",
    "                batch = apply(to_device, batch)\n",
    "\n",
    "                if len(batch) == 3:\n",
    "                    inputs = {\n",
    "                        'input_ids': batch[0],\n",
    "                        'attention_masks': batch[1],\n",
    "                        'token_type_ids': batch[2],\n",
    "                    }\n",
    "                else:\n",
    "                    inputs = {\n",
    "                        'input_ids': batch[0],\n",
    "                        'attention_masks': batch[1],\n",
    "                    }\n",
    "                # model.generate() does not have batch inference implemented yet\n",
    "                generated_text = self._batch_generate(\n",
    "                    inputs=inputs,\n",
    "                    seq_len=batch[0].shape[1],\n",
    "                    num_tokens_to_produce=num_tokens_to_produce,\n",
    "                )\n",
    "                results += generated_text\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _tokenize(self, text: Union[List[str], str]) -> TensorDataset:\n",
    "        \"\"\" Batch tokenizes text and produces a `TensorDataset` with text \"\"\"\n",
    "\n",
    "        tokenized_text = self.tokenizer.batch_encode_plus(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "        )\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            tokenized_text[\"input_ids\"],\n",
    "            tokenized_text[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _batch_generate(\n",
    "        self, inputs: Dict, seq_len: int, num_tokens_to_produce: int\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generates text data with varying text sizes\"\"\"\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attn_mask = inputs[\"attention_masks\"]\n",
    "\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        eos_token_id = self.tokenizer.eos_token_id\n",
    "        eos_not_in_sents = torch.ones(input_ids.shape[0]).long().to(self.device)\n",
    "\n",
    "        # we need to get the token ids of the last non-padded value\n",
    "        last_non_masked_idx = torch.sum(attn_mask, dim=1) - 1\n",
    "        start_idx = (\n",
    "            (last_non_masked_idx)\n",
    "            .view(-1, 1)\n",
    "            .repeat(1, self.tokenizer.vocab_size)\n",
    "            .unsqueeze(1)\n",
    "        )\n",
    "\n",
    "        # get correct position ids\n",
    "        position_ids = torch.tensor(\n",
    "            [list(range(seq_len)) for i in range(input_ids.shape[0])]\n",
    "        ).to(self.device)\n",
    "        for i, position_ids_slice in enumerate(position_ids):\n",
    "            position_ids_slice[last_non_masked_idx[i] :] = position_ids_slice[\n",
    "                last_non_masked_idx[i]\n",
    "            ]\n",
    "\n",
    "        for step in range(num_tokens_to_produce):\n",
    "            outputs = self.model(\n",
    "                input_ids, attention_mask=attn_mask, position_ids=position_ids\n",
    "            )\n",
    "\n",
    "            # in the first decoding step, we want to use the 'real' last position for each sentence\n",
    "            if step == 0:\n",
    "                next_token_logits = outputs[0].gather(1, start_idx).squeeze(1)\n",
    "            else:\n",
    "                next_token_logits = outputs[0][:, -1, :]\n",
    "\n",
    "            next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # this updates which sentences have not seen an <EOS> token so far\n",
    "            # if one <EOS> token was seen the sentence is finished\n",
    "            eos_not_in_sents.mul_(next_tokens.ne(eos_token_id).long())\n",
    "\n",
    "            # either append a padding token here if <EOS> has been seen or append next token\n",
    "            tokens_to_add = next_tokens * (eos_not_in_sents) + pad_token_id * (\n",
    "                1 - eos_not_in_sents\n",
    "            )\n",
    "\n",
    "            # Update input_ids, attn_mask and position_ids\n",
    "            input_ids = torch.cat([input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "            attn_mask = torch.cat(\n",
    "                [attn_mask, torch.ones((attn_mask.shape[0], 1)).long().to(self.device)],\n",
    "                dim=1,\n",
    "            )\n",
    "            position_ids = torch.cat(\n",
    "                [position_ids, (position_ids[:, -1] + 1).unsqueeze(-1)], dim=1\n",
    "            )\n",
    "\n",
    "        return [\n",
    "            self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "            for output in input_ids\n",
    "        ]\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "    ):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyTextGenerator:\n",
    "    \"\"\"Text Generation Module\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> generator = EasyGenerator()\n",
    "    >>> generator.generate(text=\"generate from this text\", num_tokens_to_produce=50)\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.generators: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        text: Union[List[str], str],\n",
    "        model_name_or_path: [str, HFModelResult] = \"gpt2\",\n",
    "        mini_batch_size: int = 32,\n",
    "        num_tokens_to_produce: int = 50,\n",
    "        **kwargs,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments\n",
    "        for parameters of the method `Transformers.PreTrainedModel.generate()` can be used as well.\n",
    "\n",
    "        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n",
    "        * **model_name_or_path** - A String model id or path to a pre-trained model repository or custom trained model directory\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **num_tokens_to_produce** - Number of tokens you want to generate\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n",
    "        \"\"\"\n",
    "        name = getattr(model_name_or_path, 'name', model_name_or_path)\n",
    "        if not self.generators[name]:\n",
    "            self.generators[name] = TransformersTextGenerator.load(\n",
    "                name\n",
    "            )\n",
    "\n",
    "        generator = self.generators[name]\n",
    "        return generator.predict(\n",
    "            text=text,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            num_tokens_to_produce=num_tokens_to_produce,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "# Text from encyclopedia Britannica on Einstein\n",
    "text = \"What has happened?\"\n",
    "\n",
    "generator = EasyTextGenerator()\n",
    "generated_text = generator.generate(text, model_name_or_path=\"gpt2\", mini_batch_size=2, num_tokens_to_produce=50)\n",
    "test_eq(generated_text, ['What has happened?\\n\\nThe first thing that happened was that I was in a room with a bunch of people who were all very nice and nice people. I was sitting in a chair and they were all talking about how they were going to get a job and how'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from adaptnlp.model_hub import HFModelHub\n",
    "hub = HFModelHub()\n",
    "model = hub.search_model_by_name('gpt2')[-1]\n",
    "generated_text = generator.generate(text, model_name_or_path=model, mini_batch_size=2, num_tokens_to_produce=50)\n",
    "test_eq(generated_text, ['What has happened?\\n\\nThe first thing that happened was that I was in a room with a bunch of people who were all very nice and nice people. I was sitting in a chair and they were all talking about how they were going to get a job and how'])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
