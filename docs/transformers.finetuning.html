---

title: Transformers Fine-Tuning


keywords: fastai
sidebar: home_sidebar

summary: "Fine-tuning transformers models"
description: "Fine-tuning transformers models"
nb_path: "nbs/13b_transformers.finetuning.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/13b_transformers.finetuning.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="tqdm" class="doc_header"><code>tqdm</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/transformers/finetuning.py#L75" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>tqdm</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TextDataset" class="doc_header"><code>class</code> <code>TextDataset</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/transformers/finetuning.py#L85" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TextDataset</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>Dataset</code></p>
</blockquote>
<p>An abstract class representing a :class:<code>Dataset</code>.</p>
<p>All datasets that represent a map from keys to data samples should subclass
it. All subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a
data sample for a given key. Subclasses could also optionally overwrite
:meth:<code>__len__</code>, which is expected to return the size of the dataset by many
:class:<code>~torch.utils.data.Sampler</code> implementations and the default options
of :class:<code>~torch.utils.data.DataLoader</code>.</p>
<p>.. note::
  :class:<code>~torch.utils.data.DataLoader</code> by default constructs a index
  sampler that yields integral indices.  To make it work with a map-style
  dataset with non-integral indices/keys, a custom sampler must be provided.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LMFineTunerManual" class="doc_header"><code>class</code> <code>LMFineTunerManual</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/transformers/finetuning.py#L153" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LMFineTunerManual</code>(<strong><code>train_data_file</code></strong>:<code>str</code>, <strong><code>eval_data_file</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>model_type</code></strong>:<code>str</code>=<em><code>'bert'</code></em>, <strong><code>model_name_or_path</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>mlm</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>mlm_probability</code></strong>:<code>float</code>=<em><code>0.15</code></em>, <strong><code>config_name</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>tokenizer_name</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>cache_dir</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>block_size</code></strong>:<code>int</code>=<em><code>-1</code></em>, <strong><code>no_cuda</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>overwrite_cache</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>seed</code></strong>:<code>int</code>=<em><code>42</code></em>, <strong><code>fp16</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>fp16_opt_level</code></strong>:<code>str</code>=<em><code>'O1'</code></em>, <strong><code>local_rank</code></strong>:<code>int</code>=<em><code>-1</code></em>)</p>
</blockquote>
<p>A Language Model Fine Tuner object you can set language model configurations and then train and evaluate</p>
<p>Usage:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">finetuner</span> <span class="o">=</span> <span class="n">adaptnlp</span><span class="o">.</span><span class="n">LMFineTuner</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">finetuner</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong>train_data_file</strong> - The input training data file (a text file).</li>
<li><strong>eval_data_file</strong> - An optional input evaluation data file to evaluate the perplexity on (a text file).</li>
<li><strong>model_type</strong> - The model architecture to be trained or fine-tuned.</li>
<li><strong>model_name_or_path</strong> - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.</li>
<li><strong>mlm</strong> - Train with masked-language modeling loss instead of language modeling.</li>
<li><strong>mlm_probability</strong> - Ratio of tokens to mask for masked language modeling loss</li>
<li><strong>config_name</strong> - Optional Transformers pretrained config name or path if not the same as model_name_or_path. If both are None, initialize a new config.</li>
<li><strong>tokenizer_name</strong> - Optional Transformers pretrained tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new tokenizer.</li>
<li><strong>cache_dir</strong> - Optional directory to store the pre-trained models downloaded from s3 (If None, will go to default dir)</li>
<li><strong>block_size</strong> - Optional input sequence length after tokenization.
<pre><code>              The training dataset will be truncated in block of this size for training."
              `-1` will default to the model max input length for single sentence inputs (take into account special tokens).</code></pre>
</li>
<li><strong>no_cuda</strong> - Avoid using CUDA when available</li>
<li><strong>overwrite_cache</strong> - Overwrite the cached training and evaluation sets</li>
<li><strong>seed</strong> - random seed for initialization</li>
<li><strong>fp16</strong> - Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit</li>
<li><strong>fp16_opt_level</strong> - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].</li>
<li><strong>local_rank</strong> - For distributed training: local_rank</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

</div>
 

