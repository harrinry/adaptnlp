---

title: Language Models


keywords: fastai
sidebar: home_sidebar

summary: "Language Models within the AdaptNLP library"
description: "Language Models within the AdaptNLP library"
nb_path: "nbs/11_language_model.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/11_language_model.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LMFineTuner" class="doc_header"><code>class</code> <code>LMFineTuner</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/language_model.py#L35" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LMFineTuner</code>(<strong><code>model_name_or_path</code></strong>:<code>Union</code>[<code>str</code>, <a href="/adaptnlp/model_hub.html#HFModelResult"><code>HFModelResult</code></a>]=<em><code>'bert-base-cased'</code></em>)</p>
</blockquote>
<p>A Language Model Fine Tuner object you can set language model configurations and then train and evaluate</p>
<p>Usage:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">finetuner</span> <span class="o">=</span> <span class="n">adaptnlp</span><span class="o">.</span><span class="n">LMFineTuner</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">finetuner</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong>model_name_or_path</strong> - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details class="description">
<summary data-open="Hide Examples" data-close="Show Examples"></summary>
<summary></summary>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/20b_tutorial.fine_tuning_manual.ipynb>Source</a></b>

<code>
ft_configs = {
              "train_data_file": train_data_file,
              "eval_data_file": eval_data_file,
              "model_type": "bert",
              "model_name_or_path": "bert-base-cased",
              "mlm": True,
              "mlm_probability": 0.15,
              "config_name": None,
              "tokenizer_name": None,
              "cache_dir": None,
              "block_size": -1,
              "no_cuda": False,
              "overwrite_cache": False,
              "seed": 42,
              "fp16": False,
              "fp16_opt_level": "O1",
              "local_rank": -1,
             }
finetuner = LMFineTuner(**ft_configs)
finetuner.freeze()
learning_rate_finder_configs = {
    "output_dir": OUTPUT_DIR,
    "file_name": "learning_rate.tsv",
    "start_learning_rate": 1e-7,
    "end_learning_rate": 10,
    "iterations": 100,
    "mini_batch_size": 8,
    "stop_early": True,
    "smoothing_factor": 0.7,
    "adam_epsilon": 1e-8,
    "weight_decay": 0.0,
}
learning_rate = finetuner.find_learning_rate(**learning_rate_finder_configs)
finetuner.freeze()
</code>
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

</div>
 

