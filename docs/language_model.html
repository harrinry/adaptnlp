---

title: Language Models


keywords: fastai
sidebar: home_sidebar

summary: "Language Models within the AdaptNLP library"
description: "Language Models within the AdaptNLP library"
nb_path: "nbs/11_language_model.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/11_language_model.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LMFineTuner" class="doc_header"><code>class</code> <code>LMFineTuner</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/language_model.py#L35" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LMFineTuner</code>(<strong><code>model_name_or_path</code></strong>:<code>Union</code>[<code>str</code>, <a href="/adaptnlp/model_hub.html#HFModelResult"><code>HFModelResult</code></a>]=<em><code>'bert-base-cased'</code></em>)</p>
</blockquote>
<p>A Language Model Fine Tuner object you can set language model configurations and then train and evaluate</p>
<p>Usage:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">finetuner</span> <span class="o">=</span> <span class="n">adaptnlp</span><span class="o">.</span><span class="n">LMFineTuner</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">finetuner</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong>model_name_or_path</strong> - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<p>Sample code from: 20b_tutorial.fine_tuning_manual.ipynb (<a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/20b_tutorial.fine_tuning_manual.ipynb>View Notebook</a> for more context)</p>
<div class="highlight"><pre><span></span><span class="n">ft_configs</span> <span class="o">=</span> <span class="p">{</span>
              <span class="s2">&quot;train_data_file&quot;</span><span class="p">:</span> <span class="n">train_data_file</span><span class="p">,</span>
              <span class="s2">&quot;eval_data_file&quot;</span><span class="p">:</span> <span class="n">eval_data_file</span><span class="p">,</span>
              <span class="s2">&quot;model_type&quot;</span><span class="p">:</span> <span class="s2">&quot;bert&quot;</span><span class="p">,</span>
              <span class="s2">&quot;model_name_or_path&quot;</span><span class="p">:</span> <span class="s2">&quot;bert-base-cased&quot;</span><span class="p">,</span>
              <span class="s2">&quot;mlm&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
              <span class="s2">&quot;mlm_probability&quot;</span><span class="p">:</span> <span class="mf">0.15</span><span class="p">,</span>
              <span class="s2">&quot;config_name&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
              <span class="s2">&quot;tokenizer_name&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
              <span class="s2">&quot;cache_dir&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
              <span class="s2">&quot;block_size&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
              <span class="s2">&quot;no_cuda&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
              <span class="s2">&quot;overwrite_cache&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
              <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span>
              <span class="s2">&quot;fp16&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
              <span class="s2">&quot;fp16_opt_level&quot;</span><span class="p">:</span> <span class="s2">&quot;O1&quot;</span><span class="p">,</span>
              <span class="s2">&quot;local_rank&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
             <span class="p">}</span>
<span class="n">finetuner</span> <span class="o">=</span> <span class="n">LMFineTuner</span><span class="p">(</span><span class="o">**</span><span class="n">ft_configs</span><span class="p">)</span>
<span class="n">finetuner</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

</div>
 

