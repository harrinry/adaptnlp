---

title: Tutorial - Sequence Classification


keywords: fastai
sidebar: home_sidebar

summary: "Performing Sequence Classification with AdaptNLP"
description: "Performing Sequence Classification with AdaptNLP"
nb_path: "nbs/06a_tutorial.easy_sequence_classifier.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/06a_tutorial.easy_sequence_classifier.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Sequence Classification (or Text Classification) is the NLP task of predicting a label for a sequence of words.</p>
<p>For example, a string of <code>That movie was terrible because the acting was bad</code> could be tagged with a label of <code>negative</code>. A string of <code>That movie was great because the acting was good</code> could be tagged with a label of <code>positive</code>.</p>
<p>A model that can predict sentiment from text is called a sentiment classifier, which is an example of a sequence classification model.</p>
<p>Below, we'll walk through how we can use AdaptNLP's EasySequenceClassification module to easily do the following:</p>
<ol>
<li>Load pre-trained models and tag data using mini-batched inference</li>
<li>Train and fine-tune a pre-trained model on your own dataset</li>
<li>Evaluate your model</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loading-Pretrained-Models-and-Tag-Data-using-Mini-Batched-Inference">Loading Pretrained Models and Tag Data using Mini-Batched Inference<a class="anchor-link" href="#Loading-Pretrained-Models-and-Tag-Data-using-Mini-Batched-Inference"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll first get started by importing the EasySequenceClassifier class from AdaptNLP and instantiating the
<a href="/adaptnlpsequence_classification.html#EasySequenceClassifier"><code>EasySequenceClassifier</code></a> class object.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">EasySequenceClassifier</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">EasySequenceClassifier</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With this class we can dynamically load models to run on inference.</p>
<p>Let's use the <a href="/adaptnlpmodel_hub.html#HFModelHub"><code>HFModelHub</code></a> to search for some pre-trained sequence classification models to use:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp.model_hub</span> <span class="kn">import</span> <span class="n">HFModelHub</span>
<span class="n">hub</span> <span class="o">=</span> <span class="n">HFModelHub</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can either seach by task or by model name. Below is an example of the associated models HuggingFace has come out with:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hub</span><span class="o">.</span><span class="n">search_model_by_task</span><span class="p">(</span><span class="s1">&#39;text-classification&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[Model Name: distilbert-base-uncased-finetuned-sst-2-english, Tasks: [text-classification],
 Model Name: roberta-base-openai-detector, Tasks: [text-classification],
 Model Name: roberta-large-mnli, Tasks: [text-classification],
 Model Name: roberta-large-openai-detector, Tasks: [text-classification]]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For this example though we will tag some text with a model that <a href="https://www.nlp.town/">NLP Town</a> has trained called <code>nlptown/bert-base-multilingual-uncased-sentiment</code>. Let's find it in the model hub:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">search_model_by_name</span><span class="p">(</span><span class="s1">&#39;nlptown/bert-base&#39;</span><span class="p">,</span> <span class="n">user_uploaded</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">];</span> <span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Model Name: nlptown/bert-base-multilingual-uncased-sentiment, Tasks: [text-classification]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a multi-lingual model that predicts how many stars (1-5) a text review has given a product. More information can be found via. the Transformers model card <a href="https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment">here</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we can perform some inference. First let's write some example text:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">example_text</span> <span class="o">=</span> <span class="s2">&quot;This didn&#39;t work at all&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we can tell our classifier to tag some text with <code>tag_text</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">tag_text</span><span class="p">(</span>
    <span class="n">text</span><span class="o">=</span><span class="n">example_text</span><span class="p">,</span>
    <span class="n">model_name_or_path</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">mini_batch_size</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>2021-04-20 19:15:43,548 loading file nlptown/bert-base-multilingual-uncased-sentiment
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's look at our outputs:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tag Score Outputs:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
    <span class="n">pprint</span><span class="p">({</span><span class="n">sentence</span><span class="o">.</span><span class="n">to_original_text</span><span class="p">():</span> <span class="n">sentence</span><span class="o">.</span><span class="n">labels</span><span class="p">})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tag Score Outputs:

{&#34;This didn&#39;t work at all&#34;: [1 star (0.8421),
                             2 stars (0.1379),
                             3 stars (0.018),
                             4 stars (0.0012),
                             5 stars (0.0007)]}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's easy to pass in multiple sentences at once as well (in an array). Let's try that now:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">multiple_text</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;This didn&#39;t work well at all.&quot;</span><span class="p">,</span>
                 <span class="s2">&quot;I really liked it.&quot;</span><span class="p">,</span>
                 <span class="s2">&quot;It was really useful.&quot;</span><span class="p">,</span>
                 <span class="s2">&quot;It broke after I bought it.&quot;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll pass it into the <code>classifier</code> just like before:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">tag_text</span><span class="p">(</span>
    <span class="n">text</span><span class="o">=</span><span class="n">multiple_text</span><span class="p">,</span>
    <span class="n">model_name_or_path</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">mini_batch_size</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we can check the outputs again:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tag Score Outputs:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
    <span class="n">pprint</span><span class="p">({</span><span class="n">sentence</span><span class="o">.</span><span class="n">to_original_text</span><span class="p">():</span> <span class="n">sentence</span><span class="o">.</span><span class="n">labels</span><span class="p">})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tag Score Outputs:

{&#34;This didn&#39;t work well at all.&#34;: [1 star (0.622),
                                   2 stars (0.3356),
                                   3 stars (0.0403),
                                   4 stars (0.0016),
                                   5 stars (0.0005)]}
{&#39;I really liked it.&#39;: [1 star (0.0032),
                        2 stars (0.0048),
                        3 stars (0.054),
                        4 stars (0.4813),
                        5 stars (0.4567)]}
{&#39;It was really useful.&#39;: [1 star (0.006),
                           2 stars (0.0093),
                           3 stars (0.0701),
                           4 stars (0.4136),
                           5 stars (0.501)]}
{&#39;It broke after I bought it.&#39;: [1 star (0.4489),
                                 2 stars (0.3935),
                                 3 stars (0.1416),
                                 4 stars (0.0121),
                                 5 stars (0.0039)]}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='The output is going to be a probility distribution of what the text should be tagged. If you&#8217;re running this on a GPU, you can specify the <code>mini_batch_size</code> parameter to run mini-batch inference against your data for faster run time.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can set <code>model_name_or_path</code> to any of Transformer's or Flair's pre-trained sequence classification models.</p>
<p>Let's tag some text with another model, specifically Oliver Guhr's German sentiment model called <code>oliverguhr/german-sentiment-bert</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we'll write some german text:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">german_text</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Das hat überhaupt nicht gut funktioniert.&quot;</span><span class="p">,</span>
               <span class="s2">&quot;Ich mochte es wirklich.&quot;</span><span class="p">,</span>
               <span class="s2">&quot;Es war wirklich nützlich.&quot;</span><span class="p">,</span>
               <span class="s2">&quot;Es ist kaputt gegangen, nachdem ich es gekauft habe.&quot;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And then tag it:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">tag_text</span><span class="p">(</span>
    <span class="n">german_text</span><span class="p">,</span>
    <span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;oliverguhr/german-sentiment-bert&quot;</span><span class="p">,</span>
    <span class="n">mini_batch_size</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='As seen here, you can either search for a model through the various <code>ModelHub</code> classes, or you can directly pass in the string to the model you want' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's look at the output:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tag Score Outputs:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
    <span class="n">pprint</span><span class="p">({</span><span class="n">sentence</span><span class="o">.</span><span class="n">to_original_text</span><span class="p">():</span> <span class="n">sentence</span><span class="o">.</span><span class="n">labels</span><span class="p">})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tag Score Outputs:

{&#39;Das hat überhaupt nicht gut funktioniert.&#39;: [positive (0.0008),
                                               negative (0.9991),
                                               neutral (0.0)]}
{&#39;Ich mochte es wirklich.&#39;: [positive (0.7023),
                             negative (0.2029),
                             neutral (0.0947)]}
{&#39;Es war wirklich nützlich.&#39;: [positive (0.9813),
                               negative (0.0184),
                               neutral (0.0002)]}
{&#39;Es ist kaputt gegangen, nachdem ich es gekauft habe.&#39;: [positive (0.0042),
                                                          negative (0.9957),
                                                          neutral (0.0001)]}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Don't forget you can still quickly run inference with the multi-lingual review sentiment model you loaded in earlier (memory permitting)! Just change the <code>model_name_or_path</code> param to the model you used before.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's release the german sentiment model to free up some memory for our next step...training!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">release_model</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;oliverguhr/german-sentiment-bert&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-and-Fine-Tune-a-Pre-Trained-Model-on-Your-Own-Dataset">Train and Fine-Tune a Pre-Trained Model on Your Own Dataset<a class="anchor-link" href="#Train-and-Fine-Tune-a-Pre-Trained-Model-on-Your-Own-Dataset"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's imagine you have your own dataset with text/label pairs you'd like to create a sequence classification model for.</p>
<p>With the easy sequence classifier, you can take advantage of transfer learning by fine-tuning pre-trained models on your own custom datasets.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: The <a href="/adaptnlpsequence_classification.html#EasySequenceClassifier"><code>EasySequenceClassifier</code></a> is integrated heavily with the <code>datasets.Dataset</code> and <code>transformers.Trainer</code> class objects, so please check out the <a href="https://huggingface.co/datasets">datasets</a> and <a href="https://huggingface.co/transformers">transformers</a> documentation for more information.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll first need a "custom" dataset to start training our model. Our <a href="/adaptnlpsequence_classification.html#EasySequenceClassifier.train("><code>EasySequenceClassifier.train()</code></a>) method can run with either <code>datasets.Dataset</code> objects or CSV data file paths. Since the datasets library makes it so easy, we'll use the <code>datasets.load_dataset()</code> method to load in the IMDB Sentiment dataset. We'll show an example with a CSV later.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">train_dataset</span><span class="p">,</span> <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;imdb&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train[:1%]&#39;</span><span class="p">,</span> <span class="s1">&#39;test[:1%]&#39;</span><span class="p">])</span>

<span class="c1"># Uncomment below if you want to use all the data so you don&#39;t spend an hour+ on training and evaluation</span>
<span class="c1">#train_dataset, eval_dataset = load_dataset(&#39;imdb&#39;, split=[&#39;train&#39;, &#39;test&#39;])</span>

<span class="n">pprint</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">info</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;builder_name&#39;: &#39;imdb&#39;,
 &#39;citation&#39;: &#39;@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n&#39;
             &#39;  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  &#39;
             &#39;Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, &#39;
             &#39;Christopher},\n&#39;
             &#39;  title     = {Learning Word Vectors for Sentiment Analysis},\n&#39;
             &#39;  booktitle = {Proceedings of the 49th Annual Meeting of the &#39;
             &#39;Association for Computational Linguistics: Human Language &#39;
             &#39;Technologies},\n&#39;
             &#39;  month     = {June},\n&#39;
             &#39;  year      = {2011},\n&#39;
             &#39;  address   = {Portland, Oregon, USA},\n&#39;
             &#39;  publisher = {Association for Computational Linguistics},\n&#39;
             &#39;  pages     = {142--150},\n&#39;
             &#39;  url       = {http://www.aclweb.org/anthology/P11-1015}\n&#39;
             &#39;}\n&#39;,
 &#39;config_name&#39;: &#39;plain_text&#39;,
 &#39;dataset_size&#39;: 133190346,
 &#39;description&#39;: &#39;Large Movie Review Dataset.\n&#39;
                &#39;This is a dataset for binary sentiment classification &#39;
                &#39;containing substantially more data than previous benchmark &#39;
                &#39;datasets. We provide a set of 25,000 highly polar movie &#39;
                &#39;reviews for training, and 25,000 for testing. There is &#39;
                &#39;additional unlabeled data for use as well.&#39;,
 &#39;download_checksums&#39;: {&#39;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&#39;: {&#39;checksum&#39;: &#39;c40f74a18d3b61f90feba1e17730e0d38e8b97c05fde7008942e91923d1658fe&#39;,
                                                                                           &#39;num_bytes&#39;: 84125825}},
 &#39;download_size&#39;: 84125825,
 &#39;features&#39;: {&#39;label&#39;: ClassLabel(num_classes=2, names=[&#39;neg&#39;, &#39;pos&#39;], names_file=None, id=None),
              &#39;text&#39;: Value(dtype=&#39;string&#39;, id=None)},
 &#39;homepage&#39;: &#39;http://ai.stanford.edu/~amaas/data/sentiment/&#39;,
 &#39;license&#39;: &#39;&#39;,
 &#39;post_processed&#39;: None,
 &#39;post_processing_size&#39;: None,
 &#39;size_in_bytes&#39;: 217316171,
 &#39;splits&#39;: {&#39;test&#39;: SplitInfo(name=&#39;test&#39;, num_bytes=32650697, num_examples=25000, dataset_name=&#39;imdb&#39;),
            &#39;train&#39;: SplitInfo(name=&#39;train&#39;, num_bytes=33432835, num_examples=25000, dataset_name=&#39;imdb&#39;),
            &#39;unsupervised&#39;: SplitInfo(name=&#39;unsupervised&#39;, num_bytes=67106814, num_examples=50000, dataset_name=&#39;imdb&#39;)},
 &#39;supervised_keys&#39;: None,
 &#39;version&#39;: 1.0.0}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's take a brief look at what the IMDB Sentiment dataset looks like. We can see that the label column has two classes of 0 and 1. You can see the name of the classes mapped to the integers with <code>train_dataset.features["names"]</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s2">&quot;pandas&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">])</span>
<span class="n">train_dataset</span><span class="p">[:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Bromwell High is a cartoon comedy. It ran at t...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>Homelessness (or Houselessness as George Carli...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>This is easily the most underrated film inn th...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>This is not the typical Mel Brooks film. It wa...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>245</th>
      <td>1</td>
      <td>That hilarious line is typical of what these n...</td>
    </tr>
    <tr>
      <th>246</th>
      <td>1</td>
      <td>Faith and Mortality... viewed through the lens...</td>
    </tr>
    <tr>
      <th>247</th>
      <td>1</td>
      <td>The unlikely duo of Zero Mostel and Harry Bela...</td>
    </tr>
    <tr>
      <th>248</th>
      <td>1</td>
      <td>*some spoilers*&lt;br /&gt;&lt;br /&gt;I was pleasantly su...</td>
    </tr>
    <tr>
      <th>249</th>
      <td>1</td>
      <td>... and I DO mean it. If not literally (after ...</td>
    </tr>
  </tbody>
</table>
<p>250 rows × 2 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's reformat it back into a more "pythonic" dataset:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Uncomment below to see training done with CSV files. The cell below will just save the <code>datasets.Dataset</code> objects you have in <code>train_dataset</code> and <code>eval_dataset</code> as CSVs and will train the model with the CSV file paths. Ignore to just continue to training.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#eval_dataset.set_format(type=&quot;pandas&quot;, columns=[&quot;text&quot;, &quot;label&quot;])</span>

<span class="c1">#train_dataset[:].to_csv(&quot;./IMDB train.csv&quot;, index=False)</span>
<span class="c1">#eval_dataset[:].to_csv(&quot;./IMDB eval.csv&quot;, index=False)</span>

<span class="c1">#train_dataset = &quot;./IMDB train.csv&quot;</span>
<span class="c1">#eval_dataset = &quot;./IMDB eval.csv&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One of the first things we'll need to specify before we start training are the training arguments. Training arguments consist mainly of the hyperparameters we want to provide the model. These may include batch size, initial learning rate, number of epochs, etc.</p>
<p>We will be using the <code>transformers.TrainingArguments</code> data class to store our training args. These are compatible with the <code>transformers.Trainer</code> as well as AdaptNLP's train methods. For more documention on the <code>TrainingArguments</code> class, please look <a href="https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments">here</a>. There are a lot of arguments available, but we will pass in the important args and use default values for the rest.</p>
<p>The training arguments below specify the output directory for you model and checkpoints.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;./models&#39;</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span>
    <span class="n">logging_dir</span><span class="o">=</span><span class="s1">&#39;./logs&#39;</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can run the built-in <code>train()</code> method by passing in the training arguments. The training method will also be where you specify your data arguments which include the your train and eval datasets, the pre-trained model ID (this should have been loaded from your earlier cells, but can be loaded dynamically), text column name, label column name, and ordered label names (only required if loading in paths to CSV data file for dataset args).</p>
<p>Please checkout AdaptNLP's package reference for more information <a href="https://novetta.github.io/adaptnlp/class-api/sequence-classifier-module.html">here</a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">training_args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
                 <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
                 <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_dataset</span><span class="p">,</span>
                 <span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span><span class="p">,</span>
                 <span class="n">text_col_nm</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
                 <span class="n">label_col_nm</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
                 <span class="n">label_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;positive&quot;</span><span class="p">,</span><span class="s2">&quot;negative&quot;</span><span class="p">]</span>
                <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Evaluate-your-model">Evaluate your model<a class="anchor-link" href="#Evaluate-your-model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>After training, you can evaluate the model with the eval dataset you passed in for training.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
        </style>
      
      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [63/63 00:02]
    </div>
    
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;eval_loss&#39;: 0.017184646800160408,
 &#39;eval_accuracy&#39;: 1.0,
 &#39;eval_f1&#39;: array([1.]),
 &#39;eval_precision&#39;: array([1.]),
 &#39;eval_recall&#39;: array([1.]),
 &#39;eval_runtime&#39;: 2.7201,
 &#39;eval_samples_per_second&#39;: 91.91,
 &#39;epoch&#39;: 1.0}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now you can see it's a little weird that we're still using the <code>model_name_or_path</code> of the pre-trained model we fine-tuned and took advantage of via. transfer learning. We can release the model we've fine-tuned, and then load it back in using the directory that we've serialized the fine-tuned model.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">release_model</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">tag_text</span><span class="p">(</span>
    <span class="n">multiple_text</span><span class="p">,</span>
    <span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;./models&quot;</span><span class="p">,</span>
    <span class="n">mini_batch_size</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tag Score Outputs:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
    <span class="n">pprint</span><span class="p">({</span><span class="n">sentence</span><span class="o">.</span><span class="n">to_original_text</span><span class="p">():</span> <span class="n">sentence</span><span class="o">.</span><span class="n">labels</span><span class="p">})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>2021-04-20 19:43:56,203 loading file ./models
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tag Score Outputs:

{&#34;This didn&#39;t work well at all.&#34;: [neg (0.263), pos (0.737)]}
{&#39;I really liked it.&#39;: [neg (0.1309), pos (0.8691)]}
{&#39;It was really useful.&#39;: [neg (0.184), pos (0.816)]}
{&#39;It broke after I bought it.&#39;: [neg (0.2716), pos (0.7284)]}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we're done!</p>

</div>
</div>
</div>
</div>
 

