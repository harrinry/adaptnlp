---

title: Language Model Tuning


keywords: fastai
sidebar: home_sidebar

summary: "Data and Tuning API for Language Model Fine-Tuning"
description: "Data and Tuning API for Language Model Fine-Tuning"
nb_path: "nbs/14_training.language_model.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/14_training.language_model.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Datasets">Datasets<a class="anchor-link" href="#Datasets"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LanguageModelDatasets" class="doc_header"><code>class</code> <code>LanguageModelDatasets</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L45" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LanguageModelDatasets</code>(<strong><code>train_dset</code></strong>:<code>Dataset</code>, <strong><code>valid_dset</code></strong>:<code>Dataset</code>, <strong><code>tokenizer_name</code></strong>:<code>str</code>, <strong><code>tokenize</code></strong>:<code>bool</code>, <strong><code>tokenize_kwargs</code></strong>:<code>dict</code>, <strong><code>auto_kwargs</code></strong>:<code>dict</code>, <strong><code>remove_columns</code></strong>:<code>list</code>, <strong><code>block_size</code></strong>:<code>int</code>, <strong><code>masked_lm</code></strong>:<code>bool</code>) :: <a href="/adaptnlp/training.core.html#TaskDatasets"><code>TaskDatasets</code></a></p>
</blockquote>
<p>A set of datasets designed for language model fine-tuning</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>train_dset</code></strong> : <em><code>&lt;class 'datasets.arrow_dataset.Dataset'&gt;</code></em>  <p>A training dataset</p></li>
</ul>
<ul>
<li><strong><code>valid_dset</code></strong> : <em><code>&lt;class 'datasets.arrow_dataset.Dataset'&gt;</code></em>  <p>A validation dataset</p></li>
</ul>
<ul>
<li><strong><code>tokenizer_name</code></strong> : <em><code>&lt;class 'str'&gt;</code></em> <p>The name of a tokenizer</p></li>
</ul>
<ul>
<li><strong><code>tokenize</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>  <p>Whether to tokenize immediatly</p></li>
</ul>
<ul>
<li><strong><code>tokenize_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>   <p>kwargs for the tokenize function</p></li>
</ul>
<ul>
<li><strong><code>auto_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>   <p>AutoTokenizer.from_pretrained kwargs</p></li>
</ul>
<ul>
<li><strong><code>remove_columns</code></strong> : <em><code>&lt;class 'list'&gt;</code></em>    <p>The columns to remove when tokenizing</p></li>
</ul>
<ul>
<li><strong><code>block_size</code></strong> : <em><code>&lt;class 'int'&gt;</code></em> <p>The size of each block</p></li>
</ul>
<ul>
<li><strong><code>masked_lm</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em> <p>Whether the language model is a MLM</p></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LanguageModelDatasets.from_dfs" class="doc_header"><code>LanguageModelDatasets.from_dfs</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L74" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LanguageModelDatasets.from_dfs</code>(<strong><code>train_df</code></strong>:<code>DataFrame</code>, <strong><code>text_col</code></strong>:<code>str</code>, <strong><code>tokenizer_name</code></strong>:<code>str</code>, <strong><code>block_size</code></strong>:<code>int</code>=<em><code>128</code></em>, <strong><code>masked_lm</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>valid_df</code></strong>:<code>DataFrame</code>=<em><code>None</code></em>, <strong><code>split_func</code></strong>:<code>callable</code>=<em><code>None</code></em>, <strong><code>split_pct</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>tokenize_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>, <strong><code>auto_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>)</p>
</blockquote>
<p>Builds <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a> from a <code>DataFrame</code> or file path</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>train_df</code></strong> : <em><code>&lt;class 'pandas.core.frame.DataFrame'&gt;</code></em>   <p>A Pandas Dataframe</p></li>
</ul>
<ul>
<li><strong><code>text_col</code></strong> : <em><code>&lt;class 'str'&gt;</code></em>   <p>The name of the text column</p></li>
</ul>
<ul>
<li><strong><code>tokenizer_name</code></strong> : <em><code>&lt;class 'str'&gt;</code></em> <p>The name of the tokenizer</p></li>
</ul>
<ul>
<li><strong><code>block_size</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>The size of each block</p></li>
</ul>
<ul>
<li><strong><code>masked_lm</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em> <p>Whether the language model is a MLM</p></li>
</ul>
<ul>
<li><strong><code>valid_df</code></strong> : <em><code>&lt;class 'pandas.core.frame.DataFrame'&gt;</code></em>, <em>optional</em>   <p>An optional validation DataFrame</p></li>
</ul>
<ul>
<li><strong><code>split_func</code></strong> : <em><code>&lt;built-in function callable&gt;</code></em>, <em>optional</em>  <p>Optionally a splitting function similar to RandomSplitter</p></li>
</ul>
<ul>
<li><strong><code>split_pct</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em>    <p>What % to split the df between training and validation</p></li>
</ul>
<ul>
<li><strong><code>tokenize_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>kwargs for the tokenize function</p></li>
</ul>
<ul>
<li><strong><code>auto_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>kwargs for the AutoTokenizer.from_pretrained constructor</p></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LanguageModelDatasets.from_csvs" class="doc_header"><code>LanguageModelDatasets.from_csvs</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L123" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LanguageModelDatasets.from_csvs</code>(<strong><code>train_csv</code></strong>:<code>Path</code>, <strong><code>text_col</code></strong>:<code>str</code>, <strong><code>tokenizer_name</code></strong>:<code>str</code>, <strong><code>block_size</code></strong>:<code>int</code>=<em><code>128</code></em>, <strong><code>masked_lm</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>valid_csv</code></strong>:<code>Path</code>=<em><code>None</code></em>, <strong><code>split_func</code></strong>:<code>callable</code>=<em><code>None</code></em>, <strong><code>split_pct</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>tokenize_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>, <strong><code>auto_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Builds <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a> from a single csv or set of csvs. A convience constructor for <code>from_dfs</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>train_csv</code></strong> : <em><code>&lt;class 'pathlib.Path'&gt;</code></em> <p>A training csv file</p></li>
</ul>
<ul>
<li><strong><code>text_col</code></strong> : <em><code>&lt;class 'str'&gt;</code></em>   <p>The name of the text column</p></li>
</ul>
<ul>
<li><strong><code>tokenizer_name</code></strong> : <em><code>&lt;class 'str'&gt;</code></em> <p>The name of the tokenizer</p></li>
</ul>
<ul>
<li><strong><code>block_size</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>The size of each block</p></li>
</ul>
<ul>
<li><strong><code>masked_lm</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em> <p>Whether the language model is a MLM</p></li>
</ul>
<ul>
<li><strong><code>valid_csv</code></strong> : <em><code>&lt;class 'pathlib.Path'&gt;</code></em>, <em>optional</em> <p>An optional validation csv</p></li>
</ul>
<ul>
<li><strong><code>split_func</code></strong> : <em><code>&lt;built-in function callable&gt;</code></em>, <em>optional</em>  <p>Optionally a splitting function similar to RandomSplitter</p></li>
</ul>
<ul>
<li><strong><code>split_pct</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em>    <p>What % to split the df between training and validation</p></li>
</ul>
<ul>
<li><strong><code>tokenize_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>kwargs for the tokenize function</p></li>
</ul>
<ul>
<li><strong><code>auto_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>kwargs for the AutoTokenizer.from_pretrained constructor</p></li>
</ul>
<ul>
<li><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LanguageModelDatasets.from_folders" class="doc_header"><code>LanguageModelDatasets.from_folders</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L155" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LanguageModelDatasets.from_folders</code>(<strong><code>train_path</code></strong>:<code>Path</code>, <strong><code>tokenizer_name</code></strong>:<code>str</code>, <strong><code>block_size</code></strong>:<code>int</code>=<em><code>128</code></em>, <strong><code>masked_lm</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>valid_path</code></strong>:<code>Path</code>=<em><code>None</code></em>, <strong><code>split_func</code></strong>:<code>callable</code>=<em><code>None</code></em>, <strong><code>split_pct</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>tokenize_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>, <strong><code>auto_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>)</p>
</blockquote>
<p>Builds <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a> from a folder or group of folders</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>train_path</code></strong> : <em><code>&lt;class 'pathlib.Path'&gt;</code></em>    <p>The path to the training data</p></li>
</ul>
<ul>
<li><strong><code>tokenizer_name</code></strong> : <em><code>&lt;class 'str'&gt;</code></em> <p>The name of the tokenizer</p></li>
</ul>
<ul>
<li><strong><code>block_size</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>The size of each block</p></li>
</ul>
<ul>
<li><strong><code>masked_lm</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em> <p>Whether the language model is a MLM</p></li>
</ul>
<ul>
<li><strong><code>valid_path</code></strong> : <em><code>&lt;class 'pathlib.Path'&gt;</code></em>, <em>optional</em>    <p>An optional validation path</p></li>
</ul>
<ul>
<li><strong><code>split_func</code></strong> : <em><code>&lt;built-in function callable&gt;</code></em>, <em>optional</em>  <p>Optionally a splitting function similar to RandomSplitter</p></li>
</ul>
<ul>
<li><strong><code>split_pct</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em>    <p>What % to split the df between training and validation</p></li>
</ul>
<ul>
<li><strong><code>tokenize_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>kwargs for the tokenize function</p></li>
</ul>
<ul>
<li><strong><code>auto_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>kwargs for the AutoTokenizer.from_pretrained constructor</p></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LanguageModelDatasets.dataloaders" class="doc_header"><code>LanguageModelDatasets.dataloaders</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L202" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LanguageModelDatasets.dataloaders</code>(<strong><code>batch_size</code></strong>=<em><code>8</code></em>, <strong><code>shuffle_train</code></strong>=<em><code>True</code></em>, <strong><code>collate_fn</code></strong>=<em><code>default_data_collator</code></em>, <strong><code>mlm_probability</code></strong>:<code>float</code>=<em><code>0.15</code></em>, <strong><code>path</code></strong>=<em><code>'.'</code></em>, <strong><code>device</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Build DataLoaders from <code>self</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>batch_size</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>A batch size</p></li>
</ul>
<ul>
<li><strong><code>shuffle_train</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em> <p>Whether to shuffle the training dataset</p></li>
</ul>
<ul>
<li><strong><code>collate_fn</code></strong> : <em><code>&lt;class 'function'&gt;</code></em>, <em>optional</em>    <p>A custom collation function</p></li>
</ul>
<ul>
<li><strong><code>mlm_probability</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em>  <p>Token masking probablity for Masked Language Models</p></li>
</ul>
<ul>
<li><p><strong><code>path</code></strong> : <em><code>&lt;class 'str'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>device</code></strong> : <em><code>&lt;class 'NoneType'&gt;</code></em>, <em>optional</em></p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LanguageModelTuner" class="doc_header"><code>class</code> <code>LanguageModelTuner</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L228" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LanguageModelTuner</code>(<strong><code>dls</code></strong>:<code>DataLoaders</code>, <strong><code>model_name</code></strong>, <strong><code>tokenizer</code></strong>=<em><code>None</code></em>, <strong><code>language_model_type</code></strong>:<code>LMType</code>=<em><code>'causal'</code></em>, <strong><code>loss_func</code></strong>=<em><code>CrossEntropyLoss()</code></em>, <strong><code>metrics</code></strong>=<em><code>[&lt;fastai.metrics.Perplexity object at 0x7fb8363db520&gt;]</code></em>, <strong><code>opt_func</code></strong>=<em><code>Adam</code></em>, <strong><code>additional_cbs</code></strong>=<em><code>None</code></em>, <strong><code>expose_fastai_api</code></strong>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/adaptnlp/training.core.html#AdaptiveTuner"><code>AdaptiveTuner</code></a></p>
</blockquote>
<p>An <a href="/adaptnlp/training.core.html#AdaptiveTuner"><code>AdaptiveTuner</code></a> with good defaults for Language Model fine-tuning
<strong>Valid kwargs and defaults:</strong></p>
<ul>
<li><code>lr</code>:float = 0.001</li>
<li><code>splitter</code>:function = <code>trainable_params</code></li>
<li><code>cbs</code>:list = None</li>
<li><code>path</code>:Path = None</li>
<li><code>model_dir</code>:Path = 'models'</li>
<li><code>wd</code>:float = None</li>
<li><code>wd_bn_bias</code>:bool = False</li>
<li><code>train_bn</code>:bool = True</li>
<li><code>moms</code>: tuple(float) = (0.95, 0.85, 0.95)</li>
</ul>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>dls</code></strong> : <em><code>&lt;class 'fastai.data.core.DataLoaders'&gt;</code></em>   <p>A set of DataLoaders or AdaptiveDataLoaders</p></li>
</ul>
<ul>
<li><strong><code>model_name</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em>  <p>A HuggingFace model</p></li>
</ul>
<ul>
<li><strong><code>tokenizer</code></strong> : <em><code>&lt;class 'NoneType'&gt;</code></em>, <em>optional</em> <p>A HuggingFace tokenizer</p></li>
</ul>
<ul>
<li><strong><code>language_model_type</code></strong> : <em><code>&lt;class 'fastcore.basics.LMType'&gt;</code></em>, <em>optional</em> <p>The type of language model to use</p></li>
</ul>
<ul>
<li><strong><code>loss_func</code></strong> : <em><code>&lt;class 'fastai.losses.CrossEntropyLossFlat'&gt;</code></em>, <em>optional</em>   <p>A loss function</p></li>
</ul>
<ul>
<li><strong><code>metrics</code></strong> : <em><code>&lt;class 'list'&gt;</code></em>, <em>optional</em>   <p>Metrics to monitor the training with</p></li>
</ul>
<ul>
<li><strong><code>opt_func</code></strong> : <em><code>&lt;class 'function'&gt;</code></em>, <em>optional</em>  <p>A fastai or torch Optimizer</p></li>
</ul>
<ul>
<li><strong><code>additional_cbs</code></strong> : <em><code>&lt;class 'NoneType'&gt;</code></em>, <em>optional</em>    <p>Additional Callbacks to have always tied to the Tuner,</p></li>
</ul>
<ul>
<li><strong><code>expose_fastai_api</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em> <p>Whether to expose the fastai API</p></li>
</ul>
<ul>
<li><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LanguageModelTuner.predict" class="doc_header"><code>LanguageModelTuner.predict</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L293" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LanguageModelTuner.predict</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>str</code>], <code>str</code>], <strong><code>bs</code></strong>:<code>int</code>=<em><code>64</code></em>, <strong><code>num_tokens_to_produce</code></strong>:<code>int</code>=<em><code>50</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Predict some <code>text</code> for sequence classification with the currently loaded model</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>text</code></strong> : <em><code>typing.Union[typing.List[str], str]</code></em> <p>Some text or list of texts to do inference with</p></li>
</ul>
<ul>
<li><strong><code>bs</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>A batch size to use for multiple texts</p></li>
</ul>
<ul>
<li><strong><code>num_tokens_to_produce</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em>  <p>Number of tokens to generate</p></li>
</ul>
<ul>
<li><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

