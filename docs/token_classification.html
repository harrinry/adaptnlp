---

title: Token Tagging and Classification


keywords: fastai
sidebar: home_sidebar

summary: "AdaptiveModels for using Transformers and Flair for token tagging and classification"
description: "AdaptiveModels for using Transformers and Flair for token tagging and classification"
nb_path: "nbs/05_token_classification.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/05_token_classification.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TransformersTokenTagger" class="doc_header"><code>class</code> <code>TransformersTokenTagger</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/token_classification.py#L39" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TransformersTokenTagger</code>(<strong><code>tokenizer</code></strong>:<code>PreTrainedTokenizer</code>, <strong><code>model</code></strong>:<code>PreTrainedModel</code>) :: <a href="/adaptnlp/model.html#AdaptiveModel"><code>AdaptiveModel</code></a></p>
</blockquote>
<p>Adaptive model for Transformer's Token Tagger Model</p>
<p>Usage:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">tagger</span> <span class="o">=</span> <span class="n">TransformersTokenTagger</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tagger</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;Example text&quot;</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</pre></div>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong>tokenizer</strong> - A tokenizer object from Huggingface's transformers (TODO) and tokenizers</li>
<li><strong>model</strong> - A transformers token tagger model</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details>
<summary markdown="span">Click Me to Show Code Examples</summary><b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>
```python
#hide
tagger = TransformersTokenTagger.load("dbmdz/bert-large-cased-finetuned-conll03-english")
pred = tagger.predict(text='Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.', mini_batch_size=32)
baseline = [[{'entity_group': 'I-ORG',
   'score': 0.998292068640391,
   'word': 'Novetta Solutions',
   'offsets': (0, 3)},
  {'entity_group': 'I-PER',
   'score': 0.9985582232475281,
   'word': 'Albert Einstein',
   'offsets': (7, 9)},
  {'entity_group': 'I-ORG',
   'score': 0.9970489343007406,
   'word': 'Novetta Solutions',
   'offsets': (14, 17)},
  {'entity_group': 'I-PER',
   'score': 0.9961656928062439,
   'word': 'Wright',
   'offsets': (19, 20)},
  {'entity_group': 'I-ORG',
   'score': 0.9933501183986664,
   'word': 'JBF',
   'offsets': (25, 27)}]]

for base, p in zip(baseline, pred):
    for base_items, p_items in zip(base, p):
        test_eq(base_items['entity_group'], p_items['entity_group'])
        test_close(base_items['score'], p_items['score'], 1e-3)
        test_eq(base_items['word'], p_items['word'])
        test_eq(base_items['offsets'], p_items['offsets'])
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="TransformersTokenTagger.load" class="doc_header"><code>TransformersTokenTagger.load</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/token_classification.py#L62" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>TransformersTokenTagger.load</code>(<strong><code>model_name_or_path</code></strong>:<code>str</code>)</p>
</blockquote>
<p>Class method for loading and constructing this tagger</p>
<ul>
<li><strong>model_name_or_path</strong> - A key string of one of Transformer's pre-trained Token Tagger Model or a <a href="/adaptnlp/model_hub.html#HFModelResult"><code>HFModelResult</code></a></li>
</ul>
<p>Note: To search for valid models, you should use the AdaptNLP <a href="/adaptnlp/model_hub.html"><code>model_hub</code></a> API</p>
<p><strong>Usage Examples:</strong></p>
<details>
<summary markdown="span">Click Me to Show Code Examples</summary><b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>
```python
#hide
tagger = TransformersTokenTagger.load("dbmdz/bert-large-cased-finetuned-conll03-english")
pred = tagger.predict(text='Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.', mini_batch_size=32)
baseline = [[{'entity_group': 'I-ORG',
   'score': 0.998292068640391,
   'word': 'Novetta Solutions',
   'offsets': (0, 3)},
  {'entity_group': 'I-PER',
   'score': 0.9985582232475281,
   'word': 'Albert Einstein',
   'offsets': (7, 9)},
  {'entity_group': 'I-ORG',
   'score': 0.9970489343007406,
   'word': 'Novetta Solutions',
   'offsets': (14, 17)},
  {'entity_group': 'I-PER',
   'score': 0.9961656928062439,
   'word': 'Wright',
   'offsets': (19, 20)},
  {'entity_group': 'I-ORG',
   'score': 0.9933501183986664,
   'word': 'JBF',
   'offsets': (25, 27)}]]

for base, p in zip(baseline, pred):
    for base_items, p_items in zip(base, p):
        test_eq(base_items['entity_group'], p_items['entity_group'])
        test_close(base_items['score'], p_items['score'], 1e-3)
        test_eq(base_items['word'], p_items['word'])
        test_eq(base_items['offsets'], p_items['offsets'])
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="TransformersTokenTagger.predict" class="doc_header"><code>TransformersTokenTagger.predict</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/token_classification.py#L76" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>TransformersTokenTagger.predict</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>str</code>], <code>str</code>], <strong><code>mini_batch_size</code></strong>:<code>int</code>=<em><code>32</code></em>, <strong><code>grouped_entities</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Predict method for running inference using the pre-trained token tagger model.
Returns a list of lists of tagged entities.</p>
<ul>
<li><strong>text</strong> - String, list of strings, sentences, or list of sentences to run inference on</li>
<li><strong>mini_batch_size</strong> - Mini batch size</li>
<li><strong>grouped_entities</strong> - Set True to get whole entity span strings (Default True)</li>
<li><strong>&ast;&ast;kwargs</strong>(Optional) - Optional arguments for the Transformers tagger</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details>
<summary markdown="span">Click Me to Show Code Examples</summary><b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>
```python
#hide
tagger = TransformersTokenTagger.load("dbmdz/bert-large-cased-finetuned-conll03-english")
pred = tagger.predict(text='Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.', mini_batch_size=32)
baseline = [[{'entity_group': 'I-ORG',
   'score': 0.998292068640391,
   'word': 'Novetta Solutions',
   'offsets': (0, 3)},
  {'entity_group': 'I-PER',
   'score': 0.9985582232475281,
   'word': 'Albert Einstein',
   'offsets': (7, 9)},
  {'entity_group': 'I-ORG',
   'score': 0.9970489343007406,
   'word': 'Novetta Solutions',
   'offsets': (14, 17)},
  {'entity_group': 'I-PER',
   'score': 0.9961656928062439,
   'word': 'Wright',
   'offsets': (19, 20)},
  {'entity_group': 'I-ORG',
   'score': 0.9933501183986664,
   'word': 'JBF',
   'offsets': (25, 27)}]]

for base, p in zip(baseline, pred):
    for base_items, p_items in zip(base, p):
        test_eq(base_items['entity_group'], p_items['entity_group'])
        test_close(base_items['score'], p_items['score'], 1e-3)
        test_eq(base_items['word'], p_items['word'])
        test_eq(base_items['offsets'], p_items['offsets'])
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FlairTokenTagger" class="doc_header"><code>class</code> <code>FlairTokenTagger</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/token_classification.py#L258" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FlairTokenTagger</code>(<strong><code>model_name_or_path</code></strong>:<code>str</code>) :: <a href="/adaptnlp/model.html#AdaptiveModel"><code>AdaptiveModel</code></a></p>
</blockquote>
<p>Adaptive Model for Flair's Token Tagger...very basic</p>
<p>Usage:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">tagger</span> <span class="o">=</span> <span class="n">FlairTokenTagger</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;flair/chunk-english-fast&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tagger</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;Example text&quot;</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</pre></div>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong>model_name_or_path</strong> - A key string of one of Flair's pre-trained Token tagger Model</li>
</ul>
<p>To find a list of available models, see <a href="https://huggingface.co/models?filter=flair">here</a></p>
<p><strong>Usage Examples:</strong></p>
<details>
<summary markdown="span">Click Me to Show Code Examples</summary><b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>
```python
#hide
tagger = FlairTokenTagger.load("flair/chunk-english-fast")
preds = tagger.predict(text="Example text", mini_batch_size=32)[0]
test_eq(preds.tokens[0].text, 'Example')
test_eq(preds.tokens[1].text, 'text')
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="FlairTokenTagger.load" class="doc_header"><code>FlairTokenTagger.load</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/token_classification.py#L277" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>FlairTokenTagger.load</code>(<strong><code>model_name_or_path</code></strong>:<code>str</code>)</p>
</blockquote>
<p>Class method for loading a constructing this tagger</p>
<ul>
<li><strong>model_name_or_path</strong> - A key string of one of Flair's pre-trained Token tagger Model</li>
</ul>
<p>To find a list of available models, see <a href="https://huggingface.co/models?filter=flair">here</a></p>
<p><strong>Usage Examples:</strong></p>
<details>
<summary markdown="span">Click Me to Show Code Examples</summary><b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>
```python
#hide
tagger = FlairTokenTagger.load("flair/chunk-english-fast")
preds = tagger.predict(text="Example text", mini_batch_size=32)[0]
test_eq(preds.tokens[0].text, 'Example')
test_eq(preds.tokens[1].text, 'text')
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="FlairTokenTagger.predict" class="doc_header"><code>FlairTokenTagger.predict</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/token_classification.py#L288" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>FlairTokenTagger.predict</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>Sentence</code>], <code>Sentence</code>, <code>List</code>[<code>str</code>], <code>str</code>], <strong><code>mini_batch_size</code></strong>:<code>int</code>=<em><code>32</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Predict method for running inference using the pre-trained token tagger model</p>
<ul>
<li><strong>text</strong> - String, list of strings, sentences, or list of sentences to run inference on</li>
<li><strong>mini_batch_size</strong> - Mini batch size</li>
<li><strong>&ast;&ast;kwargs</strong>(Optional) - Optional arguments for the Flair tagger</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details>
<summary markdown="span">Click Me to Show Code Examples</summary><b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>
```python
#hide
tagger = FlairTokenTagger.load("flair/chunk-english-fast")
preds = tagger.predict(text="Example text", mini_batch_size=32)[0]
test_eq(preds.tokens[0].text, 'Example')
test_eq(preds.tokens[1].text, 'text')
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="EasyTokenTagger" class="doc_header"><code>class</code> <code>EasyTokenTagger</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/token_classification.py#L324" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>EasyTokenTagger</code>()</p>
</blockquote>
<p>Token level classification models</p>
<p>Usage:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">tagger</span> <span class="o">=</span> <span class="n">adaptnlp</span><span class="o">.</span><span class="n">EasyTokenTagger</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tagger</span><span class="o">.</span><span class="n">tag_text</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;text you want to tag&quot;</span><span class="p">,</span> <span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;ner-ontonotes&quot;</span><span class="p">)</span>
</pre></div>
<p><strong>Usage Examples:</strong></p>
<details>
<summary markdown="span">Click Me to Show Code Examples</summary><b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05a_tutorial.token_tagging.ipynb>Source</a></b>
```python
example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. 
The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.'''
tagger = EasyTokenTagger()
```
</details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05a_tutorial.token_tagging.ipynb>Source</a></b>
```python
tagger = EasyTokenTagger()
_ = tagger.tag_text(text=example_text, model_name_or_path="ner-ontonotes")
_ = tagger.tag_text(text=example_text, model_name_or_path="pos")
sentences = tagger.tag_all(text=example_text)
```
</details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>
```python
#hide
tagger = EasyTokenTagger()
example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. 
The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.'''
sentences = ["Jack walked through the park on a Sunday.", "Sunday was a nice and breezy afternoon.", "Jack was going to meet Jill for dinner."]
text = tagger.tag_text(text=example_text, model_name_or_path="ner-ontonotes")
text = tagger.tag_text(text=example_text, model_name_or_path="pos")
#hide
tags = tagger.tag_all(sentences)
_types = [["PERSON", "DATE"], ["DATE"], ["PERSON", "PERSON"]]

for sentence, lbls in zip(tags, _types):
    spans = sentence.get_spans()
    for span, lbl in zip(spans, lbls):
        test_eq(span.tag, lbl)
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="EasyTokenTagger.tag_text" class="doc_header"><code>EasyTokenTagger.tag_text</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/token_classification.py#L338" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>EasyTokenTagger.tag_text</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>Sentence</code>], <code>Sentence</code>, <code>List</code>[<code>str</code>], <code>str</code>], <strong><code>model_name_or_path</code></strong>:<code>Union</code>[<code>str</code>, <a href="/adaptnlp/model_hub.html#FlairModelResult"><code>FlairModelResult</code></a>, <a href="/adaptnlp/model_hub.html#HFModelResult"><code>HFModelResult</code></a>]=<em><code>'ner-ontonotes'</code></em>, <strong><code>mini_batch_size</code></strong>:<code>int</code>=<em><code>32</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Tags tokens with labels the token classification models have been trained on</p>
<ul>
<li><strong>text</strong> - Text input, it can be a string or any of Flair's <code>Sentence</code> input formats</li>
<li><strong>model_name_or_path</strong> - The hosted model name key or model path</li>
<li><strong>mini_batch_size</strong> - The mini batch size for running inference</li>
<li><strong>&ast;&ast;kwargs</strong> - Keyword arguments for Flair's <code>SequenceTagger.predict()</code> method
<strong>return</strong> - A list of Flair's <code>Sentence</code>'s</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details>
<summary markdown="span">Click Me to Show Code Examples</summary><b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05a_tutorial.token_tagging.ipynb>Source</a></b>
```python
tagger = EasyTokenTagger()
_ = tagger.tag_text(text=example_text, model_name_or_path="ner-ontonotes")
_ = tagger.tag_text(text=example_text, model_name_or_path="pos")
```
</details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>
```python
#hide
tagger = EasyTokenTagger()
example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. 
The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.'''
sentences = ["Jack walked through the park on a Sunday.", "Sunday was a nice and breezy afternoon.", "Jack was going to meet Jill for dinner."]
text = tagger.tag_text(text=example_text, model_name_or_path="ner-ontonotes")
text = tagger.tag_text(text=example_text, model_name_or_path="pos")
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="EasyTokenTagger.tag_all" class="doc_header"><code>EasyTokenTagger.tag_all</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/token_classification.py#L398" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>EasyTokenTagger.tag_all</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>Sentence</code>], <code>Sentence</code>, <code>List</code>[<code>str</code>], <code>str</code>], <strong><code>mini_batch_size</code></strong>:<code>int</code>=<em><code>32</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Tags tokens with all labels from all token classification models</p>
<ul>
<li><strong>text</strong> - Text input, it can be a string or any of Flair's <code>Sentence</code> input formats</li>
<li><strong>mini_batch_size</strong> - The mini batch size for running inference</li>
<li><strong>&ast;&ast;kwargs</strong> - Keyword arguments for Flair's <code>SequenceTagger.predict()</code> method
<strong>return</strong> A list of Flair's <code>Sentence</code>'s</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details>
<summary markdown="span">Click Me to Show Code Examples</summary><b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05a_tutorial.token_tagging.ipynb>Source</a></b>
```python
tagger = EasyTokenTagger()
_ = tagger.tag_text(text=example_text, model_name_or_path="ner-ontonotes")
_ = tagger.tag_text(text=example_text, model_name_or_path="pos")
sentences = tagger.tag_all(text=example_text)
```
</details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/05_token_classification.ipynb>Source</a></b>
```python
#hide
tagger = EasyTokenTagger()
example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. 
The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.'''
sentences = ["Jack walked through the park on a Sunday.", "Sunday was a nice and breezy afternoon.", "Jack was going to meet Jill for dinner."]
text = tagger.tag_text(text=example_text, model_name_or_path="ner-ontonotes")
text = tagger.tag_text(text=example_text, model_name_or_path="pos")
#hide
tags = tagger.tag_all(sentences)
_types = [["PERSON", "DATE"], ["DATE"], ["PERSON", "PERSON"]]

for sentence, lbls in zip(tags, _types):
    spans = sentence.get_spans()
    for span, lbl in zip(spans, lbls):
        test_eq(span.tag, lbl)
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

