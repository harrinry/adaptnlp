<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content=" fastai">
<title>Tutorial - Fine-Tuning a Language Model | adaptnlp</title>
<link rel="stylesheet" href="/adaptnlp/assets/css/syntax.css">

<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="assets//css/bootstrap.min.css">-->
<link rel="stylesheet" href="/adaptnlp/assets/css/modern-business.css">
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="/adaptnlp/assets/css/customstyles.css">
<link rel="stylesheet" href="/adaptnlp/assets/css/boxshadowproperties.css">
<!-- most color styles are extracted out to here -->
<link rel="stylesheet" href="/adaptnlp/assets/css/theme-blue.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="/adaptnlp/assets/js/jquery.navgoco.min.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
document.addEventListener("DOMContentLoaded", function() {
  renderMathInElement( document.body, {
    delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "[%", right: "%]", display: true},
      {left: "$", right: "$", display: false}
    ]}
  );
});
</script>


<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<!-- Anchor.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/2.0.0/anchor.min.js"></script>
<script src="/adaptnlp/assets/js/toc.js"></script>
<script src="/adaptnlp/assets/js/customscripts.js"></script>

<link rel="shortcut icon" href="/adaptnlp/assets/images/favicon.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="adaptnlp" href="http://localhost:4000/adaptnlp/feed.xml">

<!-- Twitter cards -->



<meta name="twitter:description" content="An easy to use Natural Language Processing library and framework for predicting, training, fine-tuning, and serving up state-of-the-art NLP models.">



<meta name="twitter:card"  content="summary">


<!-- end of Twitter cards -->





    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

            // activate menu items where href is matching to current page
            $("#mysidebar a[href='" + location.pathname.match(/(\/[^\/]*)$/)[1] + "']")
                .parents('li').addClass('active')
                .parents('ul').css('display', 'block');
        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    <script>
        $(document).ready(function() {
            $("#tg-sb-link").click(function() {
                $("#tg-sb-sidebar").toggle();
                $("#tg-sb-content").toggleClass('col-md-9');
                $("#tg-sb-content").toggleClass('col-md-12');
                $("#tg-sb-icon").toggleClass('fa-toggle-on');
                $("#tg-sb-icon").toggleClass('fa-toggle-off');
            });
        });
    </script>
    

</head>
<body>
<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-static-top">
    <div class="container topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="fa fa-home fa-lg navbar-brand" href="http://localhost:4000/adaptnlp">&nbsp;<span class="projectTitle">adaptnlp</span></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- toggle sidebar button -->
                <li><a id="tg-sb-link" href="#"><i id="tg-sb-icon" class="fa fa-toggle-on"></i> Nav</a></li>
                <!-- entries without drop-downs appear here -->




                
                
                
                <li><a href="https://github.com/novetta/adaptnlp/tree/master/" target="_blank">github</a></li>
                
                
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                
                
                
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>

<!-- Page Content -->
<div class="container">
  <div id="main">
    <!-- Content Row -->
    <div class="row">
        
        
            <!-- Sidebar Column -->
            <div class="col-md-3" id="tg-sb-sidebar">
                


<ul id="mysidebar" class="nav">
  <li class="sidebarTitle"> </li>
  
  
  
  <li>
      <a href="#">Getting Started</a>
      <ul>
          
          
          
          <li><a href="/adaptnlp/">Overview</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#">Models and Model Hubs</a>
      <ul>
          
          
          
          <li><a href="/adaptnlp/language_model.html">Language Models</a></li>
          
          
          
          
          
          
          <li><a href="/adaptnlp/model.html">Model</a></li>
          
          
          
          
          
          
          <li><a href="/adaptnlp/model_hub.html">The Model Hub</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#">Class API</a>
      <ul>
          
          
          
          <li><a href="/adaptnlp/">Overview</a></li>
          
          
          
          <li class="subfolders">
              <a href="#">NLP Tasks</a>
              <ul>
                  
                  
                  
                  <li><a href="/adaptnlp/embeddings.html">Embeddings</a></li>
                  
                  
                  
                  
                  
                  <li><a href="/adaptnlp/question_answering.html">Question Answering</a></li>
                  
                  
                  
                  
                  
                  <li><a href="/adaptnlp/sequence_classification.html">Sequence Classification</a></li>
                  
                  
                  
                  
                  
                  <li><a href="/adaptnlp/summarization.html">Summarization</a></li>
                  
                  
                  
                  
                  
                  <li><a href="/adaptnlp/text_generation.html">Text Generation</a></li>
                  
                  
                  
                  
                  
                  <li><a href="/adaptnlp/token_classification.html">Token Tagging and Classification</a></li>
                  
                  
                  
                  
                  
                  <li><a href="/adaptnlp/translation.html">Translation</a></li>
                  
                  
                  
              </ul>
          </li>
          
          
          
          <li class="subfolders">
              <a href="#">Training Framework</a>
              <ul>
                  
                  
                  
                  <li><a href="/adaptnlp/callback.html">Callbacks</a></li>
                  
                  
                  
                  
                  
                  <li><a href="/adaptnlp/training.html">Training</a></li>
                  
                  
                  
              </ul>
          </li>
          
          
          
          <li class="subfolders">
              <a href="#">Utilities</a>
              <ul>
                  
                  
                  
                  <li><a href="/adaptnlp/file_utils.html">File Utilities</a></li>
                  
                  
                  
                  
                  
                  <li><a href="/adaptnlp/transformers.finetuning.html">Transformers Fine-Tuning</a></li>
                  
                  
                  
                  
                  
                  <li><a href="/adaptnlp/transformers.squad_metrics.html">Transformers Squad Metrics</a></li>
                  
                  
                  
                  
                  
                  <li><a href="/adaptnlp/transformers.utils_squad_evaluate.html">Transformers Squad Evaluate</a></li>
                  
                  
                  
              </ul>
          </li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#">Tutorials - User Guide</a>
      <ul>
          
          
          
          <li><a href="/adaptnlp/tutorial-intro.md">Tutorial- Intro</a></li>
          
          
          
          
          
          
          <li><a href="/adaptnlp/tutorial.embeddings.html">Embeddings</a></li>
          
          
          
          
          
          
          <li><a href="/adaptnlp/tutorial.question_answering.html">Question Answering</a></li>
          
          
          
          
          
          
          <li><a href="/adaptnlp/tutorial.easy_sequence_classifier.html">Sequence Classification</a></li>
          
          
          
          
          
          
          <li><a href="/adaptnlp/tutorial.summarization.html">Summarization</a></li>
          
          
          
          
          
          
          <li><a href="/adaptnlp/tutorial.easy_text_generator.html">Text Generation</a></li>
          
          
          
          
          
          
          <li><a href="/adaptnlp/tutorial.token_tagging.html">Token Tagging</a></li>
          
          
          
          
          
          
          <li><a href="/adaptnlp/tutorial.translation.html">Translation</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#">Tutorials - Advanced</a>
      <ul>
          
          
          
          <li><a href="/adaptnlp/tutorial.fine_tuning_lm.html">Fine-Tuning a Language Model</a></li>
          
          
          
          
          
          
          <li><a href="/adaptnlp/tutorial.fine_tuning_manual.html">Fine Tune Transformers Models</a></li>
          
          
          
          
          
          
          <li><a href="/adaptnlp/tutorial.flair_seq_class_trainer.html">Training Sequence Classifier Head</a></li>
          
          
          
          
      </ul>
   </li>
     
      
      
      <!-- if you aren't using the accordion, uncomment this block:
         <p class="external">
             <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
         </p>
         -->
</ul>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>

            </div>
            
        

        <!-- Content Column -->
        <div class="col-md-9" id="tg-sb-content">
            <div class="post-header">
    <a id="Tutorial - Fine-Tuning a Language Model"></a>
    <h1 class="post-title-main">Tutorial - Fine-Tuning a Language Model</h1>
    
        <div class="px-2">
    <a href="https://colab.research.google.com/github/muellerzr/adaptnlp/blob/master/nbs/20a_tutorial.fine_tuning_lm.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/adaptnlp/assets/images/colab.svg" alt="Open In Colab"/>
    </a>
</div>

    </div>



<div class="post-content">

   

    
    
<!-- this handles the automatic toc. use ## for subheads to auto-generate the on-page minitoc. if you use html tags, you must supply an ID for the heading element in order for it to appear in the minitoc. -->
<script>
$( document ).ready(function() {
  // Handler for .ready() called.

$('#toc').toc({ minimumHeaders: 0, listType: 'ul', showSpeed: 0, headers: 'h2,h3,h4' });

/* this offset helps account for the space taken up by the floating toolbar. */
$('#toc').on('click', 'a', function() {
  var target = $(this.getAttribute('href'))
    , scroll_target = target.offset().top

  $(window).scrollTop(scroll_target - 10);
  return false
})
  
});
</script>

<div id="toc"></div>

    


    

   <!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/20a_tutorial.fine_tuning_lm.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-a-language-model?">What is a language model?<a class="anchor-link" href="#What-is-a-language-model?"> </a></h2><p>Language modeling is the task of generating a probability distribution over a sequence of words. The language models that we are using can assign the probabilitiy of an upcoming word(s) given a sequence of words. The GPT2 language model is a good example of a <em>Causal Language Model</em> which can predict words following a sequence of words. This predicted word can then be used along the given sequence of words to predict another word and so on. This is how we actually a variant of how we produce models for the NLP task of text generation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Why-would-you-want-to-fine-tune-a-language-model?">Why would you want to fine-tune a language model?<a class="anchor-link" href="#Why-would-you-want-to-fine-tune-a-language-model?"> </a></h2><p>Fine-tuning a language model comes in handy when data of a target task comes from a different distribution compared to the general-domain data that was used for pretraining a language model.</p>
<p>When fine-tuning the language model on data from a target task, the general-domain pretrained model is able to converge
quickly and adapt to the idiosyncrasies of the target data.  This can be seen from the efforts of ULMFiT and Jeremy
Howard's and Sebastian Ruder's approach on NLP transfer learning.</p>
<p>With AdaptNLP's <a href="/adaptnlplanguage_model.html#LMFineTuner"><code>LMFineTuner</code></a>, we can start to fine-tune state-of-the-art pretrained transformers architecture 
language models provided by Hugging Face's Transformers library. <a href="/adaptnlplanguage_model.html#LMFineTuner"><code>LMFineTuner</code></a> is built on <code>transformers.Trainer</code> so additional documentation on it can be found at Hugging Face's documentation <a href="https://huggingface.co/transformers/master/main_classes/trainer.html">here</a></p>
<p>Below are the available transformers language models for fine-tuning with <a href="/adaptnlplanguage_model.html#LMFineTuner"><code>LMFineTuner</code></a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<table>
<thead><tr>
<th>Transformer Model</th>
<th>Model Type/Architecture String Key</th>
</tr>
</thead>
<tbody>
<tr>
<td>ALBERT</td>
<td>"albert"</td>
</tr>
<tr>
<td>DistilBERT</td>
<td>"distilbert"</td>
</tr>
<tr>
<td>BERT</td>
<td>"bert"</td>
</tr>
<tr>
<td>CamemBERT</td>
<td>"camembert"</td>
</tr>
<tr>
<td>RoBERTa</td>
<td>"roberta"</td>
</tr>
<tr>
<td>GPT</td>
<td>"gpt"</td>
</tr>
<tr>
<td>GPT2</td>
<td>"gpt2"</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can fine-tune on any transformers language models with the above architecture in Huggingface's Transformers
library.  Key shortcut names are located <a href="https://huggingface.co/transformers/pretrained_models.html">here</a>.</p>
<p>The same goes for Huggingface's public model-sharing repository, which is available <a href="https://huggingface.co/models">here</a>
as of v2.2.2 of the Transformers library.</p>
<p>This tutorial will go over the following simple-to-use componenets of using the <a href="/adaptnlplanguage_model.html#LMFineTuner"><code>LMFineTuner</code></a> to fine-tune pre-trained language models on your custom text data.</p>
<ol>
<li>Data loading and training arguments</li>
<li>Language model training</li>
<li>Language model evaluation</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.-Data-loading-and-training-arguments">1. Data loading and training arguments<a class="anchor-link" href="#1.-Data-loading-and-training-arguments"> </a></h2><p>We'll first start by downloading some example raw text files. If you want to fine-tune a model on your own custom data, just provide the file paths to the training and evaluation text files that contain text from your target task. You don't require a lot of formatting with the data since a language model does not necessarily require "labeled" data. All you need is the text you'd like use to "expand" the domain of knowledge that your language model is training on.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip
<span class="o">!</span>unzip wikitext-2-raw-v1.zip

<span class="n">train_file</span> <span class="o">=</span> <span class="s2">&quot;./wikitext-2-raw/wiki.train.raw&quot;</span>
<span class="n">eval_file</span> <span class="o">=</span> <span class="s2">&quot;./wikitext-2-raw/wiki.test.raw&quot;</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--2020-08-31 15:38:50--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip
Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.64.78
Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.64.78|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4721645 (4.5M) [application/zip]
Saving to: ‘wikitext-2-raw-v1.zip’

wikitext-2-raw-v1.z 100%[===================&gt;]   4.50M  2.92MB/s    in 1.5s    

2020-08-31 15:38:52 (2.92 MB/s) - ‘wikitext-2-raw-v1.zip’ saved [4721645/4721645]

Archive:  wikitext-2-raw-v1.zip
   creating: wikitext-2-raw/
  inflating: wikitext-2-raw/wiki.test.raw  
  inflating: wikitext-2-raw/wiki.valid.raw  
  inflating: wikitext-2-raw/wiki.train.raw  
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have the text data we want to fine-tune our language model on, we can move on to configuring the training component.</p>
<p>One of the first things we'll need to specify before we start training are the training arguments. Training arguments consist mainly of the hyperparameters we want to provide the model. These may include batch size, initial learning rate, number of epochs, etc.</p>
<p>We will be using the <code>transformers.TrainingArguments</code> data class to store our training args. These are compatible with the <code>transformers.Trainer</code> as well as AdaptNLP's train methods. For more documention on the <code>TrainingArguments</code> class, please look <a href="https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments">here</a>. There are a lot of arguments available, but we will pass in the important args and use default values for the rest.</p>
<p>The training arguments below specify the output directory for you model and checkpoints.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;./models&#39;</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">evaluate_during_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">logging_dir</span><span class="o">=</span><span class="s1">&#39;./logs&#39;</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">2500</span><span class="p">,</span>
    <span class="n">eval_steps</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Language-model-training">2. Language model training<a class="anchor-link" href="#2.-Language-model-training"> </a></h2><p>Now that we have our data and training arguments, let's instantiate the <a href="/adaptnlplanguage_model.html#LMFineTuner"><code>LMFineTuner</code></a> and load in a pre-trained language model we would like to fine-tune. In this case, we will use the <code>gpt2</code> pre-trained language model.</p>
<p>Note: You can load in any model with the allowable architecture that we've specified above. You can even load in custom pre-trained models or models that you find in the Hugging Face repository that have already been fine-tuned and trained on NLP target tasks.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">LMFineTuner</span>

<span class="n">finetuner</span> <span class="o">=</span> <span class="n">LMFineTuner</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/andrew/Documents/github/adaptnlp/venv-adaptnlp/lib/python3.6/site-packages/transformers/modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.
  FutureWarning,
Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: [&#39;h.0.attn.masked_bias&#39;, &#39;h.1.attn.masked_bias&#39;, &#39;h.2.attn.masked_bias&#39;, &#39;h.3.attn.masked_bias&#39;, &#39;h.4.attn.masked_bias&#39;, &#39;h.5.attn.masked_bias&#39;, &#39;h.6.attn.masked_bias&#39;, &#39;h.7.attn.masked_bias&#39;, &#39;h.8.attn.masked_bias&#39;, &#39;h.9.attn.masked_bias&#39;, &#39;h.10.attn.masked_bias&#39;, &#39;h.11.attn.masked_bias&#39;, &#39;lm_head.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can run the built-in <code>train()</code> method by passing in the training arguments. The training method will also be where you specify your data arguments which include the your train and eval datasets, the pre-trained model ID (this should have been loaded from your earlier cells, but can be loaded dynamically), text column name, label column name, and ordered label names (only required if loading in paths to CSV data file for dataset args).</p>
<p>Notice how we pass the <code>mlm</code> argument as False? The mlm argument should be true if we are using a masked language model variant such as BERT architecture language models. More information can be found on Hugging Face's documentation <a href="https://huggingface.co/transformers/master/task_summary.html#masked-language-modeling">here</a></p>
<p>Please checkout AdaptNLP's package reference for more information <a href="https://novetta.github.io/adaptnlp/class-api/language-model-module.html">here</a>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">finetuner</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">training_args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_file</span><span class="o">=</span><span class="n">eval_file</span><span class="p">,</span>
    <span class="n">eval_file</span><span class="o">=</span><span class="n">eval_file</span><span class="p">,</span>
    <span class="n">mlm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">overwrite_cache</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>08/31/2020 15:45:44 - INFO - transformers.training_args -   PyTorch: setting up devices
08/31/2020 15:45:44 - WARNING - adaptnlp.language_model -   Process rank: -1,
                device: cuda:0,
                n_gpu: 1,
                distributed training: False,
                16-bits training: False
            
08/31/2020 15:45:44 - INFO - adaptnlp.language_model -   Training/evaluation parameters: {
  &#34;output_dir&#34;: &#34;./models&#34;,
  &#34;overwrite_output_dir&#34;: false,
  &#34;do_train&#34;: false,
  &#34;do_eval&#34;: false,
  &#34;do_predict&#34;: false,
  &#34;evaluate_during_training&#34;: false,
  &#34;per_device_train_batch_size&#34;: 1,
  &#34;per_device_eval_batch_size&#34;: 1,
  &#34;per_gpu_train_batch_size&#34;: null,
  &#34;per_gpu_eval_batch_size&#34;: null,
  &#34;gradient_accumulation_steps&#34;: 1,
  &#34;learning_rate&#34;: 5e-05,
  &#34;weight_decay&#34;: 0.01,
  &#34;adam_epsilon&#34;: 1e-08,
  &#34;max_grad_norm&#34;: 1.0,
  &#34;num_train_epochs&#34;: 1,
  &#34;max_steps&#34;: -1,
  &#34;warmup_steps&#34;: 500,
  &#34;logging_dir&#34;: &#34;./logs&#34;,
  &#34;logging_first_step&#34;: false,
  &#34;logging_steps&#34;: 500,
  &#34;save_steps&#34;: 2500,
  &#34;save_total_limit&#34;: null,
  &#34;no_cuda&#34;: false,
  &#34;seed&#34;: 42,
  &#34;fp16&#34;: false,
  &#34;fp16_opt_level&#34;: &#34;O1&#34;,
  &#34;local_rank&#34;: -1,
  &#34;tpu_num_cores&#34;: null,
  &#34;tpu_metrics_debug&#34;: false,
  &#34;debug&#34;: false,
  &#34;dataloader_drop_last&#34;: false,
  &#34;eval_steps&#34;: 100,
  &#34;past_index&#34;: -1
}
08/31/2020 15:45:44 - INFO - filelock -   Lock 139826145788648 acquired on ./wikitext-2-raw/cached_lm_GPT2TokenizerFast_1024_wiki.test.raw.lock
08/31/2020 15:45:44 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ./wikitext-2-raw
08/31/2020 15:45:45 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file ./wikitext-2-raw/cached_lm_GPT2TokenizerFast_1024_wiki.test.raw [took 0.004 s]
08/31/2020 15:45:45 - INFO - filelock -   Lock 139826145788648 released on ./wikitext-2-raw/cached_lm_GPT2TokenizerFast_1024_wiki.test.raw.lock
08/31/2020 15:45:45 - INFO - filelock -   Lock 139826145788312 acquired on ./wikitext-2-raw/cached_lm_GPT2TokenizerFast_1024_wiki.test.raw.lock
08/31/2020 15:45:45 - INFO - transformers.data.datasets.language_modeling -   Loading features from cached file ./wikitext-2-raw/cached_lm_GPT2TokenizerFast_1024_wiki.test.raw [took 0.006 s]
08/31/2020 15:45:45 - INFO - filelock -   Lock 139826145788312 released on ./wikitext-2-raw/cached_lm_GPT2TokenizerFast_1024_wiki.test.raw.lock
08/31/2020 15:45:45 - INFO - transformers.trainer -   You are instantiating a Trainer but W&amp;B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
08/31/2020 15:45:45 - INFO - transformers.trainer -   ***** Running training *****
08/31/2020 15:45:45 - INFO - transformers.trainer -     Num examples = 279
08/31/2020 15:45:45 - INFO - transformers.trainer -     Num Epochs = 1
08/31/2020 15:45:45 - INFO - transformers.trainer -     Instantaneous batch size per device = 1
08/31/2020 15:45:45 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed &amp; accumulation) = 1
08/31/2020 15:45:45 - INFO - transformers.trainer -     Gradient Accumulation steps = 1
08/31/2020 15:45:45 - INFO - transformers.trainer -     Total optimization steps = 279
Epoch:   0%|          | 0/1 [00:00&lt;?, ?it/s]
Iteration:   0%|          | 0/279 [00:00&lt;?, ?it/s]
Iteration:   0%|          | 1/279 [00:00&lt;01:07,  4.11it/s]
Iteration:   1%|          | 2/279 [00:00&lt;01:06,  4.17it/s]
Iteration:   1%|          | 3/279 [00:00&lt;01:05,  4.22it/s]
Iteration:   1%|▏         | 4/279 [00:00&lt;01:04,  4.26it/s]
Iteration:   2%|▏         | 5/279 [00:01&lt;01:03,  4.29it/s]
Iteration:   2%|▏         | 6/279 [00:01&lt;01:03,  4.31it/s]
Iteration:   3%|▎         | 7/279 [00:01&lt;01:02,  4.32it/s]
Iteration:   3%|▎         | 8/279 [00:01&lt;01:02,  4.34it/s]
Iteration:   3%|▎         | 9/279 [00:02&lt;01:02,  4.34it/s]
Iteration:   4%|▎         | 10/279 [00:02&lt;01:01,  4.35it/s]
Iteration:   4%|▍         | 11/279 [00:02&lt;01:01,  4.34it/s]
Iteration:   4%|▍         | 12/279 [00:02&lt;01:01,  4.35it/s]
Iteration:   5%|▍         | 13/279 [00:03&lt;01:01,  4.36it/s]
Iteration:   5%|▌         | 14/279 [00:03&lt;01:00,  4.35it/s]
Iteration:   5%|▌         | 15/279 [00:03&lt;01:00,  4.36it/s]
Iteration:   6%|▌         | 16/279 [00:03&lt;01:00,  4.33it/s]
Iteration:   6%|▌         | 17/279 [00:03&lt;01:00,  4.36it/s]
Iteration:   6%|▋         | 18/279 [00:04&lt;00:59,  4.36it/s]
Iteration:   7%|▋         | 19/279 [00:04&lt;00:59,  4.35it/s]
Iteration:   7%|▋         | 20/279 [00:04&lt;00:59,  4.35it/s]
Iteration:   8%|▊         | 21/279 [00:04&lt;00:59,  4.34it/s]
Iteration:   8%|▊         | 22/279 [00:05&lt;00:59,  4.33it/s]
Iteration:   8%|▊         | 23/279 [00:05&lt;00:59,  4.33it/s]
Iteration:   9%|▊         | 24/279 [00:05&lt;00:58,  4.34it/s]
Iteration:   9%|▉         | 25/279 [00:05&lt;00:58,  4.34it/s]
Iteration:   9%|▉         | 26/279 [00:05&lt;00:58,  4.35it/s]
Iteration:  10%|▉         | 27/279 [00:06&lt;00:57,  4.35it/s]
Iteration:  10%|█         | 28/279 [00:06&lt;00:57,  4.34it/s]
Iteration:  10%|█         | 29/279 [00:06&lt;00:57,  4.35it/s]
Iteration:  11%|█         | 30/279 [00:06&lt;00:57,  4.34it/s]
Iteration:  11%|█         | 31/279 [00:07&lt;00:57,  4.35it/s]
Iteration:  11%|█▏        | 32/279 [00:07&lt;00:56,  4.36it/s]
Iteration:  12%|█▏        | 33/279 [00:07&lt;00:56,  4.35it/s]
Iteration:  12%|█▏        | 34/279 [00:07&lt;00:56,  4.36it/s]
Iteration:  13%|█▎        | 35/279 [00:08&lt;00:56,  4.35it/s]
Iteration:  13%|█▎        | 36/279 [00:08&lt;00:55,  4.35it/s]
Iteration:  13%|█▎        | 37/279 [00:08&lt;00:55,  4.35it/s]
Iteration:  14%|█▎        | 38/279 [00:08&lt;00:55,  4.34it/s]
Iteration:  14%|█▍        | 39/279 [00:08&lt;00:55,  4.35it/s]
Iteration:  14%|█▍        | 40/279 [00:09&lt;01:02,  3.80it/s]
Iteration:  15%|█▍        | 41/279 [00:09&lt;01:00,  3.97it/s]
Iteration:  15%|█▌        | 42/279 [00:09&lt;00:58,  4.07it/s]
Iteration:  15%|█▌        | 43/279 [00:10&lt;00:56,  4.15it/s]
Iteration:  16%|█▌        | 44/279 [00:10&lt;00:55,  4.22it/s]
Iteration:  16%|█▌        | 45/279 [00:10&lt;00:55,  4.25it/s]
Iteration:  16%|█▋        | 46/279 [00:10&lt;00:54,  4.28it/s]
Iteration:  17%|█▋        | 47/279 [00:10&lt;00:54,  4.29it/s]
Iteration:  17%|█▋        | 48/279 [00:11&lt;00:53,  4.31it/s]
Iteration:  18%|█▊        | 49/279 [00:11&lt;00:53,  4.33it/s]
Iteration:  18%|█▊        | 50/279 [00:11&lt;00:52,  4.33it/s]
Iteration:  18%|█▊        | 51/279 [00:11&lt;00:52,  4.33it/s]
Iteration:  19%|█▊        | 52/279 [00:12&lt;00:52,  4.33it/s]
Iteration:  19%|█▉        | 53/279 [00:12&lt;00:52,  4.33it/s]
Iteration:  19%|█▉        | 54/279 [00:12&lt;00:51,  4.34it/s]
Iteration:  20%|█▉        | 55/279 [00:12&lt;00:51,  4.32it/s]
Iteration:  20%|██        | 56/279 [00:13&lt;00:51,  4.35it/s]
Iteration:  20%|██        | 57/279 [00:13&lt;00:51,  4.35it/s]
Iteration:  21%|██        | 58/279 [00:13&lt;00:50,  4.35it/s]
Iteration:  21%|██        | 59/279 [00:13&lt;00:50,  4.35it/s]
Iteration:  22%|██▏       | 60/279 [00:13&lt;00:50,  4.35it/s]
Iteration:  22%|██▏       | 61/279 [00:14&lt;00:50,  4.34it/s]
Iteration:  22%|██▏       | 62/279 [00:14&lt;00:49,  4.35it/s]
Iteration:  23%|██▎       | 63/279 [00:14&lt;00:49,  4.35it/s]
Iteration:  23%|██▎       | 64/279 [00:14&lt;00:49,  4.34it/s]
Iteration:  23%|██▎       | 65/279 [00:15&lt;00:49,  4.35it/s]
Iteration:  24%|██▎       | 66/279 [00:15&lt;00:49,  4.34it/s]
Iteration:  24%|██▍       | 67/279 [00:15&lt;00:48,  4.35it/s]
Iteration:  24%|██▍       | 68/279 [00:15&lt;00:48,  4.35it/s]
Iteration:  25%|██▍       | 69/279 [00:15&lt;00:48,  4.34it/s]
Iteration:  25%|██▌       | 70/279 [00:16&lt;00:48,  4.34it/s]
Iteration:  25%|██▌       | 71/279 [00:16&lt;00:47,  4.34it/s]
Iteration:  26%|██▌       | 72/279 [00:16&lt;00:47,  4.34it/s]
Iteration:  26%|██▌       | 73/279 [00:16&lt;00:47,  4.34it/s]
Iteration:  27%|██▋       | 74/279 [00:17&lt;00:47,  4.34it/s]
Iteration:  27%|██▋       | 75/279 [00:17&lt;00:46,  4.34it/s]
Iteration:  27%|██▋       | 76/279 [00:17&lt;00:46,  4.35it/s]
Iteration:  28%|██▊       | 77/279 [00:17&lt;00:46,  4.34it/s]
Iteration:  28%|██▊       | 78/279 [00:18&lt;00:46,  4.34it/s]
Iteration:  28%|██▊       | 79/279 [00:18&lt;00:46,  4.34it/s]
Iteration:  29%|██▊       | 80/279 [00:18&lt;00:45,  4.33it/s]
Iteration:  29%|██▉       | 81/279 [00:18&lt;00:45,  4.34it/s]
Iteration:  29%|██▉       | 82/279 [00:18&lt;00:45,  4.33it/s]
Iteration:  30%|██▉       | 83/279 [00:19&lt;00:45,  4.34it/s]
Iteration:  30%|███       | 84/279 [00:19&lt;00:44,  4.34it/s]
Iteration:  30%|███       | 85/279 [00:19&lt;00:44,  4.33it/s]
Iteration:  31%|███       | 86/279 [00:19&lt;00:44,  4.33it/s]
Iteration:  31%|███       | 87/279 [00:20&lt;00:44,  4.33it/s]
Iteration:  32%|███▏      | 88/279 [00:20&lt;00:44,  4.32it/s]
Iteration:  32%|███▏      | 89/279 [00:20&lt;00:43,  4.32it/s]
Iteration:  32%|███▏      | 90/279 [00:20&lt;00:43,  4.33it/s]
Iteration:  33%|███▎      | 91/279 [00:21&lt;00:43,  4.32it/s]
Iteration:  33%|███▎      | 92/279 [00:21&lt;00:43,  4.33it/s]
Iteration:  33%|███▎      | 93/279 [00:21&lt;00:42,  4.33it/s]
Iteration:  34%|███▎      | 94/279 [00:21&lt;00:42,  4.33it/s]
Iteration:  34%|███▍      | 95/279 [00:21&lt;00:42,  4.34it/s]
Iteration:  34%|███▍      | 96/279 [00:22&lt;00:42,  4.33it/s]
Iteration:  35%|███▍      | 97/279 [00:22&lt;00:41,  4.34it/s]
Iteration:  35%|███▌      | 98/279 [00:22&lt;00:41,  4.33it/s]
Iteration:  35%|███▌      | 99/279 [00:22&lt;00:41,  4.33it/s]
Iteration:  36%|███▌      | 100/279 [00:23&lt;00:41,  4.32it/s]
Iteration:  36%|███▌      | 101/279 [00:23&lt;00:41,  4.32it/s]
Iteration:  37%|███▋      | 102/279 [00:23&lt;00:40,  4.32it/s]
Iteration:  37%|███▋      | 103/279 [00:23&lt;00:40,  4.34it/s]
Iteration:  37%|███▋      | 104/279 [00:24&lt;00:40,  4.33it/s]
Iteration:  38%|███▊      | 105/279 [00:24&lt;00:40,  4.33it/s]
Iteration:  38%|███▊      | 106/279 [00:24&lt;00:39,  4.33it/s]
Iteration:  38%|███▊      | 107/279 [00:24&lt;00:39,  4.32it/s]
Iteration:  39%|███▊      | 108/279 [00:24&lt;00:39,  4.33it/s]
Iteration:  39%|███▉      | 109/279 [00:25&lt;00:39,  4.33it/s]
Iteration:  39%|███▉      | 110/279 [00:25&lt;00:39,  4.33it/s]
Iteration:  40%|███▉      | 111/279 [00:25&lt;00:38,  4.33it/s]
Iteration:  40%|████      | 112/279 [00:25&lt;00:38,  4.33it/s]
Iteration:  41%|████      | 113/279 [00:26&lt;00:38,  4.33it/s]
Iteration:  41%|████      | 114/279 [00:26&lt;00:38,  4.33it/s]
Iteration:  41%|████      | 115/279 [00:26&lt;00:37,  4.32it/s]
Iteration:  42%|████▏     | 116/279 [00:26&lt;00:37,  4.33it/s]
Iteration:  42%|████▏     | 117/279 [00:27&lt;00:37,  4.32it/s]
Iteration:  42%|████▏     | 118/279 [00:27&lt;00:37,  4.32it/s]
Iteration:  43%|████▎     | 119/279 [00:27&lt;00:37,  4.32it/s]
Iteration:  43%|████▎     | 120/279 [00:27&lt;00:36,  4.32it/s]
Iteration:  43%|████▎     | 121/279 [00:28&lt;00:36,  4.32it/s]
Iteration:  44%|████▎     | 122/279 [00:28&lt;00:36,  4.33it/s]
Iteration:  44%|████▍     | 123/279 [00:28&lt;00:36,  4.32it/s]
Iteration:  44%|████▍     | 124/279 [00:28&lt;00:35,  4.33it/s]
Iteration:  45%|████▍     | 125/279 [00:28&lt;00:35,  4.33it/s]
Iteration:  45%|████▌     | 126/279 [00:29&lt;00:35,  4.34it/s]
Iteration:  46%|████▌     | 127/279 [00:29&lt;00:35,  4.34it/s]
Iteration:  46%|████▌     | 128/279 [00:29&lt;00:34,  4.32it/s]
Iteration:  46%|████▌     | 129/279 [00:29&lt;00:34,  4.33it/s]
Iteration:  47%|████▋     | 130/279 [00:30&lt;00:34,  4.32it/s]
Iteration:  47%|████▋     | 131/279 [00:30&lt;00:34,  4.32it/s]
Iteration:  47%|████▋     | 132/279 [00:30&lt;00:33,  4.33it/s]
Iteration:  48%|████▊     | 133/279 [00:30&lt;00:33,  4.33it/s]
Iteration:  48%|████▊     | 134/279 [00:31&lt;00:33,  4.33it/s]
Iteration:  48%|████▊     | 135/279 [00:31&lt;00:33,  4.34it/s]
Iteration:  49%|████▊     | 136/279 [00:31&lt;00:33,  4.33it/s]
Iteration:  49%|████▉     | 137/279 [00:31&lt;00:32,  4.33it/s]
Iteration:  49%|████▉     | 138/279 [00:31&lt;00:32,  4.32it/s]
Iteration:  50%|████▉     | 139/279 [00:32&lt;00:32,  4.32it/s]
Iteration:  50%|█████     | 140/279 [00:32&lt;00:32,  4.33it/s]
Iteration:  51%|█████     | 141/279 [00:32&lt;00:31,  4.33it/s]
Iteration:  51%|█████     | 142/279 [00:32&lt;00:31,  4.32it/s]
Iteration:  51%|█████▏    | 143/279 [00:33&lt;00:31,  4.32it/s]
Iteration:  52%|█████▏    | 144/279 [00:33&lt;00:31,  4.31it/s]
Iteration:  52%|█████▏    | 145/279 [00:33&lt;00:30,  4.33it/s]
Iteration:  52%|█████▏    | 146/279 [00:33&lt;00:30,  4.33it/s]
Iteration:  53%|█████▎    | 147/279 [00:34&lt;00:30,  4.33it/s]
Iteration:  53%|█████▎    | 148/279 [00:34&lt;00:30,  4.33it/s]
Iteration:  53%|█████▎    | 149/279 [00:34&lt;00:30,  4.32it/s]
Iteration:  54%|█████▍    | 150/279 [00:34&lt;00:29,  4.31it/s]
Iteration:  54%|█████▍    | 151/279 [00:34&lt;00:29,  4.32it/s]
Iteration:  54%|█████▍    | 152/279 [00:35&lt;00:29,  4.31it/s]
Iteration:  55%|█████▍    | 153/279 [00:35&lt;00:29,  4.32it/s]
Iteration:  55%|█████▌    | 154/279 [00:35&lt;00:28,  4.32it/s]
Iteration:  56%|█████▌    | 155/279 [00:35&lt;00:28,  4.31it/s]
Iteration:  56%|█████▌    | 156/279 [00:36&lt;00:28,  4.32it/s]
Iteration:  56%|█████▋    | 157/279 [00:36&lt;00:28,  4.32it/s]
Iteration:  57%|█████▋    | 158/279 [00:36&lt;00:27,  4.32it/s]
Iteration:  57%|█████▋    | 159/279 [00:36&lt;00:27,  4.33it/s]
Iteration:  57%|█████▋    | 160/279 [00:37&lt;00:27,  4.33it/s]
Iteration:  58%|█████▊    | 161/279 [00:37&lt;00:27,  4.33it/s]
Iteration:  58%|█████▊    | 162/279 [00:37&lt;00:27,  4.33it/s]
Iteration:  58%|█████▊    | 163/279 [00:37&lt;00:26,  4.32it/s]
Iteration:  59%|█████▉    | 164/279 [00:37&lt;00:26,  4.33it/s]
Iteration:  59%|█████▉    | 165/279 [00:38&lt;00:26,  4.33it/s]
Iteration:  59%|█████▉    | 166/279 [00:38&lt;00:26,  4.32it/s]
Iteration:  60%|█████▉    | 167/279 [00:38&lt;00:25,  4.32it/s]
Iteration:  60%|██████    | 168/279 [00:38&lt;00:25,  4.32it/s]
Iteration:  61%|██████    | 169/279 [00:39&lt;00:25,  4.32it/s]
Iteration:  61%|██████    | 170/279 [00:39&lt;00:25,  4.32it/s]
Iteration:  61%|██████▏   | 171/279 [00:39&lt;00:25,  4.32it/s]
Iteration:  62%|██████▏   | 172/279 [00:39&lt;00:24,  4.33it/s]
Iteration:  62%|██████▏   | 173/279 [00:40&lt;00:24,  4.31it/s]
Iteration:  62%|██████▏   | 174/279 [00:40&lt;00:24,  4.32it/s]
Iteration:  63%|██████▎   | 175/279 [00:40&lt;00:24,  4.31it/s]
Iteration:  63%|██████▎   | 176/279 [00:40&lt;00:23,  4.31it/s]
Iteration:  63%|██████▎   | 177/279 [00:40&lt;00:23,  4.33it/s]
Iteration:  64%|██████▍   | 178/279 [00:41&lt;00:23,  4.33it/s]
Iteration:  64%|██████▍   | 179/279 [00:41&lt;00:23,  4.33it/s]
Iteration:  65%|██████▍   | 180/279 [00:41&lt;00:22,  4.33it/s]
Iteration:  65%|██████▍   | 181/279 [00:41&lt;00:22,  4.34it/s]
Iteration:  65%|██████▌   | 182/279 [00:42&lt;00:22,  4.33it/s]
Iteration:  66%|██████▌   | 183/279 [00:42&lt;00:22,  4.34it/s]
Iteration:  66%|██████▌   | 184/279 [00:42&lt;00:21,  4.33it/s]
Iteration:  66%|██████▋   | 185/279 [00:42&lt;00:21,  4.34it/s]
Iteration:  67%|██████▋   | 186/279 [00:43&lt;00:21,  4.33it/s]
Iteration:  67%|██████▋   | 187/279 [00:43&lt;00:21,  4.32it/s]
Iteration:  67%|██████▋   | 188/279 [00:43&lt;00:21,  4.33it/s]
Iteration:  68%|██████▊   | 189/279 [00:43&lt;00:20,  4.33it/s]
Iteration:  68%|██████▊   | 190/279 [00:43&lt;00:20,  4.33it/s]
Iteration:  68%|██████▊   | 191/279 [00:44&lt;00:20,  4.34it/s]
Iteration:  69%|██████▉   | 192/279 [00:44&lt;00:20,  4.33it/s]
Iteration:  69%|██████▉   | 193/279 [00:44&lt;00:19,  4.34it/s]
Iteration:  70%|██████▉   | 194/279 [00:44&lt;00:19,  4.34it/s]
Iteration:  70%|██████▉   | 195/279 [00:45&lt;00:19,  4.33it/s]
Iteration:  70%|███████   | 196/279 [00:45&lt;00:19,  4.34it/s]
Iteration:  71%|███████   | 197/279 [00:45&lt;00:18,  4.33it/s]
Iteration:  71%|███████   | 198/279 [00:45&lt;00:18,  4.34it/s]
Iteration:  71%|███████▏  | 199/279 [00:46&lt;00:18,  4.35it/s]
Iteration:  72%|███████▏  | 200/279 [00:46&lt;00:18,  4.34it/s]
Iteration:  72%|███████▏  | 201/279 [00:46&lt;00:17,  4.34it/s]
Iteration:  72%|███████▏  | 202/279 [00:46&lt;00:17,  4.33it/s]
Iteration:  73%|███████▎  | 203/279 [00:46&lt;00:17,  4.33it/s]
Iteration:  73%|███████▎  | 204/279 [00:47&lt;00:17,  4.33it/s]
Iteration:  73%|███████▎  | 205/279 [00:47&lt;00:17,  4.34it/s]
Iteration:  74%|███████▍  | 206/279 [00:47&lt;00:16,  4.33it/s]
Iteration:  74%|███████▍  | 207/279 [00:47&lt;00:16,  4.33it/s]
Iteration:  75%|███████▍  | 208/279 [00:48&lt;00:16,  4.34it/s]
Iteration:  75%|███████▍  | 209/279 [00:48&lt;00:16,  4.34it/s]
Iteration:  75%|███████▌  | 210/279 [00:48&lt;00:15,  4.35it/s]
Iteration:  76%|███████▌  | 211/279 [00:48&lt;00:15,  4.34it/s]
Iteration:  76%|███████▌  | 212/279 [00:49&lt;00:15,  4.34it/s]
Iteration:  76%|███████▋  | 213/279 [00:49&lt;00:15,  4.34it/s]
Iteration:  77%|███████▋  | 214/279 [00:49&lt;00:14,  4.33it/s]
Iteration:  77%|███████▋  | 215/279 [00:49&lt;00:14,  4.33it/s]
Iteration:  77%|███████▋  | 216/279 [00:49&lt;00:14,  4.32it/s]
Iteration:  78%|███████▊  | 217/279 [00:50&lt;00:14,  4.32it/s]
Iteration:  78%|███████▊  | 218/279 [00:50&lt;00:14,  4.32it/s]
Iteration:  78%|███████▊  | 219/279 [00:50&lt;00:13,  4.31it/s]
Iteration:  79%|███████▉  | 220/279 [00:50&lt;00:13,  4.33it/s]
Iteration:  79%|███████▉  | 221/279 [00:51&lt;00:13,  4.33it/s]
Iteration:  80%|███████▉  | 222/279 [00:51&lt;00:13,  4.32it/s]
Iteration:  80%|███████▉  | 223/279 [00:51&lt;00:12,  4.33it/s]
Iteration:  80%|████████  | 224/279 [00:51&lt;00:12,  4.33it/s]
Iteration:  81%|████████  | 225/279 [00:52&lt;00:12,  4.33it/s]
Iteration:  81%|████████  | 226/279 [00:52&lt;00:12,  4.33it/s]
Iteration:  81%|████████▏ | 227/279 [00:52&lt;00:11,  4.34it/s]
Iteration:  82%|████████▏ | 228/279 [00:52&lt;00:11,  4.35it/s]
Iteration:  82%|████████▏ | 229/279 [00:52&lt;00:11,  4.35it/s]
Iteration:  82%|████████▏ | 230/279 [00:53&lt;00:11,  4.34it/s]
Iteration:  83%|████████▎ | 231/279 [00:53&lt;00:11,  4.34it/s]
Iteration:  83%|████████▎ | 232/279 [00:53&lt;00:10,  4.32it/s]
Iteration:  84%|████████▎ | 233/279 [00:53&lt;00:10,  4.33it/s]
Iteration:  84%|████████▍ | 234/279 [00:54&lt;00:10,  4.34it/s]
Iteration:  84%|████████▍ | 235/279 [00:54&lt;00:10,  4.33it/s]
Iteration:  85%|████████▍ | 236/279 [00:54&lt;00:09,  4.35it/s]
Iteration:  85%|████████▍ | 237/279 [00:54&lt;00:09,  4.36it/s]
Iteration:  85%|████████▌ | 238/279 [00:55&lt;00:09,  4.35it/s]
Iteration:  86%|████████▌ | 239/279 [00:55&lt;00:09,  4.36it/s]
Iteration:  86%|████████▌ | 240/279 [00:55&lt;00:08,  4.35it/s]
Iteration:  86%|████████▋ | 241/279 [00:55&lt;00:08,  4.34it/s]
Iteration:  87%|████████▋ | 242/279 [00:55&lt;00:08,  4.35it/s]
Iteration:  87%|████████▋ | 243/279 [00:56&lt;00:08,  4.35it/s]
Iteration:  87%|████████▋ | 244/279 [00:56&lt;00:08,  4.35it/s]
Iteration:  88%|████████▊ | 245/279 [00:56&lt;00:07,  4.36it/s]
Iteration:  88%|████████▊ | 246/279 [00:56&lt;00:07,  4.34it/s]
Iteration:  89%|████████▊ | 247/279 [00:57&lt;00:07,  4.34it/s]
Iteration:  89%|████████▉ | 248/279 [00:57&lt;00:07,  4.35it/s]
Iteration:  89%|████████▉ | 249/279 [00:57&lt;00:06,  4.30it/s]
Iteration:  90%|████████▉ | 250/279 [00:57&lt;00:06,  4.31it/s]
Iteration:  90%|████████▉ | 251/279 [00:58&lt;00:06,  4.32it/s]
Iteration:  90%|█████████ | 252/279 [00:58&lt;00:06,  4.32it/s]
Iteration:  91%|█████████ | 253/279 [00:58&lt;00:06,  4.33it/s]
Iteration:  91%|█████████ | 254/279 [00:58&lt;00:05,  4.33it/s]
Iteration:  91%|█████████▏| 255/279 [00:58&lt;00:05,  4.33it/s]
Iteration:  92%|█████████▏| 256/279 [00:59&lt;00:05,  4.33it/s]
Iteration:  92%|█████████▏| 257/279 [00:59&lt;00:05,  4.33it/s]
Iteration:  92%|█████████▏| 258/279 [00:59&lt;00:04,  4.33it/s]
Iteration:  93%|█████████▎| 259/279 [00:59&lt;00:04,  4.33it/s]
Iteration:  93%|█████████▎| 260/279 [01:00&lt;00:04,  4.34it/s]
Iteration:  94%|█████████▎| 261/279 [01:00&lt;00:04,  4.34it/s]
Iteration:  94%|█████████▍| 262/279 [01:00&lt;00:03,  4.34it/s]
Iteration:  94%|█████████▍| 263/279 [01:00&lt;00:03,  4.34it/s]
Iteration:  95%|█████████▍| 264/279 [01:01&lt;00:03,  4.34it/s]
Iteration:  95%|█████████▍| 265/279 [01:01&lt;00:03,  4.34it/s]
Iteration:  95%|█████████▌| 266/279 [01:01&lt;00:02,  4.33it/s]
Iteration:  96%|█████████▌| 267/279 [01:01&lt;00:02,  4.34it/s]
Iteration:  96%|█████████▌| 268/279 [01:01&lt;00:02,  4.33it/s]
Iteration:  96%|█████████▋| 269/279 [01:02&lt;00:02,  4.34it/s]
Iteration:  97%|█████████▋| 270/279 [01:02&lt;00:02,  4.34it/s]
Iteration:  97%|█████████▋| 271/279 [01:02&lt;00:01,  4.33it/s]
Iteration:  97%|█████████▋| 272/279 [01:02&lt;00:01,  4.33it/s]
Iteration:  98%|█████████▊| 273/279 [01:03&lt;00:01,  4.33it/s]
Iteration:  98%|█████████▊| 274/279 [01:03&lt;00:01,  4.33it/s]
Iteration:  99%|█████████▊| 275/279 [01:03&lt;00:00,  4.35it/s]
Iteration:  99%|█████████▉| 276/279 [01:03&lt;00:00,  4.33it/s]
Iteration:  99%|█████████▉| 277/279 [01:04&lt;00:00,  4.33it/s]
Iteration: 100%|█████████▉| 278/279 [01:04&lt;00:00,  4.34it/s]
Iteration: 100%|██████████| 279/279 [01:04&lt;00:00,  4.33it/s]
Epoch: 100%|██████████| 1/1 [01:04&lt;00:00, 64.48s/it]
08/31/2020 15:46:49 - INFO - transformers.trainer -   

Training completed. Do not forget to share your model on huggingface.co/models =)


08/31/2020 15:46:49 - INFO - transformers.trainer -   Saving model checkpoint to ./models
08/31/2020 15:46:49 - INFO - transformers.configuration_utils -   Configuration saved in ./models/config.json
08/31/2020 15:46:50 - INFO - transformers.modeling_utils -   Model weights saved in ./models/pytorch_model.bin
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.-Language-model-evaluation">3. Language model evaluation<a class="anchor-link" href="#3.-Language-model-evaluation"> </a></h2><p>To run evaluation on the model with your eval dataset, all you need to call is the built-in <code>finetuner.evaluate()</code>, since you've already loaded in your eval dataset during training.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">finetuner</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now you have your very own pre-trained language model that's been fine-tuned on your personal domain data!</p>
<p>Since we've just fine-tuned a causal language model, we can actually load this straight into an <a href="/adaptnlptext_generation.html#EasyTextGenerator"><code>EasyTextGenerator</code></a> class object and play around with our language model to evaluate it qualitatively with our own "eyes".</p>
<p>All we have to do is pass in the directory that we've output our trained language model, in this case it's located in "./models"</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">EasyTextGenerator</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;China and the U.S. will begin to&quot;</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">EasyTextGenerator</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">generated_text</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span> 
    <span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;./models&quot;</span><span class="p">,</span> 
    <span class="n">num_tokens_to_produce</span><span class="o">=</span><span class="mi">50</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Generating: 100%|██████████| 1/1 [00:00&lt;00:00,  2.26it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;China and the U.S. will begin to develop their own nuclear weapons in the coming years.\n\nThe U.S. has been developing a range of nuclear weapons since the 1950s, but the U.S. has never used them in combat. The U.S. has been&#39;]
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can compare this with the original pre-trained gpt2 model as well.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">generated_text</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span> 
    <span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">,</span> 
    <span class="n">num_tokens_to_produce</span><span class="o">=</span><span class="mi">50</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.
Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: [&#39;h.0.attn.masked_bias&#39;, &#39;h.1.attn.masked_bias&#39;, &#39;h.2.attn.masked_bias&#39;, &#39;h.3.attn.masked_bias&#39;, &#39;h.4.attn.masked_bias&#39;, &#39;h.5.attn.masked_bias&#39;, &#39;h.6.attn.masked_bias&#39;, &#39;h.7.attn.masked_bias&#39;, &#39;h.8.attn.masked_bias&#39;, &#39;h.9.attn.masked_bias&#39;, &#39;h.10.attn.masked_bias&#39;, &#39;h.11.attn.masked_bias&#39;, &#39;lm_head.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Generating: 100%|██████████| 1/1 [00:00&lt;00:00,  2.33it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;China and the U.S. will begin to see the effects of the new sanctions on the Russian economy.\n\n&#34;The U.S. is going to be the first to see the effects of the new sanctions,&#34; said Michael O\&#39;Hanlon, a senior fellow at the Center for Strategic&#39;]
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>
</pre>
</div>
</div>

</div>
</div>

</div>
    

</div>
 



    <div class="tags">
        
    </div>

</div>



<footer>
            <div class="row">
                <div class="col-lg-12 footer">
                  <p><img src="/adaptnlp/assets/images/company_logo.png" alt="Company logo"/></p>
               &copy;2021 Novetta. All rights reserved. <br />
 Site last generated: Apr 20, 2021 <br />
                </div>
            </div>
</footer>


        </div>
    <!-- /.row -->
</div>
<!-- /.container -->
</div>
<!-- /#main -->
    </div>

</body>

</html>
