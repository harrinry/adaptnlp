---

title: Tutorial&#58; Fine-Tuning a Language Model on DataFrames with IMDB


keywords: fastai
sidebar: home_sidebar

summary: "Tuning a base Language model on the IMDB dataset"
description: "Tuning a base Language model on the IMDB dataset"
nb_path: "nbs/training_api_tutorials/language_model/language_model_from_df.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/training_api_tutorials/language_model/language_model_from_df.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">Introduction<a class="anchor-link" href="#Introduction"> </a></h2><p>In this tutorial we will be showing an end-to-end example of fine-tuning a Transformer language model on a custom dataset in <code>DataFrame</code> format.</p>
<p>By the end of this you should be able to:</p>
<ol>
<li>Build a dataset with the <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a> class, and their DataLoaders</li>
<li>Build a <a href="/adaptnlp/training.language_model.html#LanguageModelTuner"><code>LanguageModelTuner</code></a> quickly, find a good learning rate, and train with the One-Cycle Policy</li>
<li>Save that model away, to be used with deployment or other HuggingFace libraries</li>
<li>Apply inference using both the <code>Tuner</code> available function as well as with the <a href="/adaptnlp/text_generation.html#EasyTextGenerator"><code>EasyTextGenerator</code></a> class within AdaptNLP</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Installing-the-Library">Installing the Library<a class="anchor-link" href="#Installing-the-Library"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This tutorial utilizies the latest AdaptNLP version, as well as parts of the <code>fastai</code> library. Please run the below code to install them:</p>
<div class="highlight"><pre><span></span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">adaptnlp</span> <span class="o">-</span><span class="n">U</span>
</pre></div>
<p>(or <code>pip3</code>)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Getting-the-Dataset">Getting the Dataset<a class="anchor-link" href="#Getting-the-Dataset"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we need a dataset. We will use the <code>fastai</code> library to download the <code>IMDB_SAMPLE</code> dataset, a subset of IMDB Movie Reviews.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.data.external</span> <span class="kn">import</span> <span class="n">URLs</span><span class="p">,</span> <span class="n">untar_data</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>URLs</code> holds a namespace of many data endpoints, and <code>untar_data</code> is a function that can download and extract any data from a given URL.</p>
<p>Combining both, we can download the data:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data_path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">IMDB_SAMPLE</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we look at what was downloaded, we will find a <code>texts.csv</code> file:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data_path</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#1) [Path(&#39;/root/.fastai/data/imdb_sample/texts.csv&#39;)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is our data we want to use. We should now open the <code>csv</code> in <code>pandas</code> to generate our <code>DataFrame</code> object:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="o">/</span><span class="s1">&#39;texts.csv&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's look at our data</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>is_valid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>negative</td>
      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>positive</td>
      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>negative</td>
      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>positive</td>
      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie "Duty, Honor, Country" are not just mere words blathered from the lips of a high-brassed offic...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>negative</td>
      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will find that there is a <code>label</code>, some <code>text</code>, and a <code>is_valid</code> boolean, which determines if a row is part of the training or the validation set</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we've downloaded some data, let's pick a viable model to train with</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Picking-a-Model-with-the-Hub">Picking a Model with the Hub<a class="anchor-link" href="#Picking-a-Model-with-the-Hub"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>AdaptNLP has a <a href="/adaptnlp/model_hub.html#HFModelHub"><code>HFModelHub</code></a> class that allows you to communicate with the HuggingFace Hub and pick a model from it, as well as a namespace <code>HF_TASKS</code> class with a list of valid tasks we can search by.</p>
<p>Let's try and find one suitable for sequence classification.</p>
<p>First we need to import the class and generate an instance of it:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">HFModelHub</span><span class="p">,</span> <span class="n">HF_TASKS</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hub</span> <span class="o">=</span> <span class="n">HFModelHub</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we can search for a model:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">search_model_by_task</span><span class="p">(</span><span class="n">HF_TASKS</span><span class="o">.</span><span class="n">TEXT_GENERATION</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's look at a few:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">models</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[Model Name: distilgpt2, Tasks: [text-generation],
 Model Name: gpt2-large, Tasks: [text-generation],
 Model Name: gpt2-medium, Tasks: [text-generation],
 Model Name: gpt2-xl, Tasks: [text-generation],
 Model Name: gpt2, Tasks: [text-generation],
 Model Name: openai-gpt, Tasks: [text-generation],
 Model Name: transfo-xl-wt103, Tasks: [text-generation],
 Model Name: xlnet-base-cased, Tasks: [text-generation],
 Model Name: xlnet-large-cased, Tasks: [text-generation]]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These are models specifically tagged with the <code>text-generation</code> tag, so you may not see a few models you would expect such as <code>bert_base_cased</code>.</p>
<p>We'll use that first model, <code>distilgpt2</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Model Name: distilgpt2, Tasks: [text-generation]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have picked a model, let's use the data API to prepare our data</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='It should be mentioned that this is optional, you can always just pass in the string name of a model such as "bert-base-cased"' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Building-TaskDatasets-with-LanguageModelDatasets">Building <a href="/adaptnlp/training.core.html#TaskDatasets"><code>TaskDatasets</code></a> with <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a><a class="anchor-link" href="#Building-TaskDatasets-with-LanguageModelDatasets"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each task has a high-level data wrapper around the <a href="/adaptnlp/training.core.html#TaskDatasets"><code>TaskDatasets</code></a> class. In our case this is the <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a> class:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">LanguageModelDatasets</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are multiple different constructors for the <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a> class, and you should never call the main constructor directly.</p>
<p>We will be using <code>from_dfs</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LanguageModelDatasets.from_dfs" class="doc_header"><code>LanguageModelDatasets.from_dfs</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L74" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LanguageModelDatasets.from_dfs</code>(<strong><code>train_df</code></strong>:<code>DataFrame</code>, <strong><code>text_col</code></strong>:<code>str</code>, <strong><code>tokenizer_name</code></strong>:<code>str</code>, <strong><code>block_size</code></strong>:<code>int</code>=<em><code>128</code></em>, <strong><code>masked_lm</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>valid_df</code></strong>:<code>DataFrame</code>=<em><code>None</code></em>, <strong><code>split_func</code></strong>:<code>callable</code>=<em><code>None</code></em>, <strong><code>split_pct</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>tokenize_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>, <strong><code>auto_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>)</p>
</blockquote>
<p>Builds <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a> from a <code>DataFrame</code> or file path</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>train_df</code></strong> : <em><code>&lt;class 'pandas.core.frame.DataFrame'&gt;</code></em>   <p>A Pandas Dataframe</p></li>
</ul>
<ul>
<li><strong><code>text_col</code></strong> : <em><code>&lt;class 'str'&gt;</code></em>   <p>The name of the text column</p></li>
</ul>
<ul>
<li><strong><code>tokenizer_name</code></strong> : <em><code>&lt;class 'str'&gt;</code></em> <p>The name of the tokenizer</p></li>
</ul>
<ul>
<li><strong><code>block_size</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>The size of each block</p></li>
</ul>
<ul>
<li><strong><code>masked_lm</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em> <p>Whether the language model is a MLM</p></li>
</ul>
<ul>
<li><strong><code>valid_df</code></strong> : <em><code>&lt;class 'pandas.core.frame.DataFrame'&gt;</code></em>, <em>optional</em>   <p>An optional validation DataFrame</p></li>
</ul>
<ul>
<li><strong><code>split_func</code></strong> : <em><code>&lt;built-in function callable&gt;</code></em>, <em>optional</em>  <p>Optionally a splitting function similar to RandomSplitter</p></li>
</ul>
<ul>
<li><strong><code>split_pct</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em>    <p>What % to split the df between training and validation</p></li>
</ul>
<ul>
<li><strong><code>tokenize_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>kwargs for the tokenize function</p></li>
</ul>
<ul>
<li><strong><code>auto_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>kwargs for the AutoTokenizer.from_pretrained constructor</p></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Anything you would normally pass to the tokenizer call (such as <code>max_length</code>, <code>padding</code>) should go in <code>tokenize_kwargs</code>, and anything going to the <code>AutoTokenizer.from_pretrained</code> constructor should be passed to the <code>auto_kwargs</code>.</p>
<p>In our case we only have a <code>train_df</code>, and since we are training a language model, we want to split the data 90/10 (which is the default)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Also, we will set a block_size of 128, and it is <em>not</em> a masked language model:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dsets</span> <span class="o">=</span> <span class="n">LanguageModelDatasets</span><span class="o">.</span><span class="n">from_dfs</span><span class="p">(</span>
    <span class="n">train_df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
    <span class="n">text_col</span><span class="o">=</span><span class="s1">&#39;text&#39;</span><span class="p">,</span>
    <span class="n">tokenizer_name</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">masked_lm</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>No value for `max_length` set, automatically adjusting to the size of the model and including truncation
Sequence length set to: 1024




</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='If you have a training and validation <code>DataFrame</code>, simply pass in the validation <code>DataFrame</code> as <code>valid_df=validation_dataframe</code> and do not pass in any <code>split_func</code> or <code>split_pct</code>. Everything else is the exact same' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And finally turn it into some <a href="/adaptnlp/training.core.html#AdaptiveDataLoaders"><code>AdaptiveDataLoaders</code></a>.</p>
<p>These are just fastai's <code>DataLoaders</code> class, but it overrides a few functions to have it work nicely with HuggingFace's <code>Dataset</code> class</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LanguageModelDatasets.dataloaders" class="doc_header"><code>LanguageModelDatasets.dataloaders</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L202" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LanguageModelDatasets.dataloaders</code>(<strong><code>batch_size</code></strong>=<em><code>8</code></em>, <strong><code>shuffle_train</code></strong>=<em><code>True</code></em>, <strong><code>collate_fn</code></strong>=<em><code>default_data_collator</code></em>, <strong><code>mlm_probability</code></strong>:<code>float</code>=<em><code>0.15</code></em>, <strong><code>path</code></strong>=<em><code>'.'</code></em>, <strong><code>device</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Build DataLoaders from <code>self</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>batch_size</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>A batch size</p></li>
</ul>
<ul>
<li><strong><code>shuffle_train</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em> <p>Whether to shuffle the training dataset</p></li>
</ul>
<ul>
<li><strong><code>collate_fn</code></strong> : <em><code>&lt;class 'function'&gt;</code></em>, <em>optional</em>    <p>A custom collation function</p></li>
</ul>
<ul>
<li><strong><code>mlm_probability</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em>  <p>Token masking probablity for Masked Language Models</p></li>
</ul>
<ul>
<li><p><strong><code>path</code></strong> : <em><code>&lt;class 'str'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>device</code></strong> : <em><code>&lt;class 'NoneType'&gt;</code></em>, <em>optional</em></p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, let's view a batch of data with the <code>show_batch</code> function:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Input</th>
      <th>Label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>lighter or darker narrative and theme in his film.....Alain Delon visits swift, sure vengeance on the ruthless crime family that employed him as a hit-man in the Duccio Tessari thriller "Big Guns" after they accidentally murder his wife and child. Tessari and scenarists Roberto Gandus, Ugo Liberatore of "A Minute to Pray, a Second to Die," and Franco Verucci of "Ring of Death" take this actioneer about a career gunman for the mob right down to the wire. Indeed, "Big Guns" is rather predictable, but it still qualifies as solid entertainment with lots of savage and often sudden killings. Alain Delon of "The Godson" is appropriately laconic as he methodically deals out death to the heads of the mob families who refused to let him retire so that he could enjoy life with his young son and daughter. Richard Conte of "The Godfather" plays a Sicilian crime boss who wants to bury the hatchet with the Delon character, but the rest of his hard-nosed associates want the hit-man dead. Like most crime thrillers in the 1960s and 1970s, "Big Guns" subscribes to the cinematic morality that crime does not pay. Interestingly, the one man who has nothing to do with the murder of the wife and son of the hero survives while another betrays the hero with extreme prejudice. Tessari does not waste a second in this 90-minute shoot'em up. Apart from the mother and son dying in a car bomb meant for the father, the worst thing that takes place occurs in an automobile salvage yard when an associate of the hero is crushed in a junked car. Ostensibly, "Big Guns" is a rather bloodless outing, but it does have a high body count for a 1973 mobster melodrama. Only at the last minute does our protagonist let his guard down and so the contrived morality of an eye for an eye remains intact. Tessari stages a couple of decent car chases and the death of a don in a train traveling through a train tunnel is as bloody as this violent yarn gets. The photography and the compositions are excellent.This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the</td>
      <td>lighter or darker narrative and theme in his film.....Alain Delon visits swift, sure vengeance on the ruthless crime family that employed him as a hit-man in the Duccio Tessari thriller "Big Guns" after they accidentally murder his wife and child. Tessari and scenarists Roberto Gandus, Ugo Liberatore of "A Minute to Pray, a Second to Die," and Franco Verucci of "Ring of Death" take this actioneer about a career gunman for the mob right down to the wire. Indeed, "Big Guns" is rather predictable, but it still qualifies as solid entertainment with lots of savage and often sudden killings. Alain Delon of "The Godson" is appropriately laconic as he methodically deals out death to the heads of the mob families who refused to let him retire so that he could enjoy life with his young son and daughter. Richard Conte of "The Godfather" plays a Sicilian crime boss who wants to bury the hatchet with the Delon character, but the rest of his hard-nosed associates want the hit-man dead. Like most crime thrillers in the 1960s and 1970s, "Big Guns" subscribes to the cinematic morality that crime does not pay. Interestingly, the one man who has nothing to do with the murder of the wife and son of the hero survives while another betrays the hero with extreme prejudice. Tessari does not waste a second in this 90-minute shoot'em up. Apart from the mother and son dying in a car bomb meant for the father, the worst thing that takes place occurs in an automobile salvage yard when an associate of the hero is crushed in a junked car. Ostensibly, "Big Guns" is a rather bloodless outing, but it does have a high body count for a 1973 mobster melodrama. Only at the last minute does our protagonist let his guard down and so the contrived morality of an eye for an eye remains intact. Tessari stages a couple of decent car chases and the death of a don in a train traveling through a train tunnel is as bloody as this violent yarn gets. The photography and the compositions are excellent.This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the</td>
    </tr>
    <tr>
      <th>1</th>
      <td>éus. Beautiful Frida Hallgren is enchanting with her pretty smile and her subtle acting. The choir members are all well-developed, interesting characters with their own story each.&lt;br /&gt;&lt;br /&gt;This movie tells a story, a beautiful story, about music, love, pain, memories, death, about a man who devoted his life to music, and who tries to create a calm existence in the village where he was born, while trying to make peace with the past and with the way his life has been till then. Kay Pollack shows us that the Swedish are outstanding movie creators. Go see Så som i himmelen, it's a movie that makes you think about life and love, and that's also comforting, in some way.The spoiler warning is for those people who want to see for themselves what animals and landscapes pass before their eyes, although I don't mention it in great detail.&lt;br /&gt;&lt;br /&gt;"Earth" is an approx. 90 minute cinema version based on "Planet earth" which I watched all on BBC TV.The TV version was narrated by David Attenborough, a captivating commentator, who I had wished had also done it for "Earth" but it is Patrick Stewart, Star Trek's Captain Picard. There are regularly shots of the Earth from space so that's may be appropriate. In any case he has a nice enough and calm voice for it. There are 12 chapters in which we follow animal life on earth from North Pole to Antarctica. 3 animal families, polar bear, elephant and whale, appear in more than one of these parts. Each "chapter" starts with an indication how far from north pole or equator it is. We see something of each kind of animal, but only mammals and birds, and some fish, and some beautiful shots of vegetation, mountains, waterfalls, deserts and jungle, a near perfect presentation of the variety of life and landscapes and climates on earth. You get the impression that our planet is only inhabited by animals: people or villages or cities aren't in the film, so it's a typical nature documentary, but breathtakingly shot and accompanied by delightful music. When the film opened I already knew it would end far too soon for me. It is a family film, so no brutal killings of any animals. When one is caught by his hunter the shot ends and in other cases where we see the prey being caught it's shot in slow-motion which makes it less violent and watchable for young children (age limit 6 in The Netherlands</td>
      <td>éus. Beautiful Frida Hallgren is enchanting with her pretty smile and her subtle acting. The choir members are all well-developed, interesting characters with their own story each.&lt;br /&gt;&lt;br /&gt;This movie tells a story, a beautiful story, about music, love, pain, memories, death, about a man who devoted his life to music, and who tries to create a calm existence in the village where he was born, while trying to make peace with the past and with the way his life has been till then. Kay Pollack shows us that the Swedish are outstanding movie creators. Go see Så som i himmelen, it's a movie that makes you think about life and love, and that's also comforting, in some way.The spoiler warning is for those people who want to see for themselves what animals and landscapes pass before their eyes, although I don't mention it in great detail.&lt;br /&gt;&lt;br /&gt;"Earth" is an approx. 90 minute cinema version based on "Planet earth" which I watched all on BBC TV.The TV version was narrated by David Attenborough, a captivating commentator, who I had wished had also done it for "Earth" but it is Patrick Stewart, Star Trek's Captain Picard. There are regularly shots of the Earth from space so that's may be appropriate. In any case he has a nice enough and calm voice for it. There are 12 chapters in which we follow animal life on earth from North Pole to Antarctica. 3 animal families, polar bear, elephant and whale, appear in more than one of these parts. Each "chapter" starts with an indication how far from north pole or equator it is. We see something of each kind of animal, but only mammals and birds, and some fish, and some beautiful shots of vegetation, mountains, waterfalls, deserts and jungle, a near perfect presentation of the variety of life and landscapes and climates on earth. You get the impression that our planet is only inhabited by animals: people or villages or cities aren't in the film, so it's a typical nature documentary, but breathtakingly shot and accompanied by delightful music. When the film opened I already knew it would end far too soon for me. It is a family film, so no brutal killings of any animals. When one is caught by his hunter the shot ends and in other cases where we see the prey being caught it's shot in slow-motion which makes it less violent and watchable for young children (age limit 6 in The Netherlands</td>
    </tr>
    <tr>
      <th>2</th>
      <td>the entire film. &lt;br /&gt;&lt;br /&gt;Why would a company release a DVD that the cover is so misleading? I feel like such an idiot for renting this movie based strictly on the box. As much as I explore IMDb I should have done a little research and made a list prior to visiting my local video rental store. I have no one to blame except myself. I want my money and time back. &lt;br /&gt;&lt;br /&gt;DO NOT WATCH THIS MOVIE. &lt;br /&gt;&lt;br /&gt;Even if curiosity is motivating you, stick cocktail umbrellas in your eyes instead. It will be much more enjoyable. You have been warned!I love this film. It is well written and acted and has good cinematography. The story blends action, humor, mysticism, and tenderness with great sets and beautiful location shots. See it, buy it, show it to your friends.&lt;br /&gt;&lt;br /&gt;The acting is good and Murphy especially does a fine job portraying the reluctant/unlikely hero. I enjoyed all the characters and found them to be interesting and well developed with dynamic interactions.&lt;br /&gt;&lt;br /&gt;I cared what happened to these people and, while the outcome was pretty predictable (the good guys win, the hero gets the astonishingly attractive girl and the holy child saves lives--who doesn't see that coming?), it still made me happy when everything worked out well in the end. Thank God this film's dignity was never ruined with a crappy sequel. Grab some popcorn, cuddle up on the couch, and watch this fun, happy and entertaining film.While I count myself as a fan of the Babylon 5 television series, the original movie that introduced the series was a weak start. Although many of the elements that would later mature and become much more compelling in the series are there, the pace of The Gathering is slow, the makeup somewhat inadequate, and the plot confusing. Worse, the characterization in the premiere episode is poor. Although the ratings chart shows that many fans are willing to overlook these problems, I remember The Gathering almost turned me off off what soon grew into a spectacular series.What the *bliep* is it with this movie? Couldn't they fiend a better script? All in all a 'nice' movie, but... it has been done more than once... Up till the end I thought it was okay, but... the going back to the past part... *barf* SO corny... Was waiting for the fairy god mother to appear... but wow, that</td>
      <td>the entire film. &lt;br /&gt;&lt;br /&gt;Why would a company release a DVD that the cover is so misleading? I feel like such an idiot for renting this movie based strictly on the box. As much as I explore IMDb I should have done a little research and made a list prior to visiting my local video rental store. I have no one to blame except myself. I want my money and time back. &lt;br /&gt;&lt;br /&gt;DO NOT WATCH THIS MOVIE. &lt;br /&gt;&lt;br /&gt;Even if curiosity is motivating you, stick cocktail umbrellas in your eyes instead. It will be much more enjoyable. You have been warned!I love this film. It is well written and acted and has good cinematography. The story blends action, humor, mysticism, and tenderness with great sets and beautiful location shots. See it, buy it, show it to your friends.&lt;br /&gt;&lt;br /&gt;The acting is good and Murphy especially does a fine job portraying the reluctant/unlikely hero. I enjoyed all the characters and found them to be interesting and well developed with dynamic interactions.&lt;br /&gt;&lt;br /&gt;I cared what happened to these people and, while the outcome was pretty predictable (the good guys win, the hero gets the astonishingly attractive girl and the holy child saves lives--who doesn't see that coming?), it still made me happy when everything worked out well in the end. Thank God this film's dignity was never ruined with a crappy sequel. Grab some popcorn, cuddle up on the couch, and watch this fun, happy and entertaining film.While I count myself as a fan of the Babylon 5 television series, the original movie that introduced the series was a weak start. Although many of the elements that would later mature and become much more compelling in the series are there, the pace of The Gathering is slow, the makeup somewhat inadequate, and the plot confusing. Worse, the characterization in the premiere episode is poor. Although the ratings chart shows that many fans are willing to overlook these problems, I remember The Gathering almost turned me off off what soon grew into a spectacular series.What the *bliep* is it with this movie? Couldn't they fiend a better script? All in all a 'nice' movie, but... it has been done more than once... Up till the end I thought it was okay, but... the going back to the past part... *barf* SO corny... Was waiting for the fairy god mother to appear... but wow, that</td>
    </tr>
    <tr>
      <th>3</th>
      <td>John Thaw is mesmerising as Tom Oakley. His transformation from gruff to caring was so well realised, making it more believable than Scrooge in Christmas Carol. After Inspector Morse, this is Thaw's finest hour. He was matched earnestly by a young Nick Robinson, who gave a thoroughly convincing portrayal of an evacuee traumatised by the abusive relationship with his mother. The script and music made it worth the buy, and you also see Thaw playing the organ. Amazing! The most moving scene, was Willie finding out about Zak's death, and then Tom telling him about his deceased family who died of scarlatina. Buy this, you'll love it! 10/10 Bethany CoxI had read many good things about this adaptation of my favorite novel...so invariably my expectations were crushed. But they were crushed more than should be expected. The movie would have been a decent movie if I had not read the novel beforehand, which perhaps ruined it for me.&lt;br /&gt;&lt;br /&gt;In any event, for some reason they changed the labor camp at Toulon to a ship full of galley slaves. The scene at Bishop Myriel's was fine. In fact, other than the galleys, things survived up until the dismissal of Fantine. Because we do not want to have bad things happen to a good woman, she does not cut her hair, sell her teeth, or become a prostitute. The worst she does is run into the mayor's office and spit on his face. Bamatabois is entirely eliminated. Because having children out of wedlock should also not be talked about, Tholomyes is Fantine's dead husband, rather than an irresponsible dandy. Valjean is able to fetch Cosette for Fantine before the Champmathieu affair, so they reunite happily, yet another change. Then comes the convent, which is a pretty difficult scene to screw up. Thankfully, it was saved. After this three minutes of accuracy, however, the movie again begins to hurtle towards Classic Novel Butchering.&lt;br /&gt;&lt;br /&gt;As Cosette and Valjean are riding through the park, they come across Marius giving a speech at a meeting. About prison reform. When he comes to hand out fliers to Valjean and Cosette, he says the one line in the movie that set me screaming at the TV set. "We aren't revolutionaries." I could hear Victor Hugo thrashing in his grave. OF COURSE THEY ARE REVOL</td>
      <td>John Thaw is mesmerising as Tom Oakley. His transformation from gruff to caring was so well realised, making it more believable than Scrooge in Christmas Carol. After Inspector Morse, this is Thaw's finest hour. He was matched earnestly by a young Nick Robinson, who gave a thoroughly convincing portrayal of an evacuee traumatised by the abusive relationship with his mother. The script and music made it worth the buy, and you also see Thaw playing the organ. Amazing! The most moving scene, was Willie finding out about Zak's death, and then Tom telling him about his deceased family who died of scarlatina. Buy this, you'll love it! 10/10 Bethany CoxI had read many good things about this adaptation of my favorite novel...so invariably my expectations were crushed. But they were crushed more than should be expected. The movie would have been a decent movie if I had not read the novel beforehand, which perhaps ruined it for me.&lt;br /&gt;&lt;br /&gt;In any event, for some reason they changed the labor camp at Toulon to a ship full of galley slaves. The scene at Bishop Myriel's was fine. In fact, other than the galleys, things survived up until the dismissal of Fantine. Because we do not want to have bad things happen to a good woman, she does not cut her hair, sell her teeth, or become a prostitute. The worst she does is run into the mayor's office and spit on his face. Bamatabois is entirely eliminated. Because having children out of wedlock should also not be talked about, Tholomyes is Fantine's dead husband, rather than an irresponsible dandy. Valjean is able to fetch Cosette for Fantine before the Champmathieu affair, so they reunite happily, yet another change. Then comes the convent, which is a pretty difficult scene to screw up. Thankfully, it was saved. After this three minutes of accuracy, however, the movie again begins to hurtle towards Classic Novel Butchering.&lt;br /&gt;&lt;br /&gt;As Cosette and Valjean are riding through the park, they come across Marius giving a speech at a meeting. About prison reform. When he comes to hand out fliers to Valjean and Cosette, he says the one line in the movie that set me screaming at the TV set. "We aren't revolutionaries." I could hear Victor Hugo thrashing in his grave. OF COURSE THEY ARE REVOL</td>
    </tr>
    <tr>
      <th>4</th>
      <td>first human attempt with both eyes at the same time, that's totally unprofessional. And to do all this apparently without informed consent of the patient?! And why on earth choose for eyes that have a totally unusual color for humans, and make the victim look like a freak?! By the way, I noticed that all the real wolves in the movie had puppy-like normal dark eyes, couldn't they have waited for such a specimen? The story is lame, it's about this poor guy Aaron who gets these weird eye-transplants, which suddenly makes him feel like the donor-wolf (or at least, that's what I make of it) and then he's being chased by some military men. Especially this last bit is ridiculous. I mean, I can understand that the army is interested in the results of the experiment (imagine soldiers with night-vision eye-sight!) but as the operation fails on account of the apparent nervous breakdown of the patient, it's beyond me why they're out to kill him. Why not leave him alone and look for another usable recipient? (a volunteering soldier maybe??). And why try to kill everyone else that's involved with poor Aaron, isn't that a bit steep?! Who the hell are these militaries anyway, I hope not the US army or the government, they behave like psychopaths, walking around the hospital waving automatic weapons, raiding private apartments like they're after some public enemy # 1, and displaying during the ultimate show-down in the woods a total lack of discipline, like a bunch of frightened schoolchildren, panicking and shooting randomly around.&lt;br /&gt;&lt;br /&gt;Aaron, for some unfathomable medical reason, feels like a wolf after the transplantation of the eyes. Why would that be??? He suddenly sees visions of wandering wolves. What is this? Are we supposed to believe that the memories of the donor-wolf are situated in it's eye-balls?!? And that the recipient of these eye-balls also adopts the wolf's craving for red (life-) meat and can jump off of a 30 feet high balcony and land unharmed on his all-fours like a cat (can a wolf even DO that??!).&lt;br /&gt;&lt;br /&gt;The acting (or the lack thereof) didn't help the credibility of all this either: everyone stumbles through their lines like wooden dolls, especially this Indian girl, she may be pretty but she can only come up with one expression (vexed) and some disinterested mumblings about</td>
      <td>first human attempt with both eyes at the same time, that's totally unprofessional. And to do all this apparently without informed consent of the patient?! And why on earth choose for eyes that have a totally unusual color for humans, and make the victim look like a freak?! By the way, I noticed that all the real wolves in the movie had puppy-like normal dark eyes, couldn't they have waited for such a specimen? The story is lame, it's about this poor guy Aaron who gets these weird eye-transplants, which suddenly makes him feel like the donor-wolf (or at least, that's what I make of it) and then he's being chased by some military men. Especially this last bit is ridiculous. I mean, I can understand that the army is interested in the results of the experiment (imagine soldiers with night-vision eye-sight!) but as the operation fails on account of the apparent nervous breakdown of the patient, it's beyond me why they're out to kill him. Why not leave him alone and look for another usable recipient? (a volunteering soldier maybe??). And why try to kill everyone else that's involved with poor Aaron, isn't that a bit steep?! Who the hell are these militaries anyway, I hope not the US army or the government, they behave like psychopaths, walking around the hospital waving automatic weapons, raiding private apartments like they're after some public enemy # 1, and displaying during the ultimate show-down in the woods a total lack of discipline, like a bunch of frightened schoolchildren, panicking and shooting randomly around.&lt;br /&gt;&lt;br /&gt;Aaron, for some unfathomable medical reason, feels like a wolf after the transplantation of the eyes. Why would that be??? He suddenly sees visions of wandering wolves. What is this? Are we supposed to believe that the memories of the donor-wolf are situated in it's eye-balls?!? And that the recipient of these eye-balls also adopts the wolf's craving for red (life-) meat and can jump off of a 30 feet high balcony and land unharmed on his all-fours like a cat (can a wolf even DO that??!).&lt;br /&gt;&lt;br /&gt;The acting (or the lack thereof) didn't help the credibility of all this either: everyone stumbles through their lines like wooden dolls, especially this Indian girl, she may be pretty but she can only come up with one expression (vexed) and some disinterested mumblings about</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When training a language model, the input and output are made to be the exact same, so there isn't a shown noticable difference here.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Building-Tuner">Building <code>Tuner</code><a class="anchor-link" href="#Building-Tuner"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we need to build a compatible <code>Tuner</code> for our problem. These tuners contain good defaults for our problem space, including loss functions and metrics.</p>
<p>First let's import the <a href="/adaptnlp/training.language_model.html#LanguageModelTuner"><code>LanguageModelTuner</code></a> and view it's documentation</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">LanguageModelTuner</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LanguageModelTuner" class="doc_header"><code>class</code> <code>LanguageModelTuner</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L228" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LanguageModelTuner</code>(<strong><code>dls</code></strong>:<code>DataLoaders</code>, <strong><code>model_name</code></strong>, <strong><code>tokenizer</code></strong>=<em><code>None</code></em>, <strong><code>language_model_type</code></strong>:<code>LMType</code>=<em><code>'causal'</code></em>, <strong><code>loss_func</code></strong>=<em><code>CrossEntropyLoss()</code></em>, <strong><code>metrics</code></strong>=<em><code>[&lt;fastai.metrics.Perplexity object at 0x7fe483364100&gt;]</code></em>, <strong><code>opt_func</code></strong>=<em><code>Adam</code></em>, <strong><code>additional_cbs</code></strong>=<em><code>None</code></em>, <strong><code>expose_fastai_api</code></strong>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/adaptnlp/training.core.html#AdaptiveTuner"><code>AdaptiveTuner</code></a></p>
</blockquote>
<p>An <a href="/adaptnlp/training.core.html#AdaptiveTuner"><code>AdaptiveTuner</code></a> with good defaults for Language Model fine-tuning
<strong>Valid kwargs and defaults:</strong></p>
<ul>
<li><code>lr</code>:float = 0.001</li>
<li><code>splitter</code>:function = <code>trainable_params</code></li>
<li><code>cbs</code>:list = None</li>
<li><code>path</code>:Path = None</li>
<li><code>model_dir</code>:Path = 'models'</li>
<li><code>wd</code>:float = None</li>
<li><code>wd_bn_bias</code>:bool = False</li>
<li><code>train_bn</code>:bool = True</li>
<li><code>moms</code>: tuple(float) = (0.95, 0.85, 0.95)</li>
</ul>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>dls</code></strong> : <em><code>&lt;class 'fastai.data.core.DataLoaders'&gt;</code></em>   <p>A set of DataLoaders or AdaptiveDataLoaders</p></li>
</ul>
<ul>
<li><strong><code>model_name</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em>  <p>A HuggingFace model</p></li>
</ul>
<ul>
<li><strong><code>tokenizer</code></strong> : <em><code>&lt;class 'NoneType'&gt;</code></em>, <em>optional</em> <p>A HuggingFace tokenizer</p></li>
</ul>
<ul>
<li><strong><code>language_model_type</code></strong> : <em><code>&lt;class 'fastcore.basics.LMType'&gt;</code></em>, <em>optional</em> <p>The type of language model to use</p></li>
</ul>
<ul>
<li><strong><code>loss_func</code></strong> : <em><code>&lt;class 'fastai.losses.CrossEntropyLossFlat'&gt;</code></em>, <em>optional</em>   <p>A loss function</p></li>
</ul>
<ul>
<li><strong><code>metrics</code></strong> : <em><code>&lt;class 'list'&gt;</code></em>, <em>optional</em>   <p>Metrics to monitor the training with</p></li>
</ul>
<ul>
<li><strong><code>opt_func</code></strong> : <em><code>&lt;class 'function'&gt;</code></em>, <em>optional</em>  <p>A fastai or torch Optimizer</p></li>
</ul>
<ul>
<li><strong><code>additional_cbs</code></strong> : <em><code>&lt;class 'NoneType'&gt;</code></em>, <em>optional</em>    <p>Additional Callbacks to have always tied to the Tuner,</p></li>
</ul>
<ul>
<li><strong><code>expose_fastai_api</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em> <p>Whether to expose the fastai API</p></li>
</ul>
<ul>
<li><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we'll pass in our <code>DataLoaders</code>, the name of our model, and the tokenizer:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='If you are not using the data API (<a href="/adaptnlp/training.core.html#TaskDatasets"><code>TaskDatasets</code></a>, <a href="/adaptnlp/training.sequence_classification.html#SequenceClassificationDatasets"><code>SequenceClassificationDatasets</code></a>, etc), you need to pass in the tokenizer to the constructor as well with <code>tokenizer=tokenizer</code>' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span> <span class="o">=</span> <span class="n">LanguageModelTuner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">dls</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By default we can see that it used <code>CrossEntropyLoss</code> as our loss function, and <code>Perplexity</code> as our metric</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">loss_func</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>FlattenedLoss of CrossEntropyLoss()</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">tuner</span><span class="o">.</span><span class="n">metrics</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>perplexity
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally we just need to train our model!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Fine-Tuning">Fine-Tuning<a class="anchor-link" href="#Fine-Tuning"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To fine-tune, AdaptNLP's tuner class provides only a few functions to work with. The important ones are the <code>tune</code> and <code>lr_find</code> class.</p>
<p>As the <code>Tuner</code> uses <code>fastai</code> under the hood, <code>lr_find</code> calls fastai's Learning Rate Finder to help us pick a learning rate. Let's do that now:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="AdaptiveTuner.lr_find" class="doc_header"><code>AdaptiveTuner.lr_find</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/core.py#L415" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>AdaptiveTuner.lr_find</code>(<strong><code>start_lr</code></strong>=<em><code>1e-07</code></em>, <strong><code>end_lr</code></strong>=<em><code>10</code></em>, <strong><code>num_it</code></strong>=<em><code>100</code></em>, <strong><code>stop_div</code></strong>=<em><code>True</code></em>, <strong><code>show_plot</code></strong>=<em><code>True</code></em>, <strong><code>suggest_funcs</code></strong>=<em><code>valley</code></em>)</p>
</blockquote>
<p>Runs fastai's <code>LR Finder</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><p><strong><code>start_lr</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>end_lr</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>num_it</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>stop_div</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>show_plot</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>suggest_funcs</code></strong> : <em><code>&lt;class 'function'&gt;</code></em>, <em>optional</em></p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/venv/lib/python3.8/site-packages/fastai/callback/schedule.py:270: UserWarning: color is redundantly defined by the &#39;color&#39; keyword argument and the fmt string &#34;ro&#34; (-&gt; color=&#39;r&#39;). The keyword argument will take precedence.
  ax.plot(val, idx, &#39;ro&#39;, label=nm, c=color)
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(valley=7.585775892948732e-05)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It recommends a learning rate of around 5e-5, so we will use that.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">5e-5</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's look at the documentation for <code>tune</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="AdaptiveTuner.tune" class="doc_header"><code>AdaptiveTuner.tune</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/core.py#L401" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>AdaptiveTuner.tune</code>(<strong><code>epochs</code></strong>:<code>int</code>, <strong><code>lr</code></strong>:<code>float</code>=<em><code>None</code></em>, <strong><code>strategy</code></strong>:<code>Strategy</code>=<em><code>'fit_one_cycle'</code></em>, <strong><code>callbacks</code></strong>:<code>list</code>=<em><code>[]</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Fine tune <code>self.model</code> for <code>epochs</code> with an <code>lr</code> and <code>strategy</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>epochs</code></strong> : <em><code>&lt;class 'int'&gt;</code></em> <p>Number of iterations to train for</p></li>
</ul>
<ul>
<li><strong><code>lr</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em>   <p>If None, finds a new learning rate and uses suggestion_method</p></li>
</ul>
<ul>
<li><strong><code>strategy</code></strong> : <em><code>&lt;class 'fastcore.basics.Strategy'&gt;</code></em>, <em>optional</em>  <p>A fitting method</p></li>
</ul>
<ul>
<li><strong><code>callbacks</code></strong> : <em><code>&lt;class 'list'&gt;</code></em>, <em>optional</em> <p>Extra fastai Callbacks</p></li>
</ul>
<ul>
<li><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can pass in a number of epochs, a learning rate, a strategy, and additional fastai callbacks to call.</p>
<p>Valid strategies live in the <code>Strategy</code> namespace class, and consist of:</p>
<ul>
<li>OneCycle (Also called the <a href="https://docs.fast.ai/callback.schedule.html#Learner.fit_one_cycle">One-Cycle Policy</a>)</li>
<li><a href="https://docs.fast.ai/callback.schedule.html#Learner.fit_flat_cos">CosineAnnealing</a></li>
<li><a href="https://docs.fast.ai/callback.schedule.html#Learner.fit_sgdr">SGDR</a></li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">Strategy</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this tutorial we will train with the One-Cycle policy, as currently it is one of the best schedulers to use.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">tune</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">Strategy</span><span class="o">.</span><span class="n">OneCycle</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Saving-Model">Saving Model<a class="anchor-link" href="#Saving-Model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have a trained model, let's save those weights away.</p>
<p>Calling <code>tuner.save</code> will save both the model and the tokenizer in the same format as how HuggingFace does:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="AdaptiveTuner.save" class="doc_header"><code>AdaptiveTuner.save</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/core.py#L423" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>AdaptiveTuner.save</code>(<strong><code>save_directory</code></strong>)</p>
</blockquote>
<p>Save a pretrained model to a <code>save_directory</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>save_directory</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em>  <p>A folder to save our model to</p></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;good_model&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;good_model&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Performing-Inference">Performing Inference<a class="anchor-link" href="#Performing-Inference"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are two ways to get predictions, the first is with the <code>.predict</code> method in our <code>tuner</code>. This is great for if you just finished training and want to see how your model performs on some new data!
The other method is with AdaptNLP's inference API, which we will show afterwards</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="In-Tuner">In Tuner<a class="anchor-link" href="#In-Tuner"> </a></h3><p>First let's write a sentence to test with</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;Hugh Jackman is a terrible &quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And then predict with it:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LanguageModelTuner.predict" class="doc_header"><code>LanguageModelTuner.predict</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L293" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LanguageModelTuner.predict</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>str</code>], <code>str</code>], <strong><code>bs</code></strong>:<code>int</code>=<em><code>64</code></em>, <strong><code>num_tokens_to_produce</code></strong>:<code>int</code>=<em><code>50</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Predict some <code>text</code> for sequence classification with the currently loaded model</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>text</code></strong> : <em><code>typing.Union[typing.List[str], str]</code></em> <p>Some text or list of texts to do inference with</p></li>
</ul>
<ul>
<li><strong><code>bs</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>A batch size to use for multiple texts</p></li>
</ul>
<ul>
<li><strong><code>num_tokens_to_produce</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em>  <p>Number of tokens to generate</p></li>
</ul>
<ul>
<li><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">num_tokens_to_produce</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [1/1 00:00<00:00]
    </div>
    
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;generated_text&#39;: [&#39;Hugh Jackman is a terrible icky, and very funny, character.&#39;]}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="With-the-Inference-API">With the Inference API<a class="anchor-link" href="#With-the-Inference-API"> </a></h3><p>Next we will use the <a href="/adaptnlp/text_generation.html#EasyTextGenerator"><code>EasyTextGenerator</code></a> class, which AdaptNLP offers:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">EasyTextGenerator</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We simply construct the class:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">EasyTextGenerator</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And call the <code>tag_text</code> method, passing in the sentence, the location of our saved model, and some names for our classes:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">model_name_or_path</span><span class="o">=</span><span class="s1">&#39;good_model&#39;</span><span class="p">,</span>
    <span class="n">num_tokens_to_produce</span><span class="o">=</span><span class="mi">8</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [1/1 00:00<00:00]
    </div>
    
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;generated_text&#39;: [&#39;Hugh Jackman is a terrible icky, and very funny, character.&#39;]}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we got the exact same output!</p>

</div>
</div>
</div>
</div>
 

