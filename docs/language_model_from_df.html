---

title: Tutorial&#58; Fine-Tuning a Language Model on DataFrames with IMDB


keywords: fastai
sidebar: home_sidebar

summary: "Tuning a base Language model on the IMDB dataset"
description: "Tuning a base Language model on the IMDB dataset"
nb_path: "nbs/training_api_tutorials/language_model/language_model_from_df.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/training_api_tutorials/language_model/language_model_from_df.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">Introduction<a class="anchor-link" href="#Introduction"> </a></h2><p>In this tutorial we will be showing an end-to-end example of fine-tuning a Transformer language model on a custom dataset in <code>DataFrame</code> format.</p>
<p>By the end of this you should be able to:</p>
<ol>
<li>Build a dataset with the <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a> class, and their DataLoaders</li>
<li>Build a <a href="/adaptnlp/training.language_model.html#LanguageModelTuner"><code>LanguageModelTuner</code></a> quickly, find a good learning rate, and train with the One-Cycle Policy</li>
<li>Save that model away, to be used with deployment or other HuggingFace libraries</li>
<li>Apply inference using both the <code>Tuner</code> available function as well as with the <a href="/adaptnlp/text_generation.html#EasyTextGenerator"><code>EasyTextGenerator</code></a> class within AdaptNLP</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Installing-the-Library">Installing the Library<a class="anchor-link" href="#Installing-the-Library"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This tutorial utilizies the latest AdaptNLP version, as well as parts of the <code>fastai</code> library. Please run the below code to install them:</p>
<div class="highlight"><pre><span></span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">adaptnlp</span> <span class="o">-</span><span class="n">U</span>
</pre></div>
<p>(or <code>pip3</code>)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Getting-the-Dataset">Getting the Dataset<a class="anchor-link" href="#Getting-the-Dataset"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we need a dataset. We will use the <code>fastai</code> library to download the <code>IMDB_SAMPLE</code> dataset, a subset of IMDB Movie Reviews.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.data.external</span> <span class="kn">import</span> <span class="n">URLs</span><span class="p">,</span> <span class="n">untar_data</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>URLs</code> holds a namespace of many data endpoints, and <code>untar_data</code> is a function that can download and extract any data from a given URL.</p>
<p>Combining both, we can download the data:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data_path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">IMDB_SAMPLE</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we look at what was downloaded, we will find a <code>texts.csv</code> file:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data_path</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#1) [Path(&#39;/root/.fastai/data/imdb_sample/texts.csv&#39;)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is our data we want to use. We should now open the <code>csv</code> in <code>pandas</code> to generate our <code>DataFrame</code> object:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="o">/</span><span class="s1">&#39;texts.csv&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's look at our data</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>is_valid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>negative</td>
      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>positive</td>
      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>negative</td>
      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>positive</td>
      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie "Duty, Honor, Country" are not just mere words blathered from the lips of a high-brassed offic...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>negative</td>
      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will find that there is a <code>label</code>, some <code>text</code>, and a <code>is_valid</code> boolean, which determines if a row is part of the training or the validation set</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we've downloaded some data, let's pick a viable model to train with</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Picking-a-Model-with-the-Hub">Picking a Model with the Hub<a class="anchor-link" href="#Picking-a-Model-with-the-Hub"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>AdaptNLP has a <a href="/adaptnlp/model_hub.html#HFModelHub"><code>HFModelHub</code></a> class that allows you to communicate with the HuggingFace Hub and pick a model from it, as well as a namespace <code>HF_TASKS</code> class with a list of valid tasks we can search by.</p>
<p>Let's try and find one suitable for sequence classification.</p>
<p>First we need to import the class and generate an instance of it:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">HFModelHub</span><span class="p">,</span> <span class="n">HF_TASKS</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hub</span> <span class="o">=</span> <span class="n">HFModelHub</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we can search for a model:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">search_model_by_task</span><span class="p">(</span><span class="n">HF_TASKS</span><span class="o">.</span><span class="n">TEXT_GENERATION</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's look at a few:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">models</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[Model Name: distilgpt2, Tasks: [text-generation],
 Model Name: gpt2-large, Tasks: [text-generation],
 Model Name: gpt2-medium, Tasks: [text-generation],
 Model Name: gpt2-xl, Tasks: [text-generation],
 Model Name: gpt2, Tasks: [text-generation],
 Model Name: openai-gpt, Tasks: [text-generation],
 Model Name: transfo-xl-wt103, Tasks: [text-generation],
 Model Name: xlnet-base-cased, Tasks: [text-generation],
 Model Name: xlnet-large-cased, Tasks: [text-generation]]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These are models specifically tagged with the <code>text-generation</code> tag, so you may not see a few models you would expect such as <code>bert_base_cased</code>.</p>
<p>We'll use that first model, <code>distilgpt2</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Model Name: distilgpt2, Tasks: [text-generation]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have picked a model, let's use the data API to prepare our data</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='It should be mentioned that this is optional, you can always just pass in the string name of a model such as "bert-base-cased"' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Building-TaskDatasets-with-LanguageModelDatasets">Building <a href="/adaptnlp/training.core.html#TaskDatasets"><code>TaskDatasets</code></a> with <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a><a class="anchor-link" href="#Building-TaskDatasets-with-LanguageModelDatasets"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each task has a high-level data wrapper around the <a href="/adaptnlp/training.core.html#TaskDatasets"><code>TaskDatasets</code></a> class. In our case this is the <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a> class:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">LanguageModelDatasets</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are multiple different constructors for the <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a> class, and you should never call the main constructor directly.</p>
<p>We will be using <code>from_dfs</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LanguageModelDatasets.from_dfs" class="doc_header"><code>LanguageModelDatasets.from_dfs</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L74" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LanguageModelDatasets.from_dfs</code>(<strong><code>train_df</code></strong>:<code>DataFrame</code>, <strong><code>text_col</code></strong>:<code>str</code>, <strong><code>tokenizer_name</code></strong>:<code>str</code>, <strong><code>block_size</code></strong>:<code>int</code>=<em><code>128</code></em>, <strong><code>masked_lm</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>valid_df</code></strong>:<code>DataFrame</code>=<em><code>None</code></em>, <strong><code>split_func</code></strong>:<code>callable</code>=<em><code>None</code></em>, <strong><code>split_pct</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>tokenize_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>, <strong><code>auto_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>)</p>
</blockquote>
<p>Builds <a href="/adaptnlp/training.language_model.html#LanguageModelDatasets"><code>LanguageModelDatasets</code></a> from a <code>DataFrame</code> or file path</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>train_df</code></strong> : <em><code>&lt;class 'pandas.core.frame.DataFrame'&gt;</code></em>   <p>A Pandas Dataframe</p></li>
</ul>
<ul>
<li><strong><code>text_col</code></strong> : <em><code>&lt;class 'str'&gt;</code></em>   <p>The name of the text column</p></li>
</ul>
<ul>
<li><strong><code>tokenizer_name</code></strong> : <em><code>&lt;class 'str'&gt;</code></em> <p>The name of the tokenizer</p></li>
</ul>
<ul>
<li><strong><code>block_size</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>The size of each block</p></li>
</ul>
<ul>
<li><strong><code>masked_lm</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em> <p>Whether the language model is a MLM</p></li>
</ul>
<ul>
<li><strong><code>valid_df</code></strong> : <em><code>&lt;class 'pandas.core.frame.DataFrame'&gt;</code></em>, <em>optional</em>   <p>An optional validation DataFrame</p></li>
</ul>
<ul>
<li><strong><code>split_func</code></strong> : <em><code>&lt;built-in function callable&gt;</code></em>, <em>optional</em>  <p>Optionally a splitting function similar to RandomSplitter</p></li>
</ul>
<ul>
<li><strong><code>split_pct</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em>    <p>What % to split the df between training and validation</p></li>
</ul>
<ul>
<li><strong><code>tokenize_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>kwargs for the tokenize function</p></li>
</ul>
<ul>
<li><strong><code>auto_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>kwargs for the AutoTokenizer.from_pretrained constructor</p></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Anything you would normally pass to the tokenizer call (such as <code>max_length</code>, <code>padding</code>) should go in <code>tokenize_kwargs</code>, and anything going to the <code>AutoTokenizer.from_pretrained</code> constructor should be passed to the <code>auto_kwargs</code>.</p>
<p>In our case we only have a <code>train_df</code>, and since we are training a language model, we want to split the data 90/10 (which is the default)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Also, we will set a block_size of 128, and it is <em>not</em> a masked language model:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dsets</span> <span class="o">=</span> <span class="n">LanguageModelDatasets</span><span class="o">.</span><span class="n">from_dfs</span><span class="p">(</span>
    <span class="n">train_df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
    <span class="n">text_col</span><span class="o">=</span><span class="s1">&#39;text&#39;</span><span class="p">,</span>
    <span class="n">tokenizer_name</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">masked_lm</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>No value for `max_length` set, automatically adjusting to the size of the model and including truncation
Sequence length set to: 1024




</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='If you have a training and validation <code>DataFrame</code>, simply pass in the validation <code>DataFrame</code> as <code>valid_df=validation_dataframe</code> and do not pass in any <code>split_func</code> or <code>split_pct</code>. Everything else is the exact same' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And finally turn it into some <a href="/adaptnlp/training.core.html#AdaptiveDataLoaders"><code>AdaptiveDataLoaders</code></a>.</p>
<p>These are just fastai's <code>DataLoaders</code> class, but it overrides a few functions to have it work nicely with HuggingFace's <code>Dataset</code> class</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LanguageModelDatasets.dataloaders" class="doc_header"><code>LanguageModelDatasets.dataloaders</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L202" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LanguageModelDatasets.dataloaders</code>(<strong><code>batch_size</code></strong>=<em><code>8</code></em>, <strong><code>shuffle_train</code></strong>=<em><code>True</code></em>, <strong><code>collate_fn</code></strong>=<em><code>default_data_collator</code></em>, <strong><code>mlm_probability</code></strong>:<code>float</code>=<em><code>0.15</code></em>, <strong><code>path</code></strong>=<em><code>'.'</code></em>, <strong><code>device</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Build DataLoaders from <code>self</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>batch_size</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>A batch size</p></li>
</ul>
<ul>
<li><strong><code>shuffle_train</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em> <p>Whether to shuffle the training dataset</p></li>
</ul>
<ul>
<li><strong><code>collate_fn</code></strong> : <em><code>&lt;class 'function'&gt;</code></em>, <em>optional</em>    <p>A custom collation function</p></li>
</ul>
<ul>
<li><strong><code>mlm_probability</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em>  <p>Token masking probablity for Masked Language Models</p></li>
</ul>
<ul>
<li><p><strong><code>path</code></strong> : <em><code>&lt;class 'str'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>device</code></strong> : <em><code>&lt;class 'NoneType'&gt;</code></em>, <em>optional</em></p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, let's view a batch of data with the <code>show_batch</code> function:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Input</th>
      <th>Label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>finished program or will Willie wring Al's neck?&lt;br /&gt;&lt;br /&gt;The three leads, Matthau, Burns, and Benjamin, do very well with the one-liners, frequently reminiscent of vaudeville patter (example: "Chest pains...I'm getting chest pains Uncle Willie. Every Thursday I come here and get chest pains!" "So, come on Fridays!"). Benjamin strives to prove his deep affection for his uncle, although Matthau's rough outer shell makes it difficult (he only smooths down when he discusses the glory days of vaudeville). Matthau has a little better grasp on reality (at first) than Burns, who seems senile by his repeating himself - but in actuality Matthau's sense of rejection by the world that once applauded him make him less willing to behave properly. Burns is not senile - he takes things slowly. But he seems far happier in accepting his retirement.&lt;br /&gt;&lt;br /&gt;I call this a final "Voyage of Discovery" for our modern Lewis and Clark. Al and Willie transcend their old skits, as they gradually end up realizing that they have more in common in their old age than they thought. Even the irascible Willie admits that Al may be (to him) a pain in the ass, but he was a funny man.&lt;br /&gt;&lt;br /&gt;Burns was not the original choice for the part of "Al Lewis" (supposedly Dale of the team Smith and Dale). Jack Benny was. Benny probably would have done a good job, but ill-health forced him out (he died in 1975). Burns (whose last involvement in any film was in THE SOLID GOLD CADILLAC in 1956 as the narrator) turned in such a fine performance that he got the "Oscar" for best supporting actor, and was to have a career in movies in the next decade in such films as OH GOD!; OH GOD, YOU DEVIL; and GOING IN STYLE. He died in 1996 age 100, having proved that he was more than just a brilliant straight man for his wife Gracie Allan.As far as cinematography goes, this film was pretty good for the mid 50's. There were a few times that the lighting was way too hot but the shots were generally in frame and stayed in focus. The acting was above average for a low budget stinker but the direction was horrible. Several scenes were dragged out way too long in an attempt at suspense and the effects were non-existent</td>
      <td>finished program or will Willie wring Al's neck?&lt;br /&gt;&lt;br /&gt;The three leads, Matthau, Burns, and Benjamin, do very well with the one-liners, frequently reminiscent of vaudeville patter (example: "Chest pains...I'm getting chest pains Uncle Willie. Every Thursday I come here and get chest pains!" "So, come on Fridays!"). Benjamin strives to prove his deep affection for his uncle, although Matthau's rough outer shell makes it difficult (he only smooths down when he discusses the glory days of vaudeville). Matthau has a little better grasp on reality (at first) than Burns, who seems senile by his repeating himself - but in actuality Matthau's sense of rejection by the world that once applauded him make him less willing to behave properly. Burns is not senile - he takes things slowly. But he seems far happier in accepting his retirement.&lt;br /&gt;&lt;br /&gt;I call this a final "Voyage of Discovery" for our modern Lewis and Clark. Al and Willie transcend their old skits, as they gradually end up realizing that they have more in common in their old age than they thought. Even the irascible Willie admits that Al may be (to him) a pain in the ass, but he was a funny man.&lt;br /&gt;&lt;br /&gt;Burns was not the original choice for the part of "Al Lewis" (supposedly Dale of the team Smith and Dale). Jack Benny was. Benny probably would have done a good job, but ill-health forced him out (he died in 1975). Burns (whose last involvement in any film was in THE SOLID GOLD CADILLAC in 1956 as the narrator) turned in such a fine performance that he got the "Oscar" for best supporting actor, and was to have a career in movies in the next decade in such films as OH GOD!; OH GOD, YOU DEVIL; and GOING IN STYLE. He died in 1996 age 100, having proved that he was more than just a brilliant straight man for his wife Gracie Allan.As far as cinematography goes, this film was pretty good for the mid 50's. There were a few times that the lighting was way too hot but the shots were generally in frame and stayed in focus. The acting was above average for a low budget stinker but the direction was horrible. Several scenes were dragged out way too long in an attempt at suspense and the effects were non-existent</td>
    </tr>
    <tr>
      <th>1</th>
      <td>, how could I not recommend it?&lt;br /&gt;&lt;br /&gt;War,Inc.:3.5/5In her autobiography,Laureen Bacall reveals that Bogie told her that she should not make such dud movies as this one or something like that.At the time,Douglas Sirk was labeled "weepies for women",actually,he was restored to favor,at least in Europa,after he stopped directing.And when he filmed "written on the wind",Sirk had only three movies to make:"tarnished Angels","A time to love and a time to die",his masterpiece,IMHO,and finally" Imitation of life"(1960).Then there was silence. Actually Bacall and Hudson characters do not interest Sirk.They are too straight,too virtuous.Dorothy Malone -who was some kind of substitute for his former German star Zarah Leander-and her brother Robert Stack provide the main interest of the plot.A plot constructed continuously,most of the movie being a long flashback.The instability of the brother and the sister,from a family of rich Texan oil owners,is brought to the fore by garish clothes,and rutilant cars that go at top speed in a derricks landscape. Malone's metamorphosis at the end of the movie is stunning :suit and chignon,toying with a small derrick:she's ready for life,the rebel is tamed. Now alone,because she's lost Hudson (but anyway,he was not in love with her).This end is a bit reactionary,but melodrama is par excellence reactionary;three years later,in "imitation of life",Sarah-Jane (Susan Kohner) will be blamed because she does not know her place.Over Her Dead Body was a nice little movie.It was decent and entertaining, while still being pretty funny.There were a few cliché's, but I found most stuff fresh.At first I didn't think it was going to be good at all,when it started out.If you can get past the first 20 minutes though,the movie starts getting more interesting.This film wasn't burst out in laughter hilarious,and wasn't OH MY GOSH wonderful.It was just a movie that you can sit down and enjoy for how enjoyable it was.I don't see how this movie was bad.It's rating is just a bit too low.I could've dealt with a 5.5,but a 4.8?</td>
      <td>, how could I not recommend it?&lt;br /&gt;&lt;br /&gt;War,Inc.:3.5/5In her autobiography,Laureen Bacall reveals that Bogie told her that she should not make such dud movies as this one or something like that.At the time,Douglas Sirk was labeled "weepies for women",actually,he was restored to favor,at least in Europa,after he stopped directing.And when he filmed "written on the wind",Sirk had only three movies to make:"tarnished Angels","A time to love and a time to die",his masterpiece,IMHO,and finally" Imitation of life"(1960).Then there was silence. Actually Bacall and Hudson characters do not interest Sirk.They are too straight,too virtuous.Dorothy Malone -who was some kind of substitute for his former German star Zarah Leander-and her brother Robert Stack provide the main interest of the plot.A plot constructed continuously,most of the movie being a long flashback.The instability of the brother and the sister,from a family of rich Texan oil owners,is brought to the fore by garish clothes,and rutilant cars that go at top speed in a derricks landscape. Malone's metamorphosis at the end of the movie is stunning :suit and chignon,toying with a small derrick:she's ready for life,the rebel is tamed. Now alone,because she's lost Hudson (but anyway,he was not in love with her).This end is a bit reactionary,but melodrama is par excellence reactionary;three years later,in "imitation of life",Sarah-Jane (Susan Kohner) will be blamed because she does not know her place.Over Her Dead Body was a nice little movie.It was decent and entertaining, while still being pretty funny.There were a few cliché's, but I found most stuff fresh.At first I didn't think it was going to be good at all,when it started out.If you can get past the first 20 minutes though,the movie starts getting more interesting.This film wasn't burst out in laughter hilarious,and wasn't OH MY GOSH wonderful.It was just a movie that you can sit down and enjoy for how enjoyable it was.I don't see how this movie was bad.It's rating is just a bit too low.I could've dealt with a 5.5,but a 4.8?</td>
    </tr>
    <tr>
      <th>2</th>
      <td>deliver a $6million ransom, brave mercenary Peter Weston (Al Cliver) and his Vietnam vet pilot pal travel to the island, but encounter trouble when the bad guys attempt a double-cross. During the confusion, Laura escapes into the jungle, but runs straight into the arms of the island's natives, who offer her up to their god.&lt;br /&gt;&lt;br /&gt;Franco directs in his usual torpid style and loads this laughable effort with his usual dreadful trademarks: crap gore, murky cinematography, rapid zooms, numerous crotch shots, out of focus imagery, awful sound effects, and ham-fisted editing. The result is a dire mess that is a real struggle to sit through from start to finish (It took me a couple of sittings to finish the thing), and even the sight of the luscious Buchfellner in all of her natural glory ain't enough to make me revisit this film in a hurry.You know a movie will not go well when John Carradine narrates (a.k.a. reads the script &amp; plot synopsis) over his character's funeral procession, a mere 5 minutes into the movie. The narration is his character's last will &amp; testament. It stipulates that his estate be divided amongst his 4 children and servants. The children shall split $136 million equally, but if any should die then that share is split amongst the remainders. If all the children should die then it is divided amongst the servants. To be eligible, they must live in the family estate for a week. It sounds like the typical plot of a reality show.&lt;br /&gt;&lt;br /&gt;There is little subtext as to the nature of the Deans. They are a powerful and severely dysfunctional family, but the real trouble starts with the drowning of that dog. From the opening voice-over by John Carradine you expect this movie will lead to a Machiavellian cat and mouse game with a twist ending. &lt;br /&gt;&lt;br /&gt;That journey is painfully slow and pointless. We trudge through minutes of watching people sitting around, playing pool, throwing darts, the misuse of the "through the fish bowl" shot, dramatic conversations between silk cravat wearing men, constant bickering, misplaced circus music, bizarre flashbacks reminiscent of faux-German expressionism, the horror aesthetic of the 4th grade and heaps of dramatic overacting. This all inevitably leads to the expected &amp; ungratifying ending. You will be happy to still be alive, but the pain might</td>
      <td>deliver a $6million ransom, brave mercenary Peter Weston (Al Cliver) and his Vietnam vet pilot pal travel to the island, but encounter trouble when the bad guys attempt a double-cross. During the confusion, Laura escapes into the jungle, but runs straight into the arms of the island's natives, who offer her up to their god.&lt;br /&gt;&lt;br /&gt;Franco directs in his usual torpid style and loads this laughable effort with his usual dreadful trademarks: crap gore, murky cinematography, rapid zooms, numerous crotch shots, out of focus imagery, awful sound effects, and ham-fisted editing. The result is a dire mess that is a real struggle to sit through from start to finish (It took me a couple of sittings to finish the thing), and even the sight of the luscious Buchfellner in all of her natural glory ain't enough to make me revisit this film in a hurry.You know a movie will not go well when John Carradine narrates (a.k.a. reads the script &amp; plot synopsis) over his character's funeral procession, a mere 5 minutes into the movie. The narration is his character's last will &amp; testament. It stipulates that his estate be divided amongst his 4 children and servants. The children shall split $136 million equally, but if any should die then that share is split amongst the remainders. If all the children should die then it is divided amongst the servants. To be eligible, they must live in the family estate for a week. It sounds like the typical plot of a reality show.&lt;br /&gt;&lt;br /&gt;There is little subtext as to the nature of the Deans. They are a powerful and severely dysfunctional family, but the real trouble starts with the drowning of that dog. From the opening voice-over by John Carradine you expect this movie will lead to a Machiavellian cat and mouse game with a twist ending. &lt;br /&gt;&lt;br /&gt;That journey is painfully slow and pointless. We trudge through minutes of watching people sitting around, playing pool, throwing darts, the misuse of the "through the fish bowl" shot, dramatic conversations between silk cravat wearing men, constant bickering, misplaced circus music, bizarre flashbacks reminiscent of faux-German expressionism, the horror aesthetic of the 4th grade and heaps of dramatic overacting. This all inevitably leads to the expected &amp; ungratifying ending. You will be happy to still be alive, but the pain might</td>
    </tr>
    <tr>
      <th>3</th>
      <td>very much into "that sort of thing," I watched this movie with some anticipation of being informed, changed, moved, altered, uplifted, and all the other positive mystical things that could happen to me when I suddenly see The Truth. Now this may sound like someone who is already predisposed to poo-pooing anything dealing with the metaphysical, the metaphysical/physical boundaries of existence. Believe me, I am not such a person. I try to be open about any presentation and then decide accordingly.&lt;br /&gt;&lt;br /&gt;In terms of content, the only thing I found mildly interesting and informative, was the bit about peptides, emotions, addiction, and cellular receptors. That was the only "unifying" element I could find in the documentary part of this film. The rest of the documentary rambled around several topics and never seemed to unify and cohere, try to tie up and conclude to a point. And what was all that stuff about native Americans not being able to see the ships that Columbus came in? Who told the "authorities" in this film that that was what happened in 1492? Where they there too? Had they compared this to scientific work being done in visual cognition (the famous gorilla video, for example, visit the Visual Cognition Lab at the University of Illinois site) there may have been a more convincing point made. Here, however, it seemed like unsupported mystical mumbo-jumbo.&lt;br /&gt;&lt;br /&gt;As a film: this wasn't one film, it was two. I found the documentary part mildly interesting, just to hear the people talking about what they were talking about (I was annoyed that their credentials weren't presented at the bottom of the screen when they spoke, at least initially!) But I found the "story" part of the movie with Matlin in it annoying, disjointed, intrusive, non-related and downright stupid. That bit about the Polish wedding with that dance was not in the least bit funny. It was laughable, ludicrous, sophomoric, and stupid. And I found the use of the word "Pollack" offensive. It just seemed so out of place and wrong. Is such usage okay because a member of the group uses a pejorative term to refer to the group because he or she is a member of the group? That may be okay to make a point, but it didn't seem to be used that way here. And in any case, I don't care what the reason, it offended me,</td>
      <td>very much into "that sort of thing," I watched this movie with some anticipation of being informed, changed, moved, altered, uplifted, and all the other positive mystical things that could happen to me when I suddenly see The Truth. Now this may sound like someone who is already predisposed to poo-pooing anything dealing with the metaphysical, the metaphysical/physical boundaries of existence. Believe me, I am not such a person. I try to be open about any presentation and then decide accordingly.&lt;br /&gt;&lt;br /&gt;In terms of content, the only thing I found mildly interesting and informative, was the bit about peptides, emotions, addiction, and cellular receptors. That was the only "unifying" element I could find in the documentary part of this film. The rest of the documentary rambled around several topics and never seemed to unify and cohere, try to tie up and conclude to a point. And what was all that stuff about native Americans not being able to see the ships that Columbus came in? Who told the "authorities" in this film that that was what happened in 1492? Where they there too? Had they compared this to scientific work being done in visual cognition (the famous gorilla video, for example, visit the Visual Cognition Lab at the University of Illinois site) there may have been a more convincing point made. Here, however, it seemed like unsupported mystical mumbo-jumbo.&lt;br /&gt;&lt;br /&gt;As a film: this wasn't one film, it was two. I found the documentary part mildly interesting, just to hear the people talking about what they were talking about (I was annoyed that their credentials weren't presented at the bottom of the screen when they spoke, at least initially!) But I found the "story" part of the movie with Matlin in it annoying, disjointed, intrusive, non-related and downright stupid. That bit about the Polish wedding with that dance was not in the least bit funny. It was laughable, ludicrous, sophomoric, and stupid. And I found the use of the word "Pollack" offensive. It just seemed so out of place and wrong. Is such usage okay because a member of the group uses a pejorative term to refer to the group because he or she is a member of the group? That may be okay to make a point, but it didn't seem to be used that way here. And in any case, I don't care what the reason, it offended me,</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Jones and The Temple of Doom". Unfortunately I was in for a virtual yawn. Not like any yawn i have had before though. This yawn was so large that i could barely find anything of quality in this movie. The cover described amazing special effects. There were none. The movie was so lightweight that even the stereotypes were awfully portrayed. It does give the idea that you can solve problems with violence. Good if you want to teach your kids that. I don't. Keep away from this one. If you are looking for family entertainment then you might find something that is more inspiring elsewhere.TV does influence society...just look at the surge in popularity of cappucino shops after this shallow little piece of work debuted. Besides, real people who look as good as these people do don't have any problems.&lt;br /&gt;&lt;br /&gt;Besides, does anyone really believe that these people can afford to live in a nice Manhattan loft considering what they do for a living? NBC just loves to insult the viewer's intelligence, even if they're just around Gump's level. I know a person who makes $100,000 a year as a web designer and lives in a tiny one-bedroom apartment in Manhattan that costs $2200 a month in rent. &lt;br /&gt;&lt;br /&gt;I'd like to see a show called Phriends, where it's six ugly nobodies in dead-end jobs, living in a crummy neighborhood where sirens constantly wail and someone gets mugged every week...and then the landlord jacks up the rent. Now THAT I would watch.Director / writer Michael Winner's feature is a better than expected offbeat supernatural horror film (although still schlock efficiently catered for), which really does by go unnoticed. Sure it might borrow ideas from other similar themed horror movies of this period, but still manages to bring its own psychological imprint to the smokescreen material (of good vs. evil) and a unique vision that has a fair share of impressively expansive, if somewhat exploitative set-pieces. As a whole it's sketchy, however remains intriguing by instilling an ominous charge without going gang-busters with the scares. Actually there's always something going on amongst its busy framework, but it's rather down-played with its shocks steering to soapy patterns and atmospheric tailoring, up until its vividly repellent and grisly climax with a downbeat revelation. Winner's dressed up craftsmanship might feel pedestrian, however it's the ensemble cast that really holds it together as you try</td>
      <td>Jones and The Temple of Doom". Unfortunately I was in for a virtual yawn. Not like any yawn i have had before though. This yawn was so large that i could barely find anything of quality in this movie. The cover described amazing special effects. There were none. The movie was so lightweight that even the stereotypes were awfully portrayed. It does give the idea that you can solve problems with violence. Good if you want to teach your kids that. I don't. Keep away from this one. If you are looking for family entertainment then you might find something that is more inspiring elsewhere.TV does influence society...just look at the surge in popularity of cappucino shops after this shallow little piece of work debuted. Besides, real people who look as good as these people do don't have any problems.&lt;br /&gt;&lt;br /&gt;Besides, does anyone really believe that these people can afford to live in a nice Manhattan loft considering what they do for a living? NBC just loves to insult the viewer's intelligence, even if they're just around Gump's level. I know a person who makes $100,000 a year as a web designer and lives in a tiny one-bedroom apartment in Manhattan that costs $2200 a month in rent. &lt;br /&gt;&lt;br /&gt;I'd like to see a show called Phriends, where it's six ugly nobodies in dead-end jobs, living in a crummy neighborhood where sirens constantly wail and someone gets mugged every week...and then the landlord jacks up the rent. Now THAT I would watch.Director / writer Michael Winner's feature is a better than expected offbeat supernatural horror film (although still schlock efficiently catered for), which really does by go unnoticed. Sure it might borrow ideas from other similar themed horror movies of this period, but still manages to bring its own psychological imprint to the smokescreen material (of good vs. evil) and a unique vision that has a fair share of impressively expansive, if somewhat exploitative set-pieces. As a whole it's sketchy, however remains intriguing by instilling an ominous charge without going gang-busters with the scares. Actually there's always something going on amongst its busy framework, but it's rather down-played with its shocks steering to soapy patterns and atmospheric tailoring, up until its vividly repellent and grisly climax with a downbeat revelation. Winner's dressed up craftsmanship might feel pedestrian, however it's the ensemble cast that really holds it together as you try</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When training a language model, the input and output are made to be the exact same, so there isn't a shown noticable difference here.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Building-Tuner">Building <code>Tuner</code><a class="anchor-link" href="#Building-Tuner"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we need to build a compatible <code>Tuner</code> for our problem. These tuners contain good defaults for our problem space, including loss functions and metrics.</p>
<p>First let's import the <a href="/adaptnlp/training.language_model.html#LanguageModelTuner"><code>LanguageModelTuner</code></a> and view it's documentation</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">LanguageModelTuner</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LanguageModelTuner" class="doc_header"><code>class</code> <code>LanguageModelTuner</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L228" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LanguageModelTuner</code>(<strong><code>dls</code></strong>:<code>DataLoaders</code>, <strong><code>model_name</code></strong>, <strong><code>tokenizer</code></strong>=<em><code>None</code></em>, <strong><code>language_model_type</code></strong>:<code>LMType</code>=<em><code>'causal'</code></em>, <strong><code>loss_func</code></strong>=<em><code>CrossEntropyLoss()</code></em>, <strong><code>metrics</code></strong>=<em><code>[&lt;fastai.metrics.Perplexity object at 0x7f7f0937e3d0&gt;]</code></em>, <strong><code>opt_func</code></strong>=<em><code>Adam</code></em>, <strong><code>additional_cbs</code></strong>=<em><code>None</code></em>, <strong><code>expose_fastai_api</code></strong>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/adaptnlp/training.core.html#AdaptiveTuner"><code>AdaptiveTuner</code></a></p>
</blockquote>
<p>An <a href="/adaptnlp/training.core.html#AdaptiveTuner"><code>AdaptiveTuner</code></a> with good defaults for Language Model fine-tuning
<strong>Valid kwargs and defaults:</strong></p>
<ul>
<li><code>lr</code>:float = 0.001</li>
<li><code>splitter</code>:function = <code>trainable_params</code></li>
<li><code>cbs</code>:list = None</li>
<li><code>path</code>:Path = None</li>
<li><code>model_dir</code>:Path = 'models'</li>
<li><code>wd</code>:float = None</li>
<li><code>wd_bn_bias</code>:bool = False</li>
<li><code>train_bn</code>:bool = True</li>
<li><code>moms</code>: tuple(float) = (0.95, 0.85, 0.95)</li>
</ul>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>dls</code></strong> : <em><code>&lt;class 'fastai.data.core.DataLoaders'&gt;</code></em>   <p>A set of DataLoaders or AdaptiveDataLoaders</p></li>
</ul>
<ul>
<li><strong><code>model_name</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em>  <p>A HuggingFace model</p></li>
</ul>
<ul>
<li><strong><code>tokenizer</code></strong> : <em><code>&lt;class 'NoneType'&gt;</code></em>, <em>optional</em> <p>A HuggingFace tokenizer</p></li>
</ul>
<ul>
<li><strong><code>language_model_type</code></strong> : <em><code>&lt;class 'fastcore.basics.LMType'&gt;</code></em>, <em>optional</em> <p>The type of language model to use</p></li>
</ul>
<ul>
<li><strong><code>loss_func</code></strong> : <em><code>&lt;class 'fastai.losses.CrossEntropyLossFlat'&gt;</code></em>, <em>optional</em>   <p>A loss function</p></li>
</ul>
<ul>
<li><strong><code>metrics</code></strong> : <em><code>&lt;class 'list'&gt;</code></em>, <em>optional</em>   <p>Metrics to monitor the training with</p></li>
</ul>
<ul>
<li><strong><code>opt_func</code></strong> : <em><code>&lt;class 'function'&gt;</code></em>, <em>optional</em>  <p>A fastai or torch Optimizer</p></li>
</ul>
<ul>
<li><strong><code>additional_cbs</code></strong> : <em><code>&lt;class 'NoneType'&gt;</code></em>, <em>optional</em>    <p>Additional Callbacks to have always tied to the Tuner,</p></li>
</ul>
<ul>
<li><strong><code>expose_fastai_api</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em> <p>Whether to expose the fastai API</p></li>
</ul>
<ul>
<li><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we'll pass in our <code>DataLoaders</code>, the name of our model, and the tokenizer:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='If you are not using the data API (<a href="/adaptnlp/training.core.html#TaskDatasets"><code>TaskDatasets</code></a>, <a href="/adaptnlp/training.sequence_classification.html#SequenceClassificationDatasets"><code>SequenceClassificationDatasets</code></a>, etc), you need to pass in the tokenizer to the constructor as well with <code>tokenizer=tokenizer</code>' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span> <span class="o">=</span> <span class="n">LanguageModelTuner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">dls</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By default we can see that it used <code>CrossEntropyLoss</code> as our loss function, and <code>Perplexity</code> as our metric</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">loss_func</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>FlattenedLoss of CrossEntropyLoss()</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">tuner</span><span class="o">.</span><span class="n">metrics</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>perplexity
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally we just need to train our model!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Fine-Tuning">Fine-Tuning<a class="anchor-link" href="#Fine-Tuning"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To fine-tune, AdaptNLP's tuner class provides only a few functions to work with. The important ones are the <code>tune</code> and <code>lr_find</code> class.</p>
<p>As the <code>Tuner</code> uses <code>fastai</code> under the hood, <code>lr_find</code> calls fastai's Learning Rate Finder to help us pick a learning rate. Let's do that now:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="AdaptiveTuner.lr_find" class="doc_header"><code>AdaptiveTuner.lr_find</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/core.py#L389" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>AdaptiveTuner.lr_find</code>(<strong><code>start_lr</code></strong>=<em><code>1e-07</code></em>, <strong><code>end_lr</code></strong>=<em><code>10</code></em>, <strong><code>num_it</code></strong>=<em><code>100</code></em>, <strong><code>stop_div</code></strong>=<em><code>True</code></em>, <strong><code>show_plot</code></strong>=<em><code>True</code></em>, <strong><code>suggest_funcs</code></strong>=<em><code>valley</code></em>)</p>
</blockquote>
<p>Runs fastai's <code>LR Finder</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><p><strong><code>start_lr</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>end_lr</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>num_it</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>stop_div</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>show_plot</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>suggest_funcs</code></strong> : <em><code>&lt;class 'function'&gt;</code></em>, <em>optional</em></p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/venv/lib/python3.8/site-packages/fastai/callback/schedule.py:270: UserWarning: color is redundantly defined by the &#39;color&#39; keyword argument and the fmt string &#34;ro&#34; (-&gt; color=&#39;r&#39;). The keyword argument will take precedence.
  ax.plot(val, idx, &#39;ro&#39;, label=nm, c=color)
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(valley=7.585775892948732e-05)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It recommends a learning rate of around 5e-5, so we will use that.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">5e-5</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's look at the documentation for <code>tune</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="AdaptiveTuner.tune" class="doc_header"><code>AdaptiveTuner.tune</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/core.py#L375" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>AdaptiveTuner.tune</code>(<strong><code>epochs</code></strong>:<code>int</code>, <strong><code>lr</code></strong>:<code>float</code>=<em><code>None</code></em>, <strong><code>strategy</code></strong>:<code>Strategy</code>=<em><code>'fit_one_cycle'</code></em>, <strong><code>callbacks</code></strong>:<code>list</code>=<em><code>[]</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Fine tune <code>self.model</code> for <code>epochs</code> with an <code>lr</code> and <code>strategy</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>epochs</code></strong> : <em><code>&lt;class 'int'&gt;</code></em> <p>Number of iterations to train for</p></li>
</ul>
<ul>
<li><strong><code>lr</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em>   <p>If None, finds a new learning rate and uses suggestion_method</p></li>
</ul>
<ul>
<li><strong><code>strategy</code></strong> : <em><code>&lt;class 'fastcore.basics.Strategy'&gt;</code></em>, <em>optional</em>  <p>A fitting method</p></li>
</ul>
<ul>
<li><strong><code>callbacks</code></strong> : <em><code>&lt;class 'list'&gt;</code></em>, <em>optional</em> <p>Extra fastai Callbacks</p></li>
</ul>
<ul>
<li><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can pass in a number of epochs, a learning rate, a strategy, and additional fastai callbacks to call.</p>
<p>Valid strategies live in the <code>Strategy</code> namespace class, and consist of:</p>
<ul>
<li>OneCycle (Also called the <a href="https://docs.fast.ai/callback.schedule.html#Learner.fit_one_cycle">One-Cycle Policy</a>)</li>
<li><a href="https://docs.fast.ai/callback.schedule.html#Learner.fit_flat_cos">CosineAnnealing</a></li>
<li><a href="https://docs.fast.ai/callback.schedule.html#Learner.fit_sgdr">SGDR</a></li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">Strategy</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this tutorial we will train with the One-Cycle policy, as currently it is one of the best schedulers to use.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">tune</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">Strategy</span><span class="o">.</span><span class="n">OneCycle</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Saving-Model">Saving Model<a class="anchor-link" href="#Saving-Model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have a trained model, let's save those weights away.</p>
<p>Calling <code>tuner.save</code> will save both the model and the tokenizer in the same format as how HuggingFace does:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="AdaptiveTuner.save" class="doc_header"><code>AdaptiveTuner.save</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/core.py#L397" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>AdaptiveTuner.save</code>(<strong><code>save_directory</code></strong>)</p>
</blockquote>
<p>Save a pretrained model to a <code>save_directory</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>save_directory</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em>  <p>A folder to save our model to</p></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;good_model&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;good_model&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Performing-Inference">Performing Inference<a class="anchor-link" href="#Performing-Inference"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are two ways to get predictions, the first is with the <code>.predict</code> method in our <code>tuner</code>. This is great for if you just finished training and want to see how your model performs on some new data!
The other method is with AdaptNLP's inference API, which we will show afterwards</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="In-Tuner">In Tuner<a class="anchor-link" href="#In-Tuner"> </a></h3><p>First let's write a sentence to test with</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;Hugh Jackman is a terrible &quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And then predict with it:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LanguageModelTuner.predict" class="doc_header"><code>LanguageModelTuner.predict</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/language_model.py#L293" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LanguageModelTuner.predict</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>str</code>], <code>str</code>], <strong><code>bs</code></strong>:<code>int</code>=<em><code>64</code></em>, <strong><code>num_tokens_to_produce</code></strong>:<code>int</code>=<em><code>50</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Predict some <code>text</code> for sequence classification with the currently loaded model</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>text</code></strong> : <em><code>typing.Union[typing.List[str], str]</code></em> <p>Some text or list of texts to do inference with</p></li>
</ul>
<ul>
<li><strong><code>bs</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>A batch size to use for multiple texts</p></li>
</ul>
<ul>
<li><strong><code>num_tokens_to_produce</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em>  <p>Number of tokens to generate</p></li>
</ul>
<ul>
<li><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">num_tokens_to_produce</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [1/1 00:00<00:00]
    </div>
    
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;generated_text&#39;: [&#39;Hugh Jackman is a terrible icky, and very funny, character.&#39;]}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="With-the-Inference-API">With the Inference API<a class="anchor-link" href="#With-the-Inference-API"> </a></h3><p>Next we will use the <a href="/adaptnlp/text_generation.html#EasyTextGenerator"><code>EasyTextGenerator</code></a> class, which AdaptNLP offers:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">adaptnlp</span> <span class="kn">import</span> <span class="n">EasyTextGenerator</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We simply construct the class:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">EasyTextGenerator</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And call the <code>tag_text</code> method, passing in the sentence, the location of our saved model, and some names for our classes:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">model_name_or_path</span><span class="o">=</span><span class="s1">&#39;good_model&#39;</span><span class="p">,</span>
    <span class="n">num_tokens_to_produce</span><span class="o">=</span><span class="mi">8</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [1/1 00:00<00:00]
    </div>
    
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;generated_text&#39;: [&#39;Hugh Jackman is a terrible icky, and very funny, character.&#39;]}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we got the exact same output!</p>

</div>
</div>
</div>
</div>
 

