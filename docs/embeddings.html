---

title: Embeddings


keywords: fastai
sidebar: home_sidebar

summary: "AdaptNLP Embeddings Module"
description: "AdaptNLP Embeddings Module"
nb_path: "nbs/04_embeddings.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/04_embeddings.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="EasyWordEmbeddings" class="doc_header"><code>class</code> <code>EasyWordEmbeddings</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/embeddings.py#L77" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>EasyWordEmbeddings</code>()</p>
</blockquote>
<p>Word embeddings from the latest language models</p>
<p>Usage:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="n">adaptnlp</span><span class="o">.</span><span class="n">EasyWordEmbeddings</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">embed_text</span><span class="p">(</span><span class="s2">&quot;text you want embeddings for&quot;</span><span class="p">,</span> <span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>
</pre></div>
<p><strong>Usage Examples:</strong></p>
<details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/04a_tutorial.embeddings.ipynb>Source</a></b>
```python
embeddings = EasyWordEmbeddings()
sentences = embeddings.embed_text(example_text, model_name_or_path=model)
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="EasyWordEmbeddings.embed_text" class="doc_header"><code>EasyWordEmbeddings.embed_text</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/embeddings.py#L91" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>EasyWordEmbeddings.embed_text</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>Sentence</code>], <code>Sentence</code>, <code>List</code>[<code>str</code>], <code>str</code>], <strong><code>model_name_or_path</code></strong>:<code>Union</code>[<code>str</code>, <a href="/adaptnlp/model_hub.html#HFModelResult"><code>HFModelResult</code></a>, <a href="/adaptnlp/model_hub.html#FlairModelResult"><code>FlairModelResult</code></a>]=<em><code>'bert-base-cased'</code></em>)</p>
</blockquote>
<p>Produces embeddings for text</p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><code>text</code> - Text input, it can be a string or any of Flair's <code>Sentence</code> input formats</li>
<li><code>model_name_or_path</code> - The hosted model name key, model path, or an instance of either <a href="/adaptnlp/model_hub.html#HFModelResult"><code>HFModelResult</code></a> or <a href="/adaptnlp/model_hub.html#FlairModelResult"><code>FlairModelResult</code></a></li>
</ul>
<p><strong>Return</strong>:</p>
<ul>
<li>A list of Flair's <code>Sentence</code>s</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/04a_tutorial.embeddings.ipynb>Source</a></b>
```python
embeddings = EasyWordEmbeddings()
sentences = embeddings.embed_text(example_text, model_name_or_path=model)
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="EasyWordEmbeddings.embed_all" class="doc_header"><code>EasyWordEmbeddings.embed_all</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/embeddings.py#L113" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>EasyWordEmbeddings.embed_all</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>Sentence</code>], <code>Sentence</code>, <code>List</code>[<code>str</code>], <code>str</code>], <strong>*<code>model_names_or_paths</code></strong>:<code>str</code>)</p>
</blockquote>
<p>Embeds text with all embedding models loaded</p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><code>text</code> - Text input, it can be a string or any of Flair's <code>Sentence</code> input formats</li>
<li><code>model_names_or_paths</code> -  A variable input of model names or paths to embed</li>
</ul>
<p><strong>Return</strong>:</p>
<ul>
<li>A list of Flair's <code>Sentence</code>s</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="EasyStackedEmbeddings" class="doc_header"><code>class</code> <code>EasyStackedEmbeddings</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/embeddings.py#L143" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>EasyStackedEmbeddings</code>(<strong>*<code>embeddings</code></strong>:<code>str</code>)</p>
</blockquote>
<p>Word Embeddings that have been concatenated and "stacked" as specified by flair</p>
<p>Usage:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="n">adaptnlp</span><span class="o">.</span><span class="n">EasyStackedEmbeddings</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt2&quot;</span><span class="p">,</span> <span class="s2">&quot;xlnet-base-cased&quot;</span><span class="p">)</span>
</pre></div>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>&amp;ast;embeddings</code> - Non-keyword variable number of strings specifying the embeddings you want to stack</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/04_embeddings.ipynb>Source</a></b>
```python
#hide
embeddings = EasyStackedEmbeddings("bert-base-cased", "xlnet-base-cased")
sentences = embeddings.embed_text("This is Albert.  My last name is Einstein.  I like physics and atoms.")
test_eq(sentences[0][0].get_embedding().shape, torch.Size([1536]))
```
</details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/04a_tutorial.embeddings.ipynb>Source</a></b>
```python
embeddings = EasyStackedEmbeddings("bert-base-cased", "distilbert-base-cased")
And then generate our stacked word embeddings through our `embed_text` function:
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="EasyStackedEmbeddings.embed_text" class="doc_header"><code>EasyStackedEmbeddings.embed_text</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/embeddings.py#L168" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>EasyStackedEmbeddings.embed_text</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>Sentence</code>], <code>Sentence</code>, <code>List</code>[<code>str</code>], <code>str</code>])</p>
</blockquote>
<p>Stacked embeddings</p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><code>text</code> - Text input, it can be a string or any of Flair's <code>Sentence</code> input formats</li>
</ul>
<p><strong>Return</strong>:</p>
<ul>
<li>A list of Flair's <code>Sentence</code>s</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/04_embeddings.ipynb>Source</a></b>
```python
#hide
embeddings = EasyStackedEmbeddings("bert-base-cased", "xlnet-base-cased")
sentences = embeddings.embed_text("This is Albert.  My last name is Einstein.  I like physics and atoms.")
test_eq(sentences[0][0].get_embedding().shape, torch.Size([1536]))
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="EasyDocumentEmbeddings" class="doc_header"><code>class</code> <code>EasyDocumentEmbeddings</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/embeddings.py#L188" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>EasyDocumentEmbeddings</code>(<strong>*<code>embeddings</code></strong>:<code>str</code>, <strong><code>methods</code></strong>:<code>List</code>[<code>str</code>]=<em><code>['rnn', 'pool']</code></em>, <strong><code>configs</code></strong>:<code>Dict</code>[<code>KT</code>, <code>VT</code>]=<em><code>{'pool_configs': {'fine_tune_mode': 'linear', 'pooling': 'mean'}, 'rnn_configs': {'hidden_size': 512, 'rnn_layers': 1, 'reproject_words': True, 'reproject_words_dimension': 256, 'bidirectional': False, 'dropout': 0.5, 'word_dropout': 0.0, 'locked_dropout': 0.0, 'rnn_type': 'GRU', 'fine_tune': True}}</code></em>)</p>
</blockquote>
<p>Document Embeddings generated by pool and rnn methods applied to the word embeddings of text</p>
<p>Usage:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="n">adaptnlp</span><span class="o">.</span><span class="n">EasyDocumentEmbeddings</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">,</span> <span class="s2">&quot;xlnet-base-cased&quot;</span><span class="p">,</span> <span class="n">methods</span><span class="p">[</span><span class="s2">&quot;rnn&quot;</span><span class="p">])</span>
</pre></div>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>&amp;ast;embeddings</code> - Non-keyword variable number of strings referring to model names or paths</li>
<li><code>methods</code> - A list of strings to specify which document embeddings to use i.e. ["rnn", "pool"] (avoids unncessary loading of models if only using one)</li>
<li><code>configs</code> - A dictionary of configurations for flair's rnn and pool document embeddings<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">example_configs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;pool_configs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;fine_tune_mode&quot;</span><span class="p">:</span> <span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;pooling&quot;</span><span class="p">:</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="p">},</span>
<span class="o">...</span>                   <span class="s2">&quot;rnn_configs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
<span class="o">...</span>                                   <span class="s2">&quot;rnn_layers&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="o">...</span>                                   <span class="s2">&quot;reproject_words&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="o">...</span>                                   <span class="s2">&quot;reproject_words_dimension&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
<span class="o">...</span>                                   <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="o">...</span>                                   <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
<span class="o">...</span>                                   <span class="s2">&quot;word_dropout&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="o">...</span>                                   <span class="s2">&quot;locked_dropout&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="o">...</span>                                   <span class="s2">&quot;rnn_type&quot;</span><span class="p">:</span> <span class="s2">&quot;GRU&quot;</span><span class="p">,</span>
<span class="o">...</span>                                   <span class="s2">&quot;fine_tune&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="p">},</span>
<span class="o">...</span>                  <span class="p">}</span>
</pre></div>
</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/20c_tutorial.flair_seq_class_trainer.ipynb>Source</a></b>
```python
corpus = TREC_6() # Or path to directory of train.csv, test.csv, dev.csv files at "Path/to/data/directory" 
OUTPUT_DIR = "Path/to/model/output/directory"
FINETUNED_MODEL_DIR = "Path/to/finetuned/model/directory"
doc_embeddings = EasyDocumentEmbeddings(FINETUNED_MODEL_DIR, methods = ["rnn"])
sc_configs = {
              "corpus": corpus,
              "encoder": doc_embeddings,
              "column_name_map": {0: "text", 1: "label"},
              "corpus_in_memory": True,
              "predictive_head": "flair",
             }
sc_trainer = SequenceClassifierTrainer(**sc_configs)
```
</details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/04_embeddings.ipynb>Source</a></b>
```python
#hide
embeddings = EasyDocumentEmbeddings("bert-base-cased", "xlnet-base-cased")
text = embeddings.embed_pool("This is Albert.  My last name is Einstein.  I like physics and atoms.")
test_eq(text[0].get_embedding().shape, torch.Size([1536]))
#hide
text = embeddings.embed_rnn("This is Albert.  My last name is Einstein.  I like physics and atoms.")
test_eq(text[0].get_embedding().shape, torch.Size([512]))
```
</details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/20c_tutorial.flair_seq_class_trainer.ipynb>Source</a></b>
```python
corpus = TREC_6() # Or path to directory of train.csv, test.csv, dev.csv files at "Path/to/data/directory" 
OUTPUT_DIR = "Path/to/model/output/directory" 
doc_embeddings = EasyDocumentEmbeddings("bert-base-cased", methods = ["rnn"])
sc_configs = {
              "corpus": corpus,
              "encoder": doc_embeddings,
              "column_name_map": {0: "text", 1: "label"},
              "corpus_in_memory": True,
              "predictive_head": "flair",
             }
sc_trainer = SequenceClassifierTrainer(**sc_configs)
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="EasyDocumentEmbeddings.embed_pool" class="doc_header"><code>EasyDocumentEmbeddings.embed_pool</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/embeddings.py#L269" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>EasyDocumentEmbeddings.embed_pool</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>Sentence</code>], <code>Sentence</code>, <code>List</code>[<code>str</code>], <code>str</code>])</p>
</blockquote>
<p>Generate stacked embeddings with <code>DocumentPoolEmbeddings</code></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><code>text</code> - Text input, it can be a string or any of Flair's <code>Sentence</code> input formats</li>
</ul>
<p><strong>Return</strong>:</p>
<ul>
<li>A list of Flair's <code>Sentence</code>s</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/04_embeddings.ipynb>Source</a></b>
```python
#hide
embeddings = EasyDocumentEmbeddings("bert-base-cased", "xlnet-base-cased")
text = embeddings.embed_pool("This is Albert.  My last name is Einstein.  I like physics and atoms.")
test_eq(text[0].get_embedding().shape, torch.Size([1536]))
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="EasyDocumentEmbeddings.embed_rnn" class="doc_header"><code>EasyDocumentEmbeddings.embed_rnn</code><a href="https://github.com/novetta/adaptnlp/tree/master/adaptnlp/embeddings.py#L285" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>EasyDocumentEmbeddings.embed_rnn</code>(<strong><code>text</code></strong>:<code>Union</code>[<code>List</code>[<code>Sentence</code>], <code>Sentence</code>, <code>List</code>[<code>str</code>], <code>str</code>])</p>
</blockquote>
<p>Generate stacked embeddings with <code>DocumentRNNEmbeddings</code></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><code>text</code> - Text input, it can be a string or any of Flair's <code>Sentence</code> input formats</li>
</ul>
<p><strong>Return</strong>:</p>
<ul>
<li>A list of Flair's <code>Sentence</code>s</li>
</ul>
<p><strong>Usage Examples:</strong></p>
<details>
<b><a href=https://nbviewer.jupyter.org/github/novetta/adaptnlp/tree/master/nbs/04_embeddings.ipynb>Source</a></b>
```python
#hide
embeddings = EasyDocumentEmbeddings("bert-base-cased", "xlnet-base-cased")
text = embeddings.embed_pool("This is Albert.  My last name is Einstein.  I like physics and atoms.")
test_eq(text[0].get_embedding().shape, torch.Size([1536]))
#hide
text = embeddings.embed_rnn("This is Albert.  My last name is Einstein.  I like physics and atoms.")
test_eq(text[0].get_embedding().shape, torch.Size([512]))
```
</details>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

