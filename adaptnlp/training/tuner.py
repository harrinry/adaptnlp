# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/16_training.tuner.ipynb (unless otherwise specified).

__all__ = ['AdaptiveTuner', 'SequenceClassificationTuner']

# Cell
from fastai.learner import *
from fastai.data.all import *
from fastai.callback.all import *
from fastai.metrics import *
from fastai.losses import *
from fastai.optimizer import *
# NOTE: Placeholder imports, remove once we are ready for release

# Cell
class _AdaptiveLearner(Learner):
    """
    A base fastai `Learner` that overrides `_split` and `_do_one_batch` to
    have it work with HuggingFace datasets and models
    """
    def _split(self, b):
        "Assign `self.xb` to model input and labels"
        self.xb = b
        if 'labels' in b.keys(): self.yb = b['labels'].unsqueeze(0)

    def _do_one_batch(self):
        "Move a batch of data to a device, get predictions, calculate the loss, and perform backward pass"
        self.xb = {k:v.to(self.device) for k,v in self.xb.items()} # See if `to_device` fixes this
        self.yb = self.yb.to(self.device)
        out = self.model(**self.xb)
        self.pred = out['logits'].to(self.device)
        self('after_pred')
        self.loss_grad = out['loss'].to(self.device)
        self.loss = self.loss_grad.clone()
        self('after_loss')
        if not self.training or not len(self.yb): return
        self('before_backward')
        self.loss_grad.backward()
        self._with_events(self.opt.step, 'step', CancelStepException)
        self.opt.zero_grad()

# Cell
_o = {'OneCycle':'fit_one_cycle', 'CosineAnnealing':'fit_flat_cos', 'SGDR':'fit_sgdr'}
mk_class('Strategy', **_o, doc_string='Class for fitting strategies with typo-proofing')

# Cell
class AdaptiveTuner:
    """
    A base `Tuner` that interfaces with `AdaptiveLearner` with specific exposed functions
    """
    @delegates(_AdaptiveLearner.__init__)
    def __init__(self, expose_fastai:bool=False, **kwargs):
        self._tuner = _AdaptiveLearner(**kwargs)

        exposed_attrs = ['dls', 'model', 'loss_func', 'metrics']
        for attr in exposed_attrs:
            setattr(self, attr, getattr(self._tuner, attr))
        if expose_fastai:
            cls = self.__class__
            self.__class__ = cls.__class__("AdaptiveTuner", (cls, _AdaptiveLearner), kwargs)

    def tune(
        self,
        epochs:int, # Number of epochs to train for
        lr:float = None, # If None, finds a new LR and uses suggestion_method
        strategy:Strategy = Strategy.OneCycle,
        callbacks = [], # Extra fastai Callbacks
        **kwargs ## kwargs for the fit function

    ):
        "Fine tune `self.model` for `epochs` with an `lr` and `strategy`"
        func = getattr(self, strategy, f'_{strategy}')
        for attr in 'epochs,lr,cbs'.split():
            if attr in kwargs.keys(): kwargs.pop(attr)
        func(epochs, lr, cbs=callbacks, **kwargs)

    @delegates(Learner.lr_find)
    def lr_find(self, **kwargs): return self._tuner.lr_find(**kwargs)

    @delegates(Learner.save)
    def save(self, **kwargs): return self._tuner.save(**kwargs)

    @delegates(Learner.load)
    def load(self, **kwargs): return self._tuner.load(**kwargs)

for attr in ['lr_find', 'save', 'load']:
    setattr(getattr(AdaptiveTuner, attr), '__doc__', getattr(_AdaptiveLearner, attr).__doc__)

# Cell
from transformers import AutoModelForSequenceClassification

# Cell
class SequenceClassificationTuner(AdaptiveTuner):
    """
    A `AdaptiveLearner` with good defaults for Sequence Classification tasks

    **Valid kwargs and defaults:**
      - `lr`:float = 0.001
      - `splitter`:function = `trainable_params`
      - `cbs`:list = None
      - `path`:Path = None
      - `model_dir`:Path = 'models'
      - `wd`:float = None
      - `wd_bn_bias`:bool = False
      - `train_bn`:bool = True
      - `moms`: tuple(float) = (0.95, 0.85, 0.95)

    """
    def __init__(
        self,
        dls:DataLoaders, # A set of DataLoaders
        model, # A HuggingFace model
        loss_func = CrossEntropyLossFlat(), # A loss function
        metrics = [accuracy, F1Score()], # Metrics to monitor the training with
        opt_func = Adam, # A fastai or torch Optimizer
        additional_cbs = None, # Additional Callbacks to have always tied to the Tuner,
        expose_fastai_api = False, # Whether to expose the fastai API
        **kwargs, # kwargs for `Learner.__init__`
    ):
        additional_cbs = listify(additional_cbs)
        for arg in 'dls,model,loss_func,metrics,opt_func,cbs,expose_fastai'.split(','):
            if arg in kwargs.keys(): kwargs.pop(arg) # Pop all existing kwargs
        model = AutoModelForSequenceClassification.from_pretrained(model, num_labels=len(dls[0].categorize.classes))

        super().__init__(
            expose_fastai_api,
            dls = dls,
            model = model,
            loss_func = loss_func,
            metrics = metrics,
            opt_func = opt_func,
            cbs=additional_cbs,
            **kwargs
        )