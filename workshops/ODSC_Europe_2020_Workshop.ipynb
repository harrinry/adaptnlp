{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup and Install AdaptNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adaptnlp in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
      "Requirement already satisfied: jupyterlab in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (2.2.8)\n",
      "Requirement already satisfied: torch<2.0.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (1.6.0+cu101)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (7.1.2)\n",
      "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (1.0.0)\n",
      "Requirement already satisfied: flair<1.0.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (0.6.0.post1)\n",
      "Requirement already satisfied: nlp<1.0.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (0.4.0)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (1.0.5)\n",
      "Requirement already satisfied: transformers<4.0.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (3.1.0)\n",
      "Requirement already satisfied: jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from jupyterlab->adaptnlp) (2.11.2)\n",
      "Requirement already satisfied: tornado!=6.0.0,!=6.0.1,!=6.0.2 in /usr/local/lib/python3.6/dist-packages (from jupyterlab->adaptnlp) (5.1.1)\n",
      "Requirement already satisfied: notebook>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from jupyterlab->adaptnlp) (5.3.1)\n",
      "Requirement already satisfied: jupyterlab-server<2.0,>=1.1.5 in /usr/local/lib/python3.6/dist-packages (from jupyterlab->adaptnlp) (1.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.4.0->adaptnlp) (1.18.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.4.0->adaptnlp) (0.16.0)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (7.5.1)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (4.7.6)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (5.2.0)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (4.10.1)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (5.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (2.8.1)\n",
      "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (0.3.2)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (4.2.6)\n",
      "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (4.6.1)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (0.8.7)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (2019.12.20)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (3.6.0)\n",
      "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (0.3)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (5.8)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (3.2.2)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (0.1.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (0.1.91)\n",
      "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (1.0.8)\n",
      "Requirement already satisfied: pytest>=5.3.2 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (4.41.1)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (1.7.0)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (1.2.10)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (0.22.2.post1)\n",
      "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (1.5.10)\n",
      "Requirement already satisfied: janome in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp) (0.4.0)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp<1.0.0,>=0.4.0->adaptnlp) (1.0.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp<1.0.0,>=0.4.0->adaptnlp) (0.7)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from nlp<1.0.0,>=0.4.0->adaptnlp) (2.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp<1.0.0,>=0.4.0->adaptnlp) (3.0.12)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp<1.0.0,>=0.4.0->adaptnlp) (2.23.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp<1.0.0,>=0.4.0->adaptnlp) (0.3.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<2.0.0,>=1.0.0->adaptnlp) (2018.9)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<4.0.0,>=3.0.0->adaptnlp) (20.4)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers<4.0.0,>=3.0.0->adaptnlp) (0.8.1rc2)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<4.0.0,>=3.0.0->adaptnlp) (0.0.43)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.10->jupyterlab->adaptnlp) (1.1.1)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (4.3.3)\n",
      "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (4.6.3)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (5.3.5)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (5.0.7)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (0.8.3)\n",
      "Requirement already satisfied: json5 in /usr/local/lib/python3.6/dist-packages (from jupyterlab-server<2.0,>=1.1.5->jupyterlab->adaptnlp) (0.9.5)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from jupyterlab-server<2.0,>=1.1.5->jupyterlab->adaptnlp) (3.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->adaptnlp) (3.5.1)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->adaptnlp) (5.5.0)\n",
      "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (1.9.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (2.1.3)\n",
      "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (19.0.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->adaptnlp) (1.0.18)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.4.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.6.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.8.4)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (3.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->flair<1.0.0,>=0.6.0->adaptnlp) (1.15.0)\n",
      "Requirement already satisfied: overrides==3.0.0 in /usr/local/lib/python3.6/dist-packages (from konoha<5.0.0,>=4.0.0->flair<1.0.0,>=0.6.0->adaptnlp) (3.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp) (2.1.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair<1.0.0,>=0.6.0->adaptnlp) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair<1.0.0,>=0.6.0->adaptnlp) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair<1.0.0,>=0.6.0->adaptnlp) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair<1.0.0,>=0.6.0->adaptnlp) (1.2.0)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair<1.0.0,>=0.6.0->adaptnlp) (3.11.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair<1.0.0,>=0.6.0->adaptnlp) (2.5)\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp) (1.9.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp) (8.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp) (20.1.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp) (0.13.1)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp) (0.10.1)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp) (1.7.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair<1.0.0,>=0.6.0->adaptnlp) (1.12.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair<1.0.0,>=0.6.0->adaptnlp) (0.16.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp<1.0.0,>=0.4.0->adaptnlp) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp<1.0.0,>=0.4.0->adaptnlp) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp<1.0.0,>=0.4.0->adaptnlp) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp<1.0.0,>=0.4.0->adaptnlp) (2.10)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook>=4.3.1->jupyterlab->adaptnlp) (4.4.2)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook>=4.3.1->jupyterlab->adaptnlp) (0.6.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->jupyterlab-server<2.0,>=1.1.5->jupyterlab->adaptnlp) (49.6.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->jupyterlab-server<2.0,>=1.1.5->jupyterlab->adaptnlp) (0.16.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->adaptnlp) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->adaptnlp) (4.8.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->adaptnlp) (0.8.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->adaptnlp) (0.5.1)\n",
      "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp) (2.49.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp) (1.14.48)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp) (3.1.0)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp) (1.17.48)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp) (0.3.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp) (0.15.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.7)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.0.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.12)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->datasets) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install adaptnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sequence classification fine-tuning: \"bert-base-cased\"\n",
    "\n",
    "We will be loading in a pre-trained language model called \"bert-base-cased\" and fine-tuning it on the AG News dataset. This language model is a good base model that's been trained on a large and general set of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Model loading and taggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "from adaptnlp import EasySequenceClassifier\n",
    "from pprint import pprint\n",
    "\n",
    "classifier = EasySequenceClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-16 17:38:13,981 loading file bert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/andrew/Documents/github/adaptnlp/venv-adaptnlp/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Predicting text: 100%|██████████| 1/1 [00:00<00:00, 109.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "{\"This didn't work at all\": [LABEL_0 (0.3342), LABEL_1 (0.6658)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "example_text = \"This didn't work at all\"\n",
    "\n",
    "sentences = classifier.tag_text(\n",
    "    text=example_text,\n",
    "    model_name_or_path=\"bert-base-cased\",\n",
    "    mini_batch_size=1,\n",
    ")\n",
    "\n",
    "print(\"Tag Score Outputs:\\n\")\n",
    "for sentence in sentences:\n",
    "    pprint({sentence.to_original_text(): sentence.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data loading and processing with [datasets](https://github.com/huggingface/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking /home/andrew/.cache/huggingface/datasets/d64a4904fdd2e50e1e6ea218644d57a262d44f2ca4708683474fc8acc0e92c3f.ed7f7df11daf93bb1e91f4b39dea596eada032d21abe964a4db45ebe33f18a94.py for additional imports.\n",
      "Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/trec/trec.py at /home/andrew/.cache/huggingface/modules/datasets_modules/datasets/trec\n",
      "Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/trec/trec.py at /home/andrew/.cache/huggingface/modules/datasets_modules/datasets/trec/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7\n",
      "Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/trec/trec.py to /home/andrew/.cache/huggingface/modules/datasets_modules/datasets/trec/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/trec.py\n",
      "Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/trec/dataset_infos.json to /home/andrew/.cache/huggingface/modules/datasets_modules/datasets/trec/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/dataset_infos.json\n",
      "Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/trec/trec.py at /home/andrew/.cache/huggingface/modules/datasets_modules/datasets/trec/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/trec.json\n",
      "Using custom data configuration default\n",
      "Loading Dataset Infos from /home/andrew/.cache/huggingface/modules/datasets_modules/datasets/trec/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7\n",
      "Overwrite dataset info from restored data version.\n",
      "Loading Dataset info from /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7\n",
      "Reusing dataset trec (/home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7)\n",
      "Constructing Dataset for split ['train', 'test'], from /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7\n",
      "100%|██████████| 2/2 [00:00<00:00, 705.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'builder_name': 'trec',\n",
      " 'citation': '@inproceedings{li-roth-2002-learning,\\n'\n",
      "             '    title = \"Learning Question Classifiers\",\\n'\n",
      "             '    author = \"Li, Xin  and\\n'\n",
      "             '      Roth, Dan\",\\n'\n",
      "             '    booktitle = \"{COLING} 2002: The 19th International '\n",
      "             'Conference on Computational Linguistics\",\\n'\n",
      "             '    year = \"2002\",\\n'\n",
      "             '    url = \"https://www.aclweb.org/anthology/C02-1150\",\\n'\n",
      "             '}\\n'\n",
      "             '@inproceedings{hovy-etal-2001-toward,\\n'\n",
      "             '    title = \"Toward Semantics-Based Answer Pinpointing\",\\n'\n",
      "             '    author = \"Hovy, Eduard  and\\n'\n",
      "             '      Gerber, Laurie  and\\n'\n",
      "             '      Hermjakob, Ulf  and\\n'\n",
      "             '      Lin, Chin-Yew  and\\n'\n",
      "             '      Ravichandran, Deepak\",\\n'\n",
      "             '    booktitle = \"Proceedings of the First International '\n",
      "             'Conference on Human Language Technology Research\",\\n'\n",
      "             '    year = \"2001\",\\n'\n",
      "             '    url = \"https://www.aclweb.org/anthology/H01-1069\",\\n'\n",
      "             '}\\n',\n",
      " 'config_name': 'default',\n",
      " 'dataset_size': 413073,\n",
      " 'description': 'The Text REtrieval Conference (TREC) Question Classification '\n",
      "                'dataset contains 5500 labeled questions in training set and '\n",
      "                'another 500 for test set. The dataset has 6 labels, 47 '\n",
      "                'level-2 labels. Average length of each sentence is 10, '\n",
      "                'vocabulary size of 8700.\\n'\n",
      "                '\\n'\n",
      "                'Data are collected from four sources: 4,500 English questions '\n",
      "                'published by USC (Hovy et al., 2001), about 500 manually '\n",
      "                'constructed questions for a few rare classes, 894 TREC 8 and '\n",
      "                'TREC 9 questions, and also 500 questions from TREC 10 which '\n",
      "                'serves as the test set.\\n',\n",
      " 'download_checksums': {'http://cogcomp.org/Data/QA/QC/TREC_10.label': {'checksum': '033f22c028c2bbba9ca682f68ffe204dc1aa6e1cf35dd6207f2d4ca67f0d0e8e',\n",
      "                                                                        'num_bytes': 23354},\n",
      "                        'http://cogcomp.org/Data/QA/QC/train_5500.label': {'checksum': '9e4c8bdcaffb96ed61041bd64b564183d52793a8e91d84fc3a8646885f466ec3',\n",
      "                                                                           'num_bytes': 335858}},\n",
      " 'download_size': 359212,\n",
      " 'features': {'label-coarse': ClassLabel(num_classes=6, names=['DESC', 'ENTY', 'ABBR', 'HUM', 'NUM', 'LOC'], names_file=None, id=None),\n",
      "              'label-fine': ClassLabel(num_classes=47, names=['manner', 'cremat', 'animal', 'exp', 'ind', 'gr', 'title', 'def', 'date', 'reason', 'event', 'state', 'desc', 'count', 'other', 'letter', 'religion', 'food', 'country', 'color', 'termeq', 'city', 'body', 'dismed', 'mount', 'money', 'product', 'period', 'substance', 'sport', 'plant', 'techmeth', 'volsize', 'instru', 'abb', 'speed', 'word', 'lang', 'perc', 'code', 'dist', 'temp', 'symbol', 'ord', 'veh', 'weight', 'currency'], names_file=None, id=None),\n",
      "              'text': Value(dtype='string', id=None)},\n",
      " 'homepage': 'https://cogcomp.seas.upenn.edu/Data/QA/QC/',\n",
      " 'license': '',\n",
      " 'post_processed': None,\n",
      " 'post_processing_size': None,\n",
      " 'size_in_bytes': 772285,\n",
      " 'splits': {'test': SplitInfo(name='test', num_bytes=27983, num_examples=500, dataset_name='trec'),\n",
      "            'train': SplitInfo(name='train', num_bytes=385090, num_examples=5452, dataset_name='trec')},\n",
      " 'supervised_keys': None,\n",
      " 'version': 1.1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset, eval_dataset = load_dataset('trec', split=['train', 'test'])\n",
    "\n",
    "pprint(vars(train_dataset.info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set __getitem__(key) output type to pandas for ['text', 'label-coarse'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>1</td>\n",
       "      <td>What 's the shape of a camel 's spine ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>1</td>\n",
       "      <td>What type of currency is used in China ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>4</td>\n",
       "      <td>What is the temperature today ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>4</td>\n",
       "      <td>What is the temperature for cooking ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>1</td>\n",
       "      <td>What currency is used in Australia ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5452 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse                                               text\n",
       "0                0  How did serfdom develop in and then leave Russ...\n",
       "1                1   What films featured the character Popeye Doyle ?\n",
       "2                0  How can I find a list of celebrities ' real na...\n",
       "3                1  What fowl grabs the spotlight after the Chines...\n",
       "4                2                    What is the full form of .com ?\n",
       "...            ...                                                ...\n",
       "5447             1            What 's the shape of a camel 's spine ?\n",
       "5448             1           What type of currency is used in China ?\n",
       "5449             4                    What is the temperature today ?\n",
       "5450             4              What is the temperature for cooking ?\n",
       "5451             1               What currency is used in Australia ?\n",
       "\n",
       "[5452 rows x 2 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.set_format(type=\"pandas\", columns=[\"text\", \"label-coarse\"])\n",
    "train_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set __getitem__(key) output type to python objects for ['text', 'label-coarse'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n"
     ]
    }
   ],
   "source": [
    "# We just run this to reformat back to a 'python' dataset\n",
    "train_dataset.set_format(columns=[\"text\", \"label-coarse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./trec-models',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluate_during_training=True,\n",
    "    logging_dir='./logs',\n",
    "    save_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/cache-74eaa265e83086ed.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e59111a23944fe902517cba65caec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 5452 examples in 38555508 bytes /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/tmptjm0pgu3.\n",
      "Set __getitem__(key) output type to python objects for ['label-coarse', 'text'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/cache-203ad5a27ecbd4af.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b262d4678fa04c9fb241f59e3774c6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 500 examples in 321995 bytes /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/tmpeo101k5z.\n",
      "Set __getitem__(key) output type to torch for ['input_ids', 'attention_mask', 'labels'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "Set __getitem__(key) output type to torch for ['input_ids', 'attention_mask', 'labels'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc6840514804e71874277a3b20ccbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf2dace88bf4dd8b4e23c9413c86be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1363.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Documents/github/adaptnlp/venv-adaptnlp/lib/python3.6/site-packages/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/andrew/Documents/github/adaptnlp/venv-adaptnlp/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.959968994140625, 'learning_rate': 5e-05, 'epoch': 0.36683785766691124, 'step': 500}\n",
      "{'loss': 0.388633056640625, 'learning_rate': 2.103128621089224e-05, 'epoch': 0.7336757153338225, 'step': 1000}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7407f9cccd041e8ab546f15762ca8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=125.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.16818927204841747, 'eval_accuracy': 0.966, 'eval_f1': array([0.97508897, 0.92222222, 0.94117647, 0.98461538, 0.97797357,\n",
      "       0.96969697]), 'eval_precision': array([0.95804196, 0.96511628, 1.        , 0.98461538, 0.97368421,\n",
      "       0.95238095]), 'eval_recall': array([0.99275362, 0.88297872, 0.88888889, 0.98461538, 0.98230088,\n",
      "       0.98765432]), 'epoch': 0.7336757153338225, 'step': 1000}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.train(training_args=training_args,\n",
    "                 train_dataset=train_dataset,\n",
    "                 eval_dataset=eval_dataset,\n",
    "                 model_name_or_path=\"bert-base-cased\",\n",
    "                 text_col_nm=\"text\",\n",
    "                 label_col_nm=\"label-coarse\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54dcd05216124ccaba328a667600a5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=125.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.1950705322229769, 'eval_accuracy': 0.97, 'eval_f1': array([0.97841727, 0.92391304, 1.        , 0.98461538, 0.98245614,\n",
      "       0.97530864]), 'eval_precision': array([0.97142857, 0.94444444, 1.        , 0.98461538, 0.97391304,\n",
      "       0.97530864]), 'eval_recall': array([0.98550725, 0.90425532, 1.        , 0.98461538, 0.99115044,\n",
      "       0.97530864]), 'epoch': 1.0, 'step': 1363}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1950705322229769,\n",
       " 'eval_accuracy': 0.97,\n",
       " 'eval_f1': array([0.97841727, 0.92391304, 1.        , 0.98461538, 0.98245614,\n",
       "        0.97530864]),\n",
       " 'eval_precision': array([0.97142857, 0.94444444, 1.        , 0.98461538, 0.97391304,\n",
       "        0.97530864]),\n",
       " 'eval_recall': array([0.98550725, 0.90425532, 1.        , 0.98461538, 0.99115044,\n",
       "        0.97530864]),\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(model_name_or_path=\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-16 17:40:01,844 loading file ./trec-models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Documents/github/adaptnlp/venv-adaptnlp/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Predicting text: 100%|██████████| 4/4 [00:00<00:00, 130.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Score Outputs:\n",
      "\n",
      "{'The countries in the northern hemisphere talked to the countries': [DESC (0.0008),\n",
      "                                                                      ENTY (0.0006),\n",
      "                                                                      ABBR (0.0001),\n",
      "                                                                      HUM (0.0003),\n",
      "                                                                      NUM (0.0003),\n",
      "                                                                      LOC (0.9979)]}\n",
      "{'The basketball player made a touchdown in the field goal': [DESC (0.0002),\n",
      "                                                              ENTY (0.0007),\n",
      "                                                              ABBR (0.0001),\n",
      "                                                              HUM (0.9988),\n",
      "                                                              NUM (0.0001),\n",
      "                                                              LOC (0.0001)]}\n",
      "{'The market was down 40% and economists were puzzled': [DESC (0.2248),\n",
      "                                                         ENTY (0.0932),\n",
      "                                                         ABBR (0.0104),\n",
      "                                                         HUM (0.0678),\n",
      "                                                         NUM (0.5751),\n",
      "                                                         LOC (0.0288)]}\n",
      "{'The engineer and the scientist made it to pluto in their rocket': [DESC (0.0003),\n",
      "                                                                     ENTY (0.0008),\n",
      "                                                                     ABBR (0.0001),\n",
      "                                                                     HUM (0.9984),\n",
      "                                                                     NUM (0.0001),\n",
      "                                                                     LOC (0.0001)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "multiple_text = [\"The countries in the northern hemisphere talked to the countries\",\n",
    "                 \"The basketball player made a touchdown in the field goal\",\n",
    "                 \"The market was down 40% and economists were puzzled\",\n",
    "                 \"The engineer and the scientist made it to pluto in their rocket\"]\n",
    "\n",
    "sentences = classifier.tag_text(\n",
    "    multiple_text,\n",
    "    model_name_or_path=\"./trec-models\",\n",
    "    mini_batch_size=1\n",
    ")\n",
    "\n",
    "print(\"Tag Score Outputs:\\n\")\n",
    "for sentence in sentences:\n",
    "    pprint({sentence.to_original_text(): sentence.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sequence classification fine-tuning: \"distilbert-base-uncased-finetuned-sst-2-english\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Model release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.release_model(model_name_or_path=\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./trec-from-sst-models',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluate_during_training=True,\n",
    "    logging_dir='./logs',\n",
    "    save_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing the mapped function outputs\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/cache-636eb496a50e077d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b840c97652944a98a12980b225971ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Done writing 5452 examples in 23769660 bytes /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/tmpzvonbhhy.\n",
      "Set __getitem__(key) output type to python objects for ['label-coarse', 'text'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "Testing the mapped function outputs\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Testing finished, running the mapping function on the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/cache-caee9bfa42ee29d5.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c6679058734f1294c25c12270f8015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Done writing 500 examples in 223991 bytes /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/tmplbpa8osr.\n",
      "Set __getitem__(key) output type to torch for ['input_ids', 'attention_mask', 'labels'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "Set __getitem__(key) output type to torch for ['input_ids', 'attention_mask', 'labels'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba69697619554da1aaa0ab6a100ac45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1f5355ecfa4d97889f329610aaea70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1363.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Documents/github/adaptnlp/venv-adaptnlp/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0061688232421875, 'learning_rate': 5e-05, 'epoch': 0.36683785766691124, 'step': 500}\n",
      "{'loss': 0.36240771484375, 'learning_rate': 2.103128621089224e-05, 'epoch': 0.7336757153338225, 'step': 1000}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d83c77e5887411a9bb601ce5bb301d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=125.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.19626898866705597, 'eval_accuracy': 0.956, 'eval_f1': array([0.94845361, 0.9039548 , 0.8       , 0.98461538, 0.98666667,\n",
      "       0.97530864]), 'eval_precision': array([0.90196078, 0.96385542, 1.        , 0.98461538, 0.99107143,\n",
      "       0.97530864]), 'eval_recall': array([1.        , 0.85106383, 0.66666667, 0.98461538, 0.98230088,\n",
      "       0.97530864]), 'epoch': 0.7336757153338225, 'step': 1000}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.train(training_args=training_args,\n",
    "                 train_dataset=train_dataset,\n",
    "                 eval_dataset=eval_dataset,\n",
    "                 model_name_or_path=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                 text_col_nm=\"text\",\n",
    "                 label_col_nm=\"label-coarse\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db60ca1bc30541adb77780e3e70aa189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=125.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.15069465056993067, 'eval_accuracy': 0.972, 'eval_f1': array([0.97163121, 0.93406593, 0.875     , 0.98461538, 0.99122807,\n",
      "       0.98765432]), 'eval_precision': array([0.95138889, 0.96590909, 1.        , 0.98461538, 0.9826087 ,\n",
      "       0.98765432]), 'eval_recall': array([0.99275362, 0.90425532, 0.77777778, 0.98461538, 1.        ,\n",
      "       0.98765432]), 'epoch': 1.0, 'step': 1363}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.15069465056993067,\n",
       " 'eval_accuracy': 0.972,\n",
       " 'eval_f1': array([0.97163121, 0.93406593, 0.875     , 0.98461538, 0.99122807,\n",
       "        0.98765432]),\n",
       " 'eval_precision': array([0.95138889, 0.96590909, 1.        , 0.98461538, 0.9826087 ,\n",
       "        0.98765432]),\n",
       " 'eval_recall': array([0.99275362, 0.90425532, 0.77777778, 0.98461538, 1.        ,\n",
       "        0.98765432]),\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(model_name_or_path=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sequence Classification on custom language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Language model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-16 17:41:05--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.66.22\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.66.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4721645 (4.5M) [application/zip]\n",
      "Saving to: ‘wikitext-2-raw-v1.zip’\n",
      "\n",
      "wikitext-2-raw-v1.z 100%[===================>]   4.50M  27.6MB/s    in 0.2s    \n",
      "\n",
      "2020-09-16 17:41:06 (27.6 MB/s) - ‘wikitext-2-raw-v1.zip’ saved [4721645/4721645]\n",
      "\n",
      "Archive:  wikitext-2-raw-v1.zip\n",
      "   creating: wikitext-2-raw/\n",
      "  inflating: wikitext-2-raw/wiki.test.raw  \n",
      "  inflating: wikitext-2-raw/wiki.valid.raw  \n",
      "  inflating: wikitext-2-raw/wiki.train.raw  \n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
    "!unzip -o wikitext-2-raw-v1.zip\n",
    "\n",
    "train_file = \"./wikitext-2-raw/wiki.train.raw\"\n",
    "eval_file = \"./wikitext-2-raw/wiki.test.raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./language-models',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluate_during_training=False,\n",
    "    logging_dir='./logs',\n",
    "    save_steps=2500,\n",
    "    eval_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Documents/github/adaptnlp/venv-adaptnlp/lib/python3.6/site-packages/transformers/modeling_auto.py:821: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from adaptnlp import LMFineTuner\n",
    "\n",
    "finetuner = LMFineTuner(model_name_or_path=\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/16/2020 17:41:09 - WARNING - adaptnlp.language_model -   Process rank: -1,\n",
      "                device: cuda:0,\n",
      "                n_gpu: 1,\n",
      "                distributed training: False,\n",
      "                16-bits training: False\n",
      "            \n",
      "09/16/2020 17:41:09 - INFO - adaptnlp.language_model -   Training/evaluation parameters: {\n",
      "  \"output_dir\": \"./language-models\",\n",
      "  \"overwrite_output_dir\": false,\n",
      "  \"do_train\": false,\n",
      "  \"do_eval\": false,\n",
      "  \"do_predict\": false,\n",
      "  \"evaluate_during_training\": false,\n",
      "  \"prediction_loss_only\": false,\n",
      "  \"per_device_train_batch_size\": 1,\n",
      "  \"per_device_eval_batch_size\": 1,\n",
      "  \"per_gpu_train_batch_size\": null,\n",
      "  \"per_gpu_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"learning_rate\": 5e-05,\n",
      "  \"weight_decay\": 0.01,\n",
      "  \"adam_beta1\": 0.9,\n",
      "  \"adam_beta2\": 0.999,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"max_steps\": -1,\n",
      "  \"warmup_steps\": 500,\n",
      "  \"logging_dir\": \"./logs\",\n",
      "  \"logging_first_step\": false,\n",
      "  \"logging_steps\": 500,\n",
      "  \"save_steps\": 2500,\n",
      "  \"save_total_limit\": null,\n",
      "  \"no_cuda\": false,\n",
      "  \"seed\": 42,\n",
      "  \"fp16\": false,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"local_rank\": -1,\n",
      "  \"tpu_num_cores\": null,\n",
      "  \"tpu_metrics_debug\": false,\n",
      "  \"debug\": false,\n",
      "  \"dataloader_drop_last\": false,\n",
      "  \"eval_steps\": 100,\n",
      "  \"past_index\": -1,\n",
      "  \"run_name\": null,\n",
      "  \"disable_tqdm\": false,\n",
      "  \"remove_unused_columns\": true\n",
      "}\n",
      "/home/andrew/Documents/github/adaptnlp/venv-adaptnlp/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1321: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n",
      "09/16/2020 17:41:09 - INFO - filelock -   Lock 140604814126496 acquired on ./wikitext-2-raw/cached_lm_BertTokenizerFast_510_wiki.test.raw.lock\n",
      "09/16/2020 17:41:10 - INFO - filelock -   Lock 140604814126496 released on ./wikitext-2-raw/cached_lm_BertTokenizerFast_510_wiki.test.raw.lock\n",
      "09/16/2020 17:41:10 - INFO - filelock -   Lock 140604814126384 acquired on ./wikitext-2-raw/cached_lm_BertTokenizerFast_510_wiki.test.raw.lock\n",
      "09/16/2020 17:41:10 - INFO - filelock -   Lock 140604814126384 released on ./wikitext-2-raw/cached_lm_BertTokenizerFast_510_wiki.test.raw.lock\n",
      "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc7cdff821d49069eb9487f800f6c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db76ca292e644b3b9bafe1e4fef7fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=552.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.053148681640625, 'learning_rate': 5e-05, 'epoch': 0.9057971014492754, 'step': 500}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finetuner.train(\n",
    "    training_args=training_args,\n",
    "    train_file=eval_file,\n",
    "    eval_file=eval_file,\n",
    "    mlm=True,\n",
    "    overwrite_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sequence Classification task training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./trec-from-custom-LM-models',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluate_during_training=True,\n",
    "    logging_dir='./logs',\n",
    "    save_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./language-models were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./language-models and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/cache-f700cc54a52f5815.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dee7bf3b4fe4876b73f993b7f4da692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 5452 examples in 38555508 bytes /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/tmpj9c06gac.\n",
      "Set __getitem__(key) output type to python objects for ['label-coarse', 'text'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/cache-56f3d2867237fb45.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdef32f6a9d3415e8bb912376052e678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 500 examples in 321995 bytes /home/andrew/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/tmpxhqpsqui.\n",
      "Set __getitem__(key) output type to torch for ['input_ids', 'attention_mask', 'labels'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "Set __getitem__(key) output type to torch for ['input_ids', 'attention_mask', 'labels'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b685b13ba6454d0fb1ff373c09f9f668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc145215cb64695a5a0eabce398cccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1363.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9473241577148438, 'learning_rate': 5e-05, 'epoch': 0.36683785766691124, 'step': 500}\n",
      "{'loss': 0.45172247314453123, 'learning_rate': 2.103128621089224e-05, 'epoch': 0.7336757153338225, 'step': 1000}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b4481d573c476c895a399e8d441b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=125.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.22290657736640423, 'eval_accuracy': 0.96, 'eval_f1': array([0.96167247, 0.92307692, 0.8       , 0.98461538, 0.97321429,\n",
      "       0.97530864]), 'eval_precision': array([0.9261745 , 0.95454545, 1.        , 0.98461538, 0.98198198,\n",
      "       0.97530864]), 'eval_recall': array([1.        , 0.89361702, 0.66666667, 0.98461538, 0.96460177,\n",
      "       0.97530864]), 'epoch': 0.7336757153338225, 'step': 1000}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.train(training_args=training_args,\n",
    "                 train_dataset=train_dataset,\n",
    "                 eval_dataset=eval_dataset,\n",
    "                 model_name_or_path=\"./language-models\",\n",
    "                 text_col_nm=\"text\",\n",
    "                 label_col_nm=\"label-coarse\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3094b345104542ac84d42de817d34beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=125.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.16871526526450178, 'eval_accuracy': 0.97, 'eval_f1': array([0.97508897, 0.93478261, 0.8       , 0.98461538, 0.98678414,\n",
      "       0.98159509]), 'eval_precision': array([0.95804196, 0.95555556, 1.        , 0.98461538, 0.98245614,\n",
      "       0.97560976]), 'eval_recall': array([0.99275362, 0.91489362, 0.66666667, 0.98461538, 0.99115044,\n",
      "       0.98765432]), 'epoch': 1.0, 'step': 1363}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.16871526526450178,\n",
       " 'eval_accuracy': 0.97,\n",
       " 'eval_f1': array([0.97508897, 0.93478261, 0.8       , 0.98461538, 0.98678414,\n",
       "        0.98159509]),\n",
       " 'eval_precision': array([0.95804196, 0.95555556, 1.        , 0.98461538, 0.98245614,\n",
       "        0.97560976]),\n",
       " 'eval_recall': array([0.99275362, 0.91489362, 0.66666667, 0.98461538, 0.99115044,\n",
       "        0.98765432]),\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(model_name_or_path=\"./language-models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Tutorials for NLP Tasks with AdaptNLP*\n",
    "\n",
    "  1. Token Classification: NER, POS, Chunk, and Frame Tagging\n",
    "      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/1.%20Token%20Classification/token_tagging.ipynb)\n",
    "  2. Sequence Classification: Sentiment\n",
    "      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/2.%20Sequence%20Classification/Easy%20Sequence%20Classifier.ipynb)\n",
    "  3. Embeddings: Transformer Embeddings e.g. BERT, XLM, GPT2, XLNet, roBERTa, ALBERT\n",
    "      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/3.%20Embeddings/embeddings.ipynb)\n",
    "  4. Question Answering: Span-based Question Answering Model\n",
    "      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/4.%20Question%20Answering/question_answering.ipynb)\n",
    "  5. Summarization: Abstractive and Extractive\n",
    "      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/5.%20Summarization/summarization.ipynb)\n",
    "  6. Translation: Seq2Seq\n",
    "      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/6.%20Translation/translation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Tutorial for Fine-tuning and Training Custom Models with AdaptNLP*\n",
    "\n",
    " 1. Training a Sequence Classifier\n",
    "   - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/2.%20Sequence%20Classification/Easy%20Sequence%20Classifier.ipynb)\n",
    " 2. Fine-tuning a Transformers Language Model\n",
    "   - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/Finetuning%20and%20Training%20(Advanced)/Fine-tuning%20Language%20Model.ipynb)\n",
    "  \n",
    "Checkout the [documentation](https://novetta.github.io/adaptnlp) for more information.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *NVIDIA Docker and Configurable AdaptNLP REST Microservices*\n",
    "\n",
    "  1. AdaptNLP official docker images are up on [Docker Hub](https://hub.docker.com/r/achangnovetta/adaptnlp).\n",
    "  2. REST Microservices with AdaptNLP and FastAPI are also up on [Docker Hub](https://hub.docker.com/r/achangnovetta)\n",
    " \n",
    "All images can build with GPU support if NVIDIA-Docker is correctly installed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
